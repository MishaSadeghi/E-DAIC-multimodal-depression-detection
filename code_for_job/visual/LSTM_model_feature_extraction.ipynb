{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from torchviz import make_dot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the custom dataset class\n",
    "class PHQDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_folder, max_seq_length=None, feature_type=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.data_info = pd.read_csv(csv_file)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.feature_type = feature_type\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        participant_id = self.data_info.iloc[idx]['Participant_ID']\n",
    "        phq_score = self.data_info.iloc[idx]['PHQ_Score']\n",
    "        filepath = os.path.join(self.data_folder, f\"{participant_id}_OpenFace2.1.0_Pose_gaze_AUs.csv\")\n",
    "        features_df = pd.read_csv(filepath)\n",
    "        features_df = features_df.iloc[:, 2:]  # Remove the first two columns (frame and timestamp)\n",
    "        # features = pd.read_csv(filepath).to_numpy()\n",
    "        # features = features[:, 2:] # Remove the first two features (frame and timestamp)\n",
    "\n",
    "        # Define column sets\n",
    "        pose_columns = [col for col in features_df.columns if col.startswith('pose_')]\n",
    "        gaze_columns = [col for col in features_df.columns if col.startswith('gaze_')]\n",
    "        au_r_columns = [col for col in features_df.columns if col.endswith('_r')]\n",
    "        au_c_columns = [col for col in features_df.columns if col.endswith('_c')]\n",
    "\n",
    "        # Select columns based on feature type\n",
    "        if self.feature_type == 'pose':\n",
    "            selected_columns = pose_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'gaze':\n",
    "            selected_columns = gaze_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'au_r':\n",
    "            selected_columns = au_r_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'au_c':\n",
    "            selected_columns = au_c_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'confidence_success_pose':\n",
    "            selected_columns = pose_columns + ['confidence', 'success']\n",
    "        elif self.feature_type == 'confidence_success_gaze':\n",
    "            selected_columns = gaze_columns + ['confidence', 'success']\n",
    "        elif self.feature_type == 'confidence_success_AUintens':\n",
    "            selected_columns = au_r_columns + ['confidence', 'success']\n",
    "        elif self.feature_type == 'confidence_success_AUoccurr':\n",
    "            selected_columns = au_c_columns + ['confidence', 'success']\n",
    "        elif self.feature_type == 'confidence_success_AUoccurr_pose_gaze':\n",
    "            selected_columns = au_c_columns + gaze_columns + pose_columns + ['confidence', 'success']\n",
    "        elif self.feature_type == 'confidence_success_AUintens_pose_gaze':\n",
    "            selected_columns = au_r_columns + gaze_columns + pose_columns + ['confidence', 'success']\n",
    "        elif self.feature_type == 'pose_gaze':\n",
    "            selected_columns = gaze_columns + pose_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'pose_au_r':\n",
    "            selected_columns = pose_columns + au_r_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'pose_au_c':\n",
    "            selected_columns = pose_columns + au_c_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'gaze_au_r':\n",
    "            selected_columns = gaze_columns + au_r_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'gaze_au_c':\n",
    "            selected_columns = gaze_columns + au_c_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'au_r_au_c':\n",
    "            selected_columns = au_r_columns + au_c_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'pose_gaze_au_r':\n",
    "            selected_columns = pose_columns + gaze_columns + au_r_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'pose_gaze_au_c':\n",
    "            selected_columns = pose_columns + gaze_columns + au_c_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'pose_au_r_au_c':\n",
    "            selected_columns = pose_columns + au_r_columns + au_c_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'gaze_au_r_au_c':\n",
    "            selected_columns = gaze_columns + au_r_columns + au_c_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'all':\n",
    "            selected_columns = pose_columns + gaze_columns + au_r_columns + au_c_columns\n",
    "            # Filter by confidence\n",
    "            indices_to_remove = features_df[features_df['confidence'] < 0.9].index\n",
    "            features_df = features_df.drop(index=indices_to_remove)\n",
    "        elif self.feature_type == 'confidence_success_all':\n",
    "            selected_columns = None\n",
    "        else:\n",
    "            selected_columns = None  # Use all columns if feature_type is not recognized\n",
    "        \n",
    "        if selected_columns is not None:\n",
    "            features_df = features_df[selected_columns]\n",
    "            # Remove rows with NaN values\n",
    "            features_df = features_df.dropna()\n",
    "            features = features_df.values\n",
    "        else:\n",
    "            # Remove rows with NaN values\n",
    "            features_df = features_df.dropna()\n",
    "            features = features_df.values\n",
    "        \n",
    "        # Apply feature normalization\n",
    "        # features_normalized = self.scaler.fit_transform(features)\n",
    "        # print(\"features_normalized shape: \", features_normalized.shape)\n",
    "\n",
    "        # if self.max_seq_length is not None:\n",
    "        #     padded_features = np.zeros((self.max_seq_length, features_normalized.shape[1]))\n",
    "        #     padded_features[:features_normalized.shape[0], :features_normalized.shape[1]] = features_normalized\n",
    "        #     features_normalized = padded_features\n",
    "\n",
    "        # print('features.shape at the end: ', features.shape)\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(phq_score, dtype=torch.float32)\n",
    "        # return torch.tensor(features_normalized, dtype=torch.float32), torch.tensor(phq_score, dtype=torch.float32)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.feature_dim = feature_dim\n",
    "        self.proj = nn.Linear(feature_dim, 64)\n",
    "        self.context_vector = nn.Linear(64, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = torch.tanh(self.proj(x))\n",
    "        context_vector = self.context_vector(x_proj).squeeze(2)\n",
    "        attention_weights = torch.softmax(context_vector, dim=1)\n",
    "        weighted = torch.mul(x, attention_weights.unsqueeze(-1).expand_as(x))\n",
    "        return torch.sum(weighted, dim=1)\n",
    "    \n",
    "# class EnhancedPHQLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers=2, dropout_rate=0.5):\n",
    "#         super(EnhancedPHQLSTM, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "#                             batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "#         self.attention = Attention(hidden_size * 2)\n",
    "#         self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         attn_out = self.attention(lstm_out)\n",
    "#         # final_output = self.fc(lstm_out[:, -1, :])  # Taking the last output of the sequence to check the performance without attention layer\n",
    "#         final_output = self.fc(attn_out)\n",
    "#         return final_output\n",
    "\n",
    "class EnhancedPHQLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, dropout_rate=0.5):\n",
    "        super(EnhancedPHQLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "        self.attention = Attention(hidden_size * 2)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 128)  # Adjust the output size of the fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_out = self.attention(lstm_out)\n",
    "        return attn_out  # Return the output of the attention layer\n",
    "\n",
    "    def get_last_fc_output(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # print(\"Shape of lstm_out:\", lstm_out.shape)\n",
    "        attn_out = self.attention(lstm_out)\n",
    "        print(\"Shape of attn_out:\", attn_out.shape)\n",
    "        fc_out = self.fc(attn_out)\n",
    "        # print(\"Shape of fc_out:\", fc_out.shape)\n",
    "        \n",
    "        # final_output_a = self.fc(lstm_out[:, -1, :]) \n",
    "        # print(\"Shape of final_output_a:\", final_output_a.shape)\n",
    "        # final_output_b = self.fc(attn_out)\n",
    "        # print(\"Shape of final_output_b:\", final_output_b.shape)\n",
    "        # return fc_out\n",
    "        return attn_out\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "mae = nn.L1Loss()\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    yy_stack = torch.stack(yy, dim=0)\n",
    "    return xx_pad, yy_stack, x_lens\n",
    "\n",
    "# Define different combinations of feature types\n",
    "feature_combinations = ['all', 'pose','gaze','au_r','au_c', \\\n",
    "                        'pose_gaze', 'pose_au_r', 'pose_au_c', \\\n",
    "                        'gaze_au_r', 'gaze_au_c', \\\n",
    "                        'au_r_au_c', \\\n",
    "                        'pose_gaze_au_r', 'pose_gaze_au_c', 'pose_au_r_au_c', 'gaze_au_r_au_c', \\\n",
    "                        'confidence_success_all', \\\n",
    "                        'confidence_success_AUintens', 'confidence_success_AUoccurr', \\\n",
    "                        'confidence_success_pose','confidence_success_gaze', \\\n",
    "                        'confidence_success_AUintens_pose_gaze', \\\n",
    "                        'confidence_success_AUoccurr_pose_gaze']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "openface_data_path = \"/Users/misha/My_Projects/DAIC_Depression/depression-detection/data/DAIC_openface_features/\"\n",
    "labels_path = \"/Users/misha/My_Projects/DAIC_Depression/depression-detection/data/labels/\"\n",
    "\n",
    "# Read labels\n",
    "train_labels = pd.read_csv(os.path.join(labels_path, 'train_split.csv'))\n",
    "dev_labels = pd.read_csv(os.path.join(labels_path, 'dev_split.csv'))\n",
    "test_labels = pd.read_csv(os.path.join(labels_path, 'test_split.csv'))\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "train_ids = train_labels['Participant_ID'].values\n",
    "dev_ids = dev_labels['Participant_ID'].values\n",
    "test_ids = test_labels['Participant_ID'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PHQDataset at 0x2a2f829f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = os.path.join(openface_data_path, \"train\")\n",
    "dev_data_path = os.path.join(openface_data_path, \"dev\")\n",
    "test_data_path = os.path.join(openface_data_path, \"test\")\n",
    "\n",
    "train_dataset = PHQDataset(os.path.join(labels_path, 'train_split.csv'), train_data_path, feature_type=\"all\")\n",
    "dev_dataset = PHQDataset(os.path.join(labels_path, 'dev_split.csv'), dev_data_path, feature_type=\"all\")\n",
    "test_dataset = PHQDataset(os.path.join(labels_path, 'test_split.csv'), test_data_path, feature_type=\"all\")\n",
    "\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model layers:\n",
      "lstm LSTM(49, 64, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "attention Attention(\n",
      "  (proj): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (context_vector): Linear(in_features=64, out_features=1, bias=False)\n",
      ")\n",
      "fc Linear(in_features=128, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the pretrained model\n",
    "model_path = \"lstm_model_all.pth\"  # Update the path if necessary\n",
    "pretrained_model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "print(\"Model layers:\")\n",
    "for name, layer in pretrained_model.named_children():\n",
    "    print(name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n",
      "Shape of attn_out: torch.Size([1, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m extracted_features\n\u001b[1;32m     31\u001b[0m \u001b[39m# Extract features from train, test, and dev sets\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m train_features \u001b[39m=\u001b[39m extract_features_from_dataset(pretrained_model, train_dataset)\n\u001b[1;32m     33\u001b[0m test_features \u001b[39m=\u001b[39m extract_features_from_dataset(pretrained_model, test_dataset)\n\u001b[1;32m     34\u001b[0m dev_features \u001b[39m=\u001b[39m extract_features_from_dataset(pretrained_model, dev_dataset)\n",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mextract_features_from_dataset\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m features, phq_scores \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m     20\u001b[0m     \u001b[39m# print(\"features: \", features)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39m# print(\"phq_scores: \", phq_scores)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[39m# print(\"i: \", i)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)  \u001b[39m# Add batch dimension if needed\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mget_last_fc_output(features)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()  \u001b[39m# Get output from last fully connected layer\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[39m# print(\"output shape: \", output.shape)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     extracted_features\u001b[39m.\u001b[39mappend(output)\n",
      "Cell \u001b[0;32mIn[2], line 203\u001b[0m, in \u001b[0;36mEnhancedPHQLSTM.get_last_fc_output\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_last_fc_output\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 203\u001b[0m     lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[1;32m    204\u001b[0m     \u001b[39m# print(\"Shape of lstm_out:\", lstm_out.shape)\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(lstm_out)\n",
      "File \u001b[0;32m~/My_Projects/DAIC_Depression/depression-detection/NEW_Experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/My_Projects/DAIC_Depression/depression-detection/NEW_Experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/My_Projects/DAIC_Depression/depression-detection/NEW_Experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:911\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    908\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    910\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    912\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    913\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    915\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming you have already defined your datasets (train_dataset, test_dataset, dev_dataset)\n",
    "\n",
    "# # Function to extract features from a dataset using the pretrained model\n",
    "# def extract_features_from_dataset(model, dataset):\n",
    "#     extracted_features = []\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for features, phq_scores in dataset:\n",
    "#             features = features.unsqueeze(0).to(device)  # Add batch dimension if needed\n",
    "#             output = model(features).squeeze().cpu().numpy()  # Forward pass through the model\n",
    "#             extracted_features.append(output)\n",
    "#     return extracted_features\n",
    "\n",
    "def extract_features_from_dataset(model, dataset):\n",
    "    i = 0\n",
    "    extracted_features = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for features, phq_scores in dataset:\n",
    "            # print(\"features: \", features)\n",
    "            # print(\"phq_scores: \", phq_scores)\n",
    "            # print(\"i: \", i)\n",
    "            features = features.unsqueeze(0).to(device)  # Add batch dimension if needed\n",
    "            output = model.get_last_fc_output(features).squeeze().cpu().numpy()  # Get output from last fully connected layer\n",
    "            # print(\"output shape: \", output.shape)\n",
    "            extracted_features.append(output)\n",
    "            i += 1\n",
    "    return extracted_features\n",
    "\n",
    "\n",
    "# Extract features from train, test, and dev sets\n",
    "train_features = extract_features_from_dataset(pretrained_model, train_dataset)\n",
    "test_features = extract_features_from_dataset(pretrained_model, test_dataset)\n",
    "dev_features = extract_features_from_dataset(pretrained_model, dev_dataset)\n",
    "\n",
    "# Now train_features, test_features, and dev_features contain the extracted features for the respective datasets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
