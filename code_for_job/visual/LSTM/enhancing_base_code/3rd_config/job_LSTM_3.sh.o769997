### Starting TaskPrologue of job 769997 on tg091 at Fri 16 Feb 2024 04:57:07 PM CET
Running on cores 96-127 with governor ondemand
Fri Feb 16 16:57:07 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:C1:00.0 Off |                    0 |
| N/A   32C    P0              51W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
### Finished TaskPrologue

Using device: cuda
feature_type:  all
Traceback (most recent call last):
  File "/home/hpc/empk/empk004h/depression-detection/code_for_job/visual/LSTM/enhancing_base_code/3rd_config/LSTM_enhance.py", line 310, in <module>
    outputs = model(features).squeeze()
              ^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/empk/empk004h/depression-detection/code_for_job/visual/LSTM/enhancing_base_code/3rd_config/LSTM_enhance.py", line 232, in forward
    attn_out = self.attention(lstm_out)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/empk/empk004h/depression-detection/code_for_job/visual/LSTM/enhancing_base_code/3rd_config/LSTM_enhance.py", line 216, in forward
    energy = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.feature_dim) # scaled dot-product attention
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 54.88 GiB. GPU 0 has a total capacity of 39.39 GiB of which 33.49 GiB is free. Including non-PyTorch memory, this process has 5.89 GiB memory in use. Of the allocated memory 3.70 GiB is allocated by PyTorch, and 1.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: tg091: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=769997.0
=== JOB_STATISTICS ===
=== current date     : Fri 16 Feb 2024 04:57:33 PM CET
= Job-ID             : 769997 on tinygpu
= Job-Name           : job_LSTM_3.sh
= Job-Command        : /home/hpc/empk/empk004h/depression-detection/code_for_job/visual/LSTM/enhancing_base_code/3rd_config/job_LSTM_3.sh
= Initial workdir    : /home/hpc/empk/empk004h/depression-detection/code_for_job/visual/LSTM/enhancing_base_code/3rd_config
= Queue/Partition    : a100
= Slurm account      : empk with QOS=normal
= Requested resources: cpu=32,mem=120000M,node=1,billing=32,gres/gpu=1,gres/gpu:a100=1 for 20:00:00
= Elapsed runtime    : 00:00:27
= Total RAM usage    : 0.0 GiB of requested 117 GiB (0.0%)   
= Node list          : tg091
= Subm/Elig/Start/End: 2024-02-16T16:57:06 / 2024-02-16T16:57:06 / 2024-02-16T16:57:06 / 2024-02-16T16:57:33
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           76.5G   104.9G   209.7G        N/A  46,064      500K   1,000K        N/A    
    /home/vault       2899.8G     0.0K     0.0K        N/A      75K     300K     450K        N/A    
    /home/woody        311.0G   500.0G   750.0G        N/A     314K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:C1:00.0, 3618115, 2 %, 0 %, 6100 MiB, 11528 ms
