### Starting TaskPrologue of job 766450 on tg096 at Fri 09 Feb 2024 01:37:46 PM CET
Running on cores 0-31 with governor ondemand
Fri Feb  9 13:37:46 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:01:00.0 Off |                    0 |
| N/A   42C    P0              54W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
### Finished TaskPrologue

Using device: cuda
Epoch [1/20], Step [1/6], Loss: 6.2417
Traceback (most recent call last):
  File "/home/hpc/empk/empk004h/depression-detection/code_for_job/visual/LSTM/base_code/new_LSTM.py", line 127, in <module>
    outputs = model(features).squeeze()
              ^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/empk/empk004h/depression-detection/code_for_job/visual/LSTM/base_code/new_LSTM.py", line 73, in forward
    lstm_out, _ = self.lstm(x)
                  ^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/rnn.py", line 878, in forward
    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.42 GiB. GPU 0 has a total capacity of 39.39 GiB of which 14.20 GiB is free. Including non-PyTorch memory, this process has 25.18 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 10.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: tg096: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=766450.0
=== JOB_STATISTICS ===
=== current date     : Fri 09 Feb 2024 01:38:35 PM CET
= Job-ID             : 766450 on tinygpu
= Job-Name           : job_new_LSTM.sh
= Job-Command        : /home/hpc/empk/empk004h/depression-detection/code_for_job/visual/LSTM/base_code/job_new_LSTM.sh
= Initial workdir    : /home/hpc/empk/empk004h/depression-detection/code_for_job/visual/LSTM/base_code
= Queue/Partition    : a100
= Slurm account      : empk with QOS=normal
= Requested resources: cpu=32,mem=120000M,node=1,billing=32,gres/gpu=1,gres/gpu:a100=1 for 20:00:00
= Elapsed runtime    : 00:00:49
= Total RAM usage    : 1.0 GiB of requested 117 GiB (0.9%)   
= Node list          : tg096
= Subm/Elig/Start/End: 2024-02-09T13:37:45 / 2024-02-09T13:37:45 / 2024-02-09T13:37:46 / 2024-02-09T13:38:35
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           50.4G   104.9G   209.7G        N/A  46,206      500K   1,000K        N/A    
    /home/vault        635.3G     0.0K     0.0K        N/A      74K     300K     450K        N/A    
    /home/woody        311.0G   500.0G   750.0G        N/A     314K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:01:00.0, 595311, 8 %, 1 %, 37880 MiB, 23271 ms
