{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-03 18:26:20.032760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM, AutoPeftModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import DataCollatorWithPadding, AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, GPTQConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "training_data_path = \"/home/hpc/empk/empk004h/depression-detection/data/EDI_deproberta/preprocessed_dataset/train.csv\"\n",
    "validation_data_path = \"/home/hpc/empk/empk004h/depression-detection/data/EDI_deproberta/preprocessed_dataset/dev.csv\"\n",
    "max_memory = f'{79960}MB'\n",
    "\n",
    "training_data = []\n",
    "validation_data = []\n",
    "\n",
    "# load dataset using csv\n",
    "with open(training_data_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate over each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        # Each row is a list of values representing the columns\n",
    "        training_data.append({\"text\": row[1], \"label\": row[2]})\n",
    "\n",
    "    # remove the header\n",
    "    training_data.pop(0)\n",
    "\n",
    "with open(validation_data_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate over each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        # Each row is a list of values representing the columns\n",
    "        validation_data.append({\"text\": row[1], \"label\": row[2]})\n",
    "\n",
    "    # remove the header\n",
    "    validation_data.pop(0)\n",
    "\n",
    "# export them as json files\n",
    "with open('training_data_tmp.json', 'w') as f:\n",
    "    json.dump(training_data, f)\n",
    "\n",
    "with open('validation_data_tmp.json', 'w') as f:\n",
    "    json.dump(validation_data, f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "        token=token\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=token)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max length: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # create a huggingface dataset from dataset (list of dictionnaries)\n",
    "\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    # dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'response' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        # remove_columns=[\"text\", \"label\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    # dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 5029.14it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 38.90it/s]\n",
      "Generating train split: 6006 examples [00:00, 25972.18 examples/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2359.00it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 72.39it/s]\n",
      "Generating train split: 1000 examples [00:00, 28599.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='./training_data_tmp.json')\n",
    "dataset = dataset['train']\n",
    "\n",
    "dataset_val = load_dataset('json', data_files='./validation_data_tmp.json')\n",
    "dataset_val = dataset_val['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': '0',\n",
       " 'text': \"I feel so empty : I just broke up with my boyfriend and I feel nothing, I cried but felt nothing. There was no passion on my side, I miss that passion, that obsession.\\nOnce I gave a guy 5 bucks for him to kiss me, I used to be obsessed with that guy, and I think that if I see him again my feelings will grow again, but he was an asshole, a homeless bipolar schizophrenic asshole, and I loved it. I could relate to him, see his mental struggle. I would tell all the reasons I liked him but that's not the point.\\nThe point is, it's not healthy to be obsessed, I think I just want to have a distraction, something to distract my heart. \\nIdk what to do with my life, I'm doing nothing atm, I don't go to school, I don't have a job, I spend most of my time home (I'm doing exams to do 12th grade but didn't study shit yet) and I'm depressed and lonely and empty. But tbh, I kinda like feeling those things, it makes me feel truly like a human being, if I was a character from a movie or tv show, I think I would be my favorite character.\\nI think I wanna write a book about my life and my struggles but depression doesn't let me, I like to write and I can be good at it, but for some reason, I never ended a single story, why? Idk, maybe i get bored, or lack ideas, maybe I'm just shit at writing.\\nI really think I wanna write a book about my life and as always, that is a distraction but I wanna do it, I need distractions or I'll go crazy, maybe I'll never end it, probably I won't since I started 20 + stories and finished none but I still have faith I'll finish something.\\nBut for that I would need to organize my ideas, with a psychologist, I talked with my mom to come back to one, and I think I want her to help me write my book, to organize my ideas.\\nIdk why I made this post, I think I just wanna hear you guys' thoughts on this and your experiences, idk if anyone will respond but I'll really appreciate if you did.\\nThank you for reading.\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/6006 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6006/6006 [00:01<00:00, 5838.95 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5556.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = dataset_val.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {'0': \"severe\", '1': \"moderate\", '2': \"no-depression\"}\n",
    "label2id = {\"severe\": '0', \"moderate\": '1', \"no-depression\": '2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=32,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 609/609 [00:00<00:00, 1.22MB/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m bnb_config \u001b[39m=\u001b[39m create_bnb_config()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model, tokenizer \u001b[39m=\u001b[39m load_model(model_name, bnb_config)\n",
      "\u001b[1;32m/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(model_name, bnb_config):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     n_gpus \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         model_name,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         quantization_config\u001b[39m=\u001b[39;49mbnb_config,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# dispatch efficiently the model on the available ressources\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         max_memory \u001b[39m=\u001b[39;49m {i: max_memory \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n_gpus)},\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         token\u001b[39m=\u001b[39;49mtoken\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[39m\"\u001b[39m, token\u001b[39m=\u001b[39mtoken)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/EDA_dataset_finetune.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Needed for LLaMA tokenizer\u001b[39;00m\n",
      "File \u001b[0;32m/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/transformers/modeling_utils.py:2714\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2712\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit \u001b[39mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   2713\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_accelerate_available() \u001b[39mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m-> 2714\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   2715\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2716\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2717\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m pip install bitsandbytes` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2718\u001b[0m         )\n\u001b[1;32m   2720\u001b[0m     \u001b[39mif\u001b[39;00m torch_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2721\u001b[0m         \u001b[39m# We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\u001b[39;00m\n\u001b[1;32m   2722\u001b[0m         logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2723\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOverriding torch_dtype=\u001b[39m\u001b[39m{\u001b[39;00mtorch_dtype\u001b[39m}\u001b[39;00m\u001b[39m with `torch_dtype=torch.float16` due to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2724\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2725\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2726\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m torch_dtype=torch.float16 to remove this warning.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2727\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` "
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Get lora module names\n",
    "modules = find_all_linear_names(model)\n",
    "\n",
    "# Create PEFT config for these modules and wrap the model to PEFT\n",
    "peft_config = create_peft_config(modules)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print information about the percentage of trainable parameters\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./textract_data_model_llama_easy_withprompt_1\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    # tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=1),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
