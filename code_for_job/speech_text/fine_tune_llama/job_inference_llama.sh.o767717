### Starting TaskPrologue of job 767717 on tg091 at Mon 12 Feb 2024 11:48:46 PM CET
Running on cores 0-31 with governor ondemand
Mon Feb 12 23:48:46 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:01:00.0 Off |                    0 |
| N/A   40C    P0              55W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
### Finished TaskPrologue

/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 17.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 19.66s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/finetuned_llama_inference.py", line 68, in <module>
    output = llama_inference(texts, model_path, './EDI_finetune_llama_output/checkpoint-15000')
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/finetuned_llama_inference.py", line 46, in llama_inference
    model = PeftModel.from_pretrained(model, llama_adapter_dir)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/peft/peft_model.py", line 332, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/peft/peft_model.py", line 632, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 157, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/woody/empk/empk004h/software/privat/conda/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for PeftModelForSequenceClassification:
	size mismatch for base_model.model.score.modules_to_save.default.weight: copying a param with shape torch.Size([3, 4096]) from checkpoint, the shape in current model is torch.Size([2, 4096]).
srun: error: tg091: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=767717.0
=== JOB_STATISTICS ===
=== current date     : Mon 12 Feb 2024 11:54:05 PM CET
= Job-ID             : 767717 on tinygpu
= Job-Name           : job_inference_llama.sh
= Job-Command        : /home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama/job_inference_llama.sh
= Initial workdir    : /home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/fine_tune_llama
= Queue/Partition    : a100
= Slurm account      : empk with QOS=normal
= Requested resources: cpu=32,mem=120000M,node=1,billing=32,gres/gpu=1,gres/gpu:a100=1 for 20:00:00
= Elapsed runtime    : 00:05:32
= Total RAM usage    : 0.4 GiB of requested 117 GiB (0.3%)   
= Node list          : tg091
= Subm/Elig/Start/End: 2024-02-12T23:05:03 / 2024-02-12T23:05:03 / 2024-02-12T23:48:33 / 2024-02-12T23:54:05
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           76.2G   104.9G   209.7G        N/A  43,434      500K   1,000K        N/A    
    /home/vault        193.8G     0.0K     0.0K        N/A      74K     300K     450K        N/A    
    /home/woody        311.0G   500.0G   750.0G        N/A     314K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:01:00.0, 490029, 1 %, 0 %, 4366 MiB, 51830 ms
