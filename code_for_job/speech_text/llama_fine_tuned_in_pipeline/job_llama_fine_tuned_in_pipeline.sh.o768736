### Starting TaskPrologue of job 768736 on tg093 at Wed 14 Feb 2024 01:43:37 PM CET
Running on cores 64-95 with governor ondemand
Wed Feb 14 13:43:37 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:81:00.0 Off |                    0 |
| N/A   33C    P0              57W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
### Finished TaskPrologue

/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline
def_dev:      id  ...                                               text
0  300  ...   which will record your body. So I'll show you...
1  301  ...   Yeah, there's all sorts of different studies ...
2  306  ...   Okay, looks like we're good. But let's move a...
3  317  ...   Okay. How long is this? This is probably goin...
4  320  ...   Okay, everything looks good. Okay. Perfect. O...

[5 rows x 7 columns]
max PHQ score in df_train:  23
prompt_number:  1
prompt:   Your task is to read the following text which is an interview with a person and to summarize the key points that might be related to the depression of the person. Be concise and to the point.
prompt_number:  2
prompt:   Your task is to read the following text which is an interview with a person and to summarize the key points that might be related to the depression of the person. Be concise and to the point. It is very essential that you write your answer in the first-person perspective, as if the interviewee is narrating about himself or herself. 
prompt_number:  3
prompt:   After reading the interview, briefly summarize the main aspects that pertain to the person's depression. 
prompt_number:  4
prompt:   Based on the interview, highlight the key factors that might be indicative of the interviewee's depression. 
prompt_number:  5
prompt:   Your task is to summarize the interviewee's main points that could be linked to their depression. Keep it concise. 
prompt_number:  6
prompt:   After reading the interview, identify and summarize the main challenges or difficulties the interviewee faces that are indicative of depression. 
prompt_number:  7
prompt:   Based on the interview, provide a concise analysis of the interviewee's emotional state and behaviors that may indicate the presence of depression. 
prompt_number:  8
prompt:   Read the interview carefully and extract the most significant indicators of depression exhibited by the interviewee. Summarize them concisely. 
prompt_number:  9
prompt:   Your task is to analyze the interviewee's responses and highlight the key signs or symptoms of depression that are evident in the interview. 
prompt_number:  10
prompt:   Provide a brief summary of the interview, focusing on aspects that strongly suggest the presence of depression in the interviewee. 
Extracting Features for Prompt  1  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 15.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 17.42s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-02-14 13:44:35.674083: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
output:  [array([2.334e-04, 6.616e-06, 1.000e+00], dtype=float16), array([7.284e-05, 9.894e-06, 1.000e+00], dtype=float16), array([5.884e-04, 8.512e-05, 9.995e-01], dtype=float16), array([4.035e-05, 1.264e-05, 1.000e+00], dtype=float16), array([3.605e-04, 2.897e-05, 9.995e-01], dtype=float16), array([1.768e-04, 2.289e-05, 1.000e+00], dtype=float16), array([2.156e-04, 2.146e-06, 1.000e+00], dtype=float16), array([5.066e-05, 2.027e-06, 1.000e+00], dtype=float16), array([6.074e-05, 1.431e-05, 1.000e+00], dtype=float16), array([9.97e-04, 2.19e-05, 9.99e-01], dtype=float16), array([1.526e-05, 2.563e-06, 1.000e+00], dtype=float16), array([0.002497, 0.01338 , 0.984   ], dtype=float16), array([0.01897 , 0.011734, 0.969   ], dtype=float16), array([2.165e-04, 1.093e-04, 9.995e-01], dtype=float16), array([3.059e-04, 2.092e-05, 9.995e-01], dtype=float16), array([1.173e-04, 6.557e-07, 1.000e+00], dtype=float16), array([2.323e-04, 2.044e-05, 9.995e-01], dtype=float16), array([1.683e-04, 1.490e-06, 1.000e+00], dtype=float16), array([2.07e-05, 7.03e-06, 1.00e+00], dtype=float16), array([5.031e-04, 2.068e-05, 9.995e-01], dtype=float16), array([2.011e-04, 1.049e-05, 1.000e+00], dtype=float16), array([4.79e-05, 4.35e-06, 1.00e+00], dtype=float16), array([3.606e-05, 5.662e-06, 1.000e+00], dtype=float16), array([8.094e-05, 8.881e-06, 1.000e+00], dtype=float16), array([4.023e-05, 3.821e-05, 1.000e+00], dtype=float16), array([1.778e-02, 3.273e-04, 9.819e-01], dtype=float16), array([1.895e-05, 5.424e-06, 1.000e+00], dtype=float16), array([1.108e-04, 6.658e-05, 1.000e+00], dtype=float16), array([6.218e-04, 2.700e-01, 7.295e-01], dtype=float16), array([1.56e-05, 7.15e-07, 1.00e+00], dtype=float16), array([5.469e-04, 1.508e-05, 9.995e-01], dtype=float16), array([5.7e-06, 5.4e-07, 1.0e+00], dtype=float16), array([5.364e-04, 2.697e-03, 9.966e-01], dtype=float16), array([1.342e-04, 9.537e-07, 1.000e+00], dtype=float16), array([2.475e-04, 1.252e-06, 9.995e-01], dtype=float16), array([2.074e-04, 2.527e-05, 1.000e+00], dtype=float16), array([0.1614 , 0.8228 , 0.01598], dtype=float16), array([4.215e-03, 2.176e-05, 9.956e-01], dtype=float16), array([1.066e-03, 6.776e-04, 9.980e-01], dtype=float16), array([9.66e-06, 2.38e-07, 1.00e+00], dtype=float16), array([5.999e-04, 1.297e-04, 9.995e-01], dtype=float16), array([2.763e-04, 1.192e-05, 9.995e-01], dtype=float16), array([7.963e-04, 2.420e-05, 9.990e-01], dtype=float16), array([2.486e-05, 2.801e-06, 1.000e+00], dtype=float16), array([1.105e-03, 3.451e-05, 9.990e-01], dtype=float16), array([7.086e-04, 2.503e-06, 9.995e-01], dtype=float16), array([4.756e-05, 1.609e-06, 1.000e+00], dtype=float16), array([5.274e-04, 2.190e-03, 9.971e-01], dtype=float16), array([3.e-07, 2.e-07, 1.e+00], dtype=float16), array([3.626e-04, 5.543e-06, 9.995e-01], dtype=float16), array([0.3374 , 0.652  , 0.01094], dtype=float16), array([0.001812, 0.007965, 0.99    ], dtype=float16), array([0.01522, 0.01999, 0.965  ], dtype=float16), array([1.0805e-03, 8.8358e-04, 9.9805e-01], dtype=float16), array([1.074e-04, 2.646e-05, 1.000e+00], dtype=float16), array([1.693e-04, 1.913e-05, 1.000e+00], dtype=float16), array([0.010025, 0.03513 , 0.955   ], dtype=float16), array([1.28e-05, 1.13e-06, 1.00e+00], dtype=float16), array([5.474e-04, 5.603e-06, 9.995e-01], dtype=float16), array([2.247e-04, 6.258e-06, 1.000e+00], dtype=float16), array([1.503e-04, 2.503e-06, 1.000e+00], dtype=float16), array([0.02725  , 0.0014515, 0.971    ], dtype=float16), array([0.06223, 0.05145, 0.886  ], dtype=float16), array([2.937e-04, 2.265e-05, 9.995e-01], dtype=float16), array([1.913e-05, 5.960e-06, 1.000e+00], dtype=float16), array([4.83e-06, 3.04e-06, 1.00e+00], dtype=float16), array([1.157e-04, 9.656e-06, 1.000e+00], dtype=float16), array([0.001046, 0.003887, 0.995   ], dtype=float16), array([0.001772, 0.001984, 0.996   ], dtype=float16), array([8.3e-06, 1.5e-06, 1.0e+00], dtype=float16), array([1.158e-02, 5.990e-05, 9.883e-01], dtype=float16), array([2.295e-05, 8.583e-06, 1.000e+00], dtype=float16), array([1.497e-04, 2.265e-06, 1.000e+00], dtype=float16), array([2.44e-06, 4.17e-07, 1.00e+00], dtype=float16), array([1.44e-04, 7.39e-06, 1.00e+00], dtype=float16), array([9.06e-06, 4.41e-06, 1.00e+00], dtype=float16), array([1.690e-04, 1.025e-05, 1.000e+00], dtype=float16), array([3.145e-03, 8.888e-04, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5763e-05, 1.0000e+00], dtype=float16), array([0.08386 , 0.003172, 0.913   ], dtype=float16), array([1.72e-05, 1.31e-06, 1.00e+00], dtype=float16), array([7.546e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.072e-04, 4.351e-06, 1.000e+00], dtype=float16), array([3.386e-05, 2.027e-06, 1.000e+00], dtype=float16), array([0.01016, 0.00961, 0.9805 ], dtype=float16), array([7.45e-06, 2.21e-06, 1.00e+00], dtype=float16), array([3.22e-05, 7.75e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.431e-06, 1.000e+00], dtype=float16), array([1.034e-03, 2.813e-05, 9.990e-01], dtype=float16), array([1.276e-04, 3.994e-06, 1.000e+00], dtype=float16), array([2.63e-05, 1.07e-06, 1.00e+00], dtype=float16), array([1.298e-04, 1.729e-06, 1.000e+00], dtype=float16), array([1.746e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.569e-03, 7.515e-04, 9.976e-01], dtype=float16), array([3.433e-05, 3.576e-07, 1.000e+00], dtype=float16), array([0.0686, 0.0521, 0.8794], dtype=float16), array([4.053e-05, 2.503e-06, 1.000e+00], dtype=float16), array([1.0073e-04, 1.6749e-05, 1.0000e+00], dtype=float16), array([2.393e-04, 8.076e-05, 9.995e-01], dtype=float16), array([3.419e-04, 6.604e-05, 9.995e-01], dtype=float16), array([2.02e-05, 1.25e-06, 1.00e+00], dtype=float16), array([3.054e-04, 1.168e-05, 9.995e-01], dtype=float16), array([4.685e-05, 3.576e-06, 1.000e+00], dtype=float16), array([6.71e-05, 8.61e-05, 1.00e+00], dtype=float16), array([2.260e-04, 8.482e-05, 9.995e-01], dtype=float16), array([2.627e-04, 1.913e-05, 9.995e-01], dtype=float16), array([2.68e-05, 2.80e-06, 1.00e+00], dtype=float16), array([7.86e-05, 2.62e-06, 1.00e+00], dtype=float16), array([1.18e-05, 1.73e-06, 1.00e+00], dtype=float16), array([8.04e-04, 2.38e-05, 9.99e-01], dtype=float16), array([5.65e-05, 9.48e-06, 1.00e+00], dtype=float16), array([1.878e-05, 1.097e-05, 1.000e+00], dtype=float16), array([6.194e-04, 2.408e-05, 9.995e-01], dtype=float16), array([4.189e-04, 8.702e-06, 9.995e-01], dtype=float16), array([0.0117  , 0.001235, 0.987   ], dtype=float16), array([6.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([1.264e-05, 1.788e-07, 1.000e+00], dtype=float16), array([4.4e-06, 7.2e-07, 1.0e+00], dtype=float16), array([1.406e-03, 5.823e-05, 9.985e-01], dtype=float16), array([1.53e-04, 1.81e-05, 1.00e+00], dtype=float16), array([2.307e-05, 1.073e-06, 1.000e+00], dtype=float16), array([3.982e-05, 2.986e-05, 1.000e+00], dtype=float16), array([4.27e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.707e-03, 1.639e-04, 9.951e-01], dtype=float16), array([2.601e-04, 2.533e-05, 9.995e-01], dtype=float16), array([3.867e-04, 1.615e-04, 9.995e-01], dtype=float16), array([4.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([4.15e-05, 5.60e-06, 1.00e+00], dtype=float16), array([3.011e-04, 1.782e-05, 9.995e-01], dtype=float16), array([8.e-05, 8.e-07, 1.e+00], dtype=float16), array([2.542e-04, 2.211e-05, 9.995e-01], dtype=float16), array([1.434e-04, 6.974e-06, 1.000e+00], dtype=float16), array([5.064e-04, 4.345e-05, 9.995e-01], dtype=float16), array([3.574e-04, 1.860e-05, 9.995e-01], dtype=float16), array([1.6e-04, 5.4e-07, 1.0e+00], dtype=float16), array([1.5335e-03, 1.2469e-04, 9.9854e-01], dtype=float16), array([1.79e-05, 6.56e-07, 1.00e+00], dtype=float16), array([1.777e-04, 1.907e-06, 1.000e+00], dtype=float16), array([2.797e-04, 1.901e-05, 9.995e-01], dtype=float16), array([1.432e-03, 2.152e-05, 9.985e-01], dtype=float16), array([2.87e-05, 2.86e-06, 1.00e+00], dtype=float16), array([9.19e-05, 3.87e-06, 1.00e+00], dtype=float16), array([8.58e-06, 3.34e-06, 1.00e+00], dtype=float16), array([2.927e-05, 7.153e-07, 1.000e+00], dtype=float16), array([0.00378, 0.00596, 0.99   ], dtype=float16), array([7.067e-04, 2.325e-05, 9.995e-01], dtype=float16), array([5.037e-05, 1.556e-05, 1.000e+00], dtype=float16), array([0.0634, 0.5986, 0.338 ], dtype=float16), array([0.007  , 0.12024, 0.8726 ], dtype=float16), array([7.254e-05, 2.205e-06, 1.000e+00], dtype=float16), array([5.54e-06, 2.68e-06, 1.00e+00], dtype=float16), array([7.933e-05, 2.086e-06, 1.000e+00], dtype=float16), array([3.58e-05, 2.38e-07, 1.00e+00], dtype=float16), array([3.87e-05, 3.81e-06, 1.00e+00], dtype=float16), array([1.118e-03, 9.942e-05, 9.990e-01], dtype=float16), array([2.378e-01, 3.459e-04, 7.617e-01], dtype=float16), array([3.181e-03, 8.285e-06, 9.966e-01], dtype=float16), array([1.843e-03, 8.345e-07, 9.980e-01], dtype=float16), array([4.010e-02, 4.661e-04, 9.595e-01], dtype=float16), array([2.02e-05, 2.15e-06, 1.00e+00], dtype=float16), array([3.191e-03, 6.872e-05, 9.966e-01], dtype=float16), array([0.00827 , 0.001407, 0.99    ], dtype=float16), array([7.367e-05, 1.645e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.29e-05, 2.38e-07, 1.00e+00], dtype=float16), array([2.474e-05, 8.523e-06, 1.000e+00], dtype=float16), array([1.32e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.816e-05, 9.656e-06, 1.000e+00], dtype=float16), array([7.067e-04, 1.127e-05, 9.995e-01], dtype=float16), array([5.760e-04, 3.517e-06, 9.995e-01], dtype=float16), array([2.165e-03, 2.718e-05, 9.980e-01], dtype=float16), array([3.684e-05, 1.121e-05, 1.000e+00], dtype=float16), array([7.734e-04, 4.232e-06, 9.990e-01], dtype=float16), array([8.583e-05, 5.722e-06, 1.000e+00], dtype=float16), array([2.384e-05, 1.609e-06, 1.000e+00], dtype=float16), array([1.35e-04, 4.85e-05, 1.00e+00], dtype=float16), array([2.174e-04, 3.517e-06, 1.000e+00], dtype=float16), array([1.041e-04, 5.484e-06, 1.000e+00], dtype=float16), array([2.241e-04, 4.172e-07, 1.000e+00], dtype=float16), array([3.779e-05, 4.196e-05, 1.000e+00], dtype=float16), array([1.498e-03, 3.121e-04, 9.980e-01], dtype=float16), array([1.59e-05, 9.72e-06, 1.00e+00], dtype=float16), array([1.1325e-04, 3.4928e-05, 1.0000e+00], dtype=float16), array([9.03e-05, 3.24e-05, 1.00e+00], dtype=float16), array([5.679e-04, 5.841e-06, 9.995e-01], dtype=float16), array([3.264e-04, 1.270e-05, 9.995e-01], dtype=float16), array([0.03882, 0.2043 , 0.757  ], dtype=float16), array([2.74e-06, 8.94e-07, 1.00e+00], dtype=float16), array([6.952e-04, 9.918e-04, 9.985e-01], dtype=float16), array([2.426e-05, 2.980e-07, 1.000e+00], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([5.4e-05, 7.1e-06, 1.0e+00], dtype=float16), array([7.668e-04, 2.587e-05, 9.990e-01], dtype=float16), array([2.26e-06, 2.98e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.848e-05, 1.000e+00], dtype=float16), array([3.386e-04, 4.965e-05, 9.995e-01], dtype=float16), array([8.135e-04, 8.345e-05, 9.990e-01], dtype=float16), array([1.472e-02, 5.925e-05, 9.854e-01], dtype=float16), array([7.34e-05, 4.98e-05, 1.00e+00], dtype=float16), array([1.70e-04, 9.83e-06, 1.00e+00], dtype=float16), array([1.053e-04, 2.092e-05, 1.000e+00], dtype=float16), array([7.33e-06, 1.85e-06, 1.00e+00], dtype=float16), array([4.49e-05, 3.52e-06, 1.00e+00], dtype=float16), array([5.012e-04, 6.258e-06, 9.995e-01], dtype=float16), array([3.672e-05, 1.365e-05, 1.000e+00], dtype=float16), array([4.9e-06, 3.6e-07, 1.0e+00], dtype=float16), array([2.03e-05, 6.56e-06, 1.00e+00], dtype=float16), array([1.915e-02, 5.298e-04, 9.805e-01], dtype=float16), array([2.867e-03, 1.121e-05, 9.971e-01], dtype=float16), array([7.486e-05, 4.947e-05, 1.000e+00], dtype=float16), array([2.209e-03, 1.612e-04, 9.976e-01], dtype=float16), array([3.169e-04, 6.801e-05, 9.995e-01], dtype=float16), array([2.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([6.23e-05, 1.91e-06, 1.00e+00], dtype=float16), array([1.491e-04, 4.768e-07, 1.000e+00], dtype=float16), array([2.277e-05, 6.557e-07, 1.000e+00], dtype=float16), array([1.4e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.989e-03, 1.490e-04, 9.927e-01], dtype=float16), array([8.15e-04, 9.18e-06, 9.99e-01], dtype=float16), array([7.615e-04, 5.895e-05, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.215, 0.039, 0.746], dtype=float16), array([6.74e-06, 1.19e-07, 1.00e+00], dtype=float16), array([6.658e-05, 2.843e-05, 1.000e+00], dtype=float16), array([1.1635e-04, 7.8678e-06, 1.0000e+00], dtype=float16), array([9.146e-04, 1.371e-05, 9.990e-01], dtype=float16), array([1.248e-04, 1.252e-06, 1.000e+00], dtype=float16), array([0.0109 , 0.02061, 0.9683 ], dtype=float16), array([7.224e-05, 8.059e-05, 1.000e+00], dtype=float16), array([9.459e-05, 1.234e-05, 1.000e+00], dtype=float16), array([4.116e-03, 2.037e-04, 9.956e-01], dtype=float16), array([1.35e-05, 4.77e-07, 1.00e+00], dtype=float16), array([2.080e-05, 1.264e-05, 1.000e+00], dtype=float16), array([1.6680e-03, 1.1754e-04, 9.9805e-01], dtype=float16), array([0.11176, 0.672  , 0.2163 ], dtype=float16), array([1.788e-04, 1.782e-05, 1.000e+00], dtype=float16), array([7.87e-06, 4.17e-07, 1.00e+00], dtype=float16), array([4.3e-05, 1.0e-06, 1.0e+00], dtype=float16), array([3.034e-05, 2.444e-06, 1.000e+00], dtype=float16), array([7.51e-06, 2.26e-06, 1.00e+00], dtype=float16), array([0.002886, 0.015076, 0.982   ], dtype=float16), array([5.215e-03, 2.766e-04, 9.946e-01], dtype=float16), array([9.209e-03, 5.764e-05, 9.907e-01], dtype=float16), array([8.988e-04, 2.664e-05, 9.990e-01], dtype=float16), array([1.205e-04, 1.365e-05, 1.000e+00], dtype=float16), array([0.0556  , 0.001326, 0.943   ], dtype=float16), array([3.641e-04, 7.570e-06, 9.995e-01], dtype=float16), array([1.881e-04, 1.073e-06, 1.000e+00], dtype=float16), array([8.459e-04, 3.517e-05, 9.990e-01], dtype=float16), array([3.32e-05, 1.49e-06, 1.00e+00], dtype=float16), array([2.60e-05, 1.33e-05, 1.00e+00], dtype=float16), array([3.464e-03, 4.786e-05, 9.966e-01], dtype=float16), array([1.325e-03, 1.026e-04, 9.985e-01], dtype=float16), array([3.838e-03, 2.323e-04, 9.961e-01], dtype=float16), array([2.84e-05, 3.70e-06, 1.00e+00], dtype=float16), array([8.976e-05, 7.629e-06, 1.000e+00], dtype=float16), array([3.951e-04, 1.788e-06, 9.995e-01], dtype=float16), array([3.440e-04, 1.800e-05, 9.995e-01], dtype=float16), array([6.026e-05, 1.073e-06, 1.000e+00], dtype=float16), array([5.584e-04, 3.493e-05, 9.995e-01], dtype=float16), array([3.564e-05, 1.013e-05, 1.000e+00], dtype=float16), array([1.6775e-03, 7.7367e-05, 9.9805e-01], dtype=float16), array([6.48e-05, 1.19e-06, 1.00e+00], dtype=float16), array([1.396e-04, 9.298e-06, 1.000e+00], dtype=float16), array([1.960e-04, 1.388e-04, 9.995e-01], dtype=float16), array([3.340e-04, 5.203e-05, 9.995e-01], dtype=float16), array([2.670e-05, 2.737e-03, 9.971e-01], dtype=float16), array([3.066e-04, 1.121e-05, 9.995e-01], dtype=float16), array([3.440e-04, 2.533e-05, 9.995e-01], dtype=float16), array([5.19e-05, 4.10e-05, 1.00e+00], dtype=float16), array([3.21e-05, 4.17e-07, 1.00e+00], dtype=float16), array([2.68e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.89e-05, 2.80e-06, 1.00e+00], dtype=float16), array([0.003986, 0.00123 , 0.9946  ], dtype=float16), array([5.444e-03, 2.283e-05, 9.946e-01], dtype=float16), array([3.312e-04, 1.562e-05, 9.995e-01], dtype=float16), array([7.2e-05, 3.8e-06, 1.0e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  1
---------------------------------------------------
Extracting Features for Prompt  2  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.334e-04, 6.616e-06, 1.000e+00], dtype=float16), array([7.284e-05, 9.894e-06, 1.000e+00], dtype=float16), array([5.884e-04, 8.512e-05, 9.995e-01], dtype=float16), array([4.035e-05, 1.264e-05, 1.000e+00], dtype=float16), array([3.605e-04, 2.897e-05, 9.995e-01], dtype=float16), array([1.768e-04, 2.289e-05, 1.000e+00], dtype=float16), array([2.156e-04, 2.146e-06, 1.000e+00], dtype=float16), array([5.066e-05, 2.027e-06, 1.000e+00], dtype=float16), array([6.074e-05, 1.431e-05, 1.000e+00], dtype=float16), array([9.97e-04, 2.19e-05, 9.99e-01], dtype=float16), array([1.526e-05, 2.563e-06, 1.000e+00], dtype=float16), array([0.002497, 0.01338 , 0.984   ], dtype=float16), array([0.01897 , 0.011734, 0.969   ], dtype=float16), array([2.165e-04, 1.093e-04, 9.995e-01], dtype=float16), array([3.059e-04, 2.092e-05, 9.995e-01], dtype=float16), array([1.173e-04, 6.557e-07, 1.000e+00], dtype=float16), array([2.323e-04, 2.044e-05, 9.995e-01], dtype=float16), array([1.683e-04, 1.490e-06, 1.000e+00], dtype=float16), array([2.07e-05, 7.03e-06, 1.00e+00], dtype=float16), array([5.031e-04, 2.068e-05, 9.995e-01], dtype=float16), array([2.011e-04, 1.049e-05, 1.000e+00], dtype=float16), array([4.79e-05, 4.35e-06, 1.00e+00], dtype=float16), array([3.606e-05, 5.662e-06, 1.000e+00], dtype=float16), array([8.094e-05, 8.881e-06, 1.000e+00], dtype=float16), array([4.023e-05, 3.821e-05, 1.000e+00], dtype=float16), array([1.778e-02, 3.273e-04, 9.819e-01], dtype=float16), array([1.895e-05, 5.424e-06, 1.000e+00], dtype=float16), array([1.108e-04, 6.658e-05, 1.000e+00], dtype=float16), array([6.218e-04, 2.700e-01, 7.295e-01], dtype=float16), array([1.56e-05, 7.15e-07, 1.00e+00], dtype=float16), array([5.469e-04, 1.508e-05, 9.995e-01], dtype=float16), array([5.7e-06, 5.4e-07, 1.0e+00], dtype=float16), array([5.364e-04, 2.697e-03, 9.966e-01], dtype=float16), array([1.342e-04, 9.537e-07, 1.000e+00], dtype=float16), array([2.475e-04, 1.252e-06, 9.995e-01], dtype=float16), array([2.074e-04, 2.527e-05, 1.000e+00], dtype=float16), array([0.1614 , 0.8228 , 0.01598], dtype=float16), array([4.215e-03, 2.176e-05, 9.956e-01], dtype=float16), array([1.066e-03, 6.776e-04, 9.980e-01], dtype=float16), array([9.66e-06, 2.38e-07, 1.00e+00], dtype=float16), array([5.999e-04, 1.297e-04, 9.995e-01], dtype=float16), array([2.763e-04, 1.192e-05, 9.995e-01], dtype=float16), array([7.963e-04, 2.420e-05, 9.990e-01], dtype=float16), array([2.486e-05, 2.801e-06, 1.000e+00], dtype=float16), array([1.105e-03, 3.451e-05, 9.990e-01], dtype=float16), array([7.086e-04, 2.503e-06, 9.995e-01], dtype=float16), array([4.756e-05, 1.609e-06, 1.000e+00], dtype=float16), array([5.274e-04, 2.190e-03, 9.971e-01], dtype=float16), array([3.e-07, 2.e-07, 1.e+00], dtype=float16), array([3.626e-04, 5.543e-06, 9.995e-01], dtype=float16), array([0.3374 , 0.652  , 0.01094], dtype=float16), array([0.001812, 0.007965, 0.99    ], dtype=float16), array([0.01522, 0.01999, 0.965  ], dtype=float16), array([1.0805e-03, 8.8358e-04, 9.9805e-01], dtype=float16), array([1.074e-04, 2.646e-05, 1.000e+00], dtype=float16), array([1.693e-04, 1.913e-05, 1.000e+00], dtype=float16), array([0.010025, 0.03513 , 0.955   ], dtype=float16), array([1.28e-05, 1.13e-06, 1.00e+00], dtype=float16), array([5.474e-04, 5.603e-06, 9.995e-01], dtype=float16), array([2.247e-04, 6.258e-06, 1.000e+00], dtype=float16), array([1.503e-04, 2.503e-06, 1.000e+00], dtype=float16), array([0.02725  , 0.0014515, 0.971    ], dtype=float16), array([0.06223, 0.05145, 0.886  ], dtype=float16), array([2.937e-04, 2.265e-05, 9.995e-01], dtype=float16), array([1.913e-05, 5.960e-06, 1.000e+00], dtype=float16), array([4.83e-06, 3.04e-06, 1.00e+00], dtype=float16), array([1.157e-04, 9.656e-06, 1.000e+00], dtype=float16), array([0.001046, 0.003887, 0.995   ], dtype=float16), array([0.001772, 0.001984, 0.996   ], dtype=float16), array([8.3e-06, 1.5e-06, 1.0e+00], dtype=float16), array([1.158e-02, 5.990e-05, 9.883e-01], dtype=float16), array([2.295e-05, 8.583e-06, 1.000e+00], dtype=float16), array([1.497e-04, 2.265e-06, 1.000e+00], dtype=float16), array([2.44e-06, 4.17e-07, 1.00e+00], dtype=float16), array([1.44e-04, 7.39e-06, 1.00e+00], dtype=float16), array([9.06e-06, 4.41e-06, 1.00e+00], dtype=float16), array([1.690e-04, 1.025e-05, 1.000e+00], dtype=float16), array([3.145e-03, 8.888e-04, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5763e-05, 1.0000e+00], dtype=float16), array([0.08386 , 0.003172, 0.913   ], dtype=float16), array([1.72e-05, 1.31e-06, 1.00e+00], dtype=float16), array([7.546e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.072e-04, 4.351e-06, 1.000e+00], dtype=float16), array([3.386e-05, 2.027e-06, 1.000e+00], dtype=float16), array([0.01016, 0.00961, 0.9805 ], dtype=float16), array([7.45e-06, 2.21e-06, 1.00e+00], dtype=float16), array([3.22e-05, 7.75e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.431e-06, 1.000e+00], dtype=float16), array([1.034e-03, 2.813e-05, 9.990e-01], dtype=float16), array([1.276e-04, 3.994e-06, 1.000e+00], dtype=float16), array([2.63e-05, 1.07e-06, 1.00e+00], dtype=float16), array([1.298e-04, 1.729e-06, 1.000e+00], dtype=float16), array([1.746e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.569e-03, 7.515e-04, 9.976e-01], dtype=float16), array([3.433e-05, 3.576e-07, 1.000e+00], dtype=float16), array([0.0686, 0.0521, 0.8794], dtype=float16), array([4.053e-05, 2.503e-06, 1.000e+00], dtype=float16), array([1.0073e-04, 1.6749e-05, 1.0000e+00], dtype=float16), array([2.393e-04, 8.076e-05, 9.995e-01], dtype=float16), array([3.419e-04, 6.604e-05, 9.995e-01], dtype=float16), array([2.02e-05, 1.25e-06, 1.00e+00], dtype=float16), array([3.054e-04, 1.168e-05, 9.995e-01], dtype=float16), array([4.685e-05, 3.576e-06, 1.000e+00], dtype=float16), array([6.71e-05, 8.61e-05, 1.00e+00], dtype=float16), array([2.260e-04, 8.482e-05, 9.995e-01], dtype=float16), array([2.627e-04, 1.913e-05, 9.995e-01], dtype=float16), array([2.68e-05, 2.80e-06, 1.00e+00], dtype=float16), array([7.86e-05, 2.62e-06, 1.00e+00], dtype=float16), array([1.18e-05, 1.73e-06, 1.00e+00], dtype=float16), array([8.04e-04, 2.38e-05, 9.99e-01], dtype=float16), array([5.65e-05, 9.48e-06, 1.00e+00], dtype=float16), array([1.878e-05, 1.097e-05, 1.000e+00], dtype=float16), array([6.194e-04, 2.408e-05, 9.995e-01], dtype=float16), array([4.189e-04, 8.702e-06, 9.995e-01], dtype=float16), array([0.0117  , 0.001235, 0.987   ], dtype=float16), array([6.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([1.264e-05, 1.788e-07, 1.000e+00], dtype=float16), array([4.4e-06, 7.2e-07, 1.0e+00], dtype=float16), array([1.406e-03, 5.823e-05, 9.985e-01], dtype=float16), array([1.53e-04, 1.81e-05, 1.00e+00], dtype=float16), array([2.307e-05, 1.073e-06, 1.000e+00], dtype=float16), array([3.982e-05, 2.986e-05, 1.000e+00], dtype=float16), array([4.27e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.707e-03, 1.639e-04, 9.951e-01], dtype=float16), array([2.601e-04, 2.533e-05, 9.995e-01], dtype=float16), array([3.867e-04, 1.615e-04, 9.995e-01], dtype=float16), array([4.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([4.15e-05, 5.60e-06, 1.00e+00], dtype=float16), array([3.011e-04, 1.782e-05, 9.995e-01], dtype=float16), array([8.e-05, 8.e-07, 1.e+00], dtype=float16), array([2.542e-04, 2.211e-05, 9.995e-01], dtype=float16), array([1.434e-04, 6.974e-06, 1.000e+00], dtype=float16), array([5.064e-04, 4.345e-05, 9.995e-01], dtype=float16), array([3.574e-04, 1.860e-05, 9.995e-01], dtype=float16), array([1.6e-04, 5.4e-07, 1.0e+00], dtype=float16), array([1.5335e-03, 1.2469e-04, 9.9854e-01], dtype=float16), array([1.79e-05, 6.56e-07, 1.00e+00], dtype=float16), array([1.777e-04, 1.907e-06, 1.000e+00], dtype=float16), array([2.797e-04, 1.901e-05, 9.995e-01], dtype=float16), array([1.432e-03, 2.152e-05, 9.985e-01], dtype=float16), array([2.87e-05, 2.86e-06, 1.00e+00], dtype=float16), array([9.19e-05, 3.87e-06, 1.00e+00], dtype=float16), array([8.58e-06, 3.34e-06, 1.00e+00], dtype=float16), array([2.927e-05, 7.153e-07, 1.000e+00], dtype=float16), array([0.00378, 0.00596, 0.99   ], dtype=float16), array([7.067e-04, 2.325e-05, 9.995e-01], dtype=float16), array([5.037e-05, 1.556e-05, 1.000e+00], dtype=float16), array([0.0634, 0.5986, 0.338 ], dtype=float16), array([0.007  , 0.12024, 0.8726 ], dtype=float16), array([7.254e-05, 2.205e-06, 1.000e+00], dtype=float16), array([5.54e-06, 2.68e-06, 1.00e+00], dtype=float16), array([7.933e-05, 2.086e-06, 1.000e+00], dtype=float16), array([3.58e-05, 2.38e-07, 1.00e+00], dtype=float16), array([3.87e-05, 3.81e-06, 1.00e+00], dtype=float16), array([1.118e-03, 9.942e-05, 9.990e-01], dtype=float16), array([2.378e-01, 3.459e-04, 7.617e-01], dtype=float16), array([3.181e-03, 8.285e-06, 9.966e-01], dtype=float16), array([1.843e-03, 8.345e-07, 9.980e-01], dtype=float16), array([4.010e-02, 4.661e-04, 9.595e-01], dtype=float16), array([2.02e-05, 2.15e-06, 1.00e+00], dtype=float16), array([3.191e-03, 6.872e-05, 9.966e-01], dtype=float16), array([0.00827 , 0.001407, 0.99    ], dtype=float16), array([7.367e-05, 1.645e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.29e-05, 2.38e-07, 1.00e+00], dtype=float16), array([2.474e-05, 8.523e-06, 1.000e+00], dtype=float16), array([1.32e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.816e-05, 9.656e-06, 1.000e+00], dtype=float16), array([7.067e-04, 1.127e-05, 9.995e-01], dtype=float16), array([5.760e-04, 3.517e-06, 9.995e-01], dtype=float16), array([2.165e-03, 2.718e-05, 9.980e-01], dtype=float16), array([3.684e-05, 1.121e-05, 1.000e+00], dtype=float16), array([7.734e-04, 4.232e-06, 9.990e-01], dtype=float16), array([8.583e-05, 5.722e-06, 1.000e+00], dtype=float16), array([2.384e-05, 1.609e-06, 1.000e+00], dtype=float16), array([1.35e-04, 4.85e-05, 1.00e+00], dtype=float16), array([2.174e-04, 3.517e-06, 1.000e+00], dtype=float16), array([1.041e-04, 5.484e-06, 1.000e+00], dtype=float16), array([2.241e-04, 4.172e-07, 1.000e+00], dtype=float16), array([3.779e-05, 4.196e-05, 1.000e+00], dtype=float16), array([1.498e-03, 3.121e-04, 9.980e-01], dtype=float16), array([1.59e-05, 9.72e-06, 1.00e+00], dtype=float16), array([1.1325e-04, 3.4928e-05, 1.0000e+00], dtype=float16), array([9.03e-05, 3.24e-05, 1.00e+00], dtype=float16), array([5.679e-04, 5.841e-06, 9.995e-01], dtype=float16), array([3.264e-04, 1.270e-05, 9.995e-01], dtype=float16), array([0.03882, 0.2043 , 0.757  ], dtype=float16), array([2.74e-06, 8.94e-07, 1.00e+00], dtype=float16), array([6.952e-04, 9.918e-04, 9.985e-01], dtype=float16), array([2.426e-05, 2.980e-07, 1.000e+00], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([5.4e-05, 7.1e-06, 1.0e+00], dtype=float16), array([7.668e-04, 2.587e-05, 9.990e-01], dtype=float16), array([2.26e-06, 2.98e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.848e-05, 1.000e+00], dtype=float16), array([3.386e-04, 4.965e-05, 9.995e-01], dtype=float16), array([8.135e-04, 8.345e-05, 9.990e-01], dtype=float16), array([1.472e-02, 5.925e-05, 9.854e-01], dtype=float16), array([7.34e-05, 4.98e-05, 1.00e+00], dtype=float16), array([1.70e-04, 9.83e-06, 1.00e+00], dtype=float16), array([1.053e-04, 2.092e-05, 1.000e+00], dtype=float16), array([7.33e-06, 1.85e-06, 1.00e+00], dtype=float16), array([4.49e-05, 3.52e-06, 1.00e+00], dtype=float16), array([5.012e-04, 6.258e-06, 9.995e-01], dtype=float16), array([3.672e-05, 1.365e-05, 1.000e+00], dtype=float16), array([4.9e-06, 3.6e-07, 1.0e+00], dtype=float16), array([2.03e-05, 6.56e-06, 1.00e+00], dtype=float16), array([1.915e-02, 5.298e-04, 9.805e-01], dtype=float16), array([2.867e-03, 1.121e-05, 9.971e-01], dtype=float16), array([7.486e-05, 4.947e-05, 1.000e+00], dtype=float16), array([2.209e-03, 1.612e-04, 9.976e-01], dtype=float16), array([3.169e-04, 6.801e-05, 9.995e-01], dtype=float16), array([2.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([6.23e-05, 1.91e-06, 1.00e+00], dtype=float16), array([1.491e-04, 4.768e-07, 1.000e+00], dtype=float16), array([2.277e-05, 6.557e-07, 1.000e+00], dtype=float16), array([1.4e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.989e-03, 1.490e-04, 9.927e-01], dtype=float16), array([8.15e-04, 9.18e-06, 9.99e-01], dtype=float16), array([7.615e-04, 5.895e-05, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.215, 0.039, 0.746], dtype=float16), array([6.74e-06, 1.19e-07, 1.00e+00], dtype=float16), array([6.658e-05, 2.843e-05, 1.000e+00], dtype=float16), array([1.1635e-04, 7.8678e-06, 1.0000e+00], dtype=float16), array([9.146e-04, 1.371e-05, 9.990e-01], dtype=float16), array([1.248e-04, 1.252e-06, 1.000e+00], dtype=float16), array([0.0109 , 0.02061, 0.9683 ], dtype=float16), array([7.224e-05, 8.059e-05, 1.000e+00], dtype=float16), array([9.459e-05, 1.234e-05, 1.000e+00], dtype=float16), array([4.116e-03, 2.037e-04, 9.956e-01], dtype=float16), array([1.35e-05, 4.77e-07, 1.00e+00], dtype=float16), array([2.080e-05, 1.264e-05, 1.000e+00], dtype=float16), array([1.6680e-03, 1.1754e-04, 9.9805e-01], dtype=float16), array([0.11176, 0.672  , 0.2163 ], dtype=float16), array([1.788e-04, 1.782e-05, 1.000e+00], dtype=float16), array([7.87e-06, 4.17e-07, 1.00e+00], dtype=float16), array([4.3e-05, 1.0e-06, 1.0e+00], dtype=float16), array([3.034e-05, 2.444e-06, 1.000e+00], dtype=float16), array([7.51e-06, 2.26e-06, 1.00e+00], dtype=float16), array([0.002886, 0.015076, 0.982   ], dtype=float16), array([5.215e-03, 2.766e-04, 9.946e-01], dtype=float16), array([9.209e-03, 5.764e-05, 9.907e-01], dtype=float16), array([8.988e-04, 2.664e-05, 9.990e-01], dtype=float16), array([1.205e-04, 1.365e-05, 1.000e+00], dtype=float16), array([0.0556  , 0.001326, 0.943   ], dtype=float16), array([3.641e-04, 7.570e-06, 9.995e-01], dtype=float16), array([1.881e-04, 1.073e-06, 1.000e+00], dtype=float16), array([8.459e-04, 3.517e-05, 9.990e-01], dtype=float16), array([3.32e-05, 1.49e-06, 1.00e+00], dtype=float16), array([2.60e-05, 1.33e-05, 1.00e+00], dtype=float16), array([3.464e-03, 4.786e-05, 9.966e-01], dtype=float16), array([1.325e-03, 1.026e-04, 9.985e-01], dtype=float16), array([3.838e-03, 2.323e-04, 9.961e-01], dtype=float16), array([2.84e-05, 3.70e-06, 1.00e+00], dtype=float16), array([8.976e-05, 7.629e-06, 1.000e+00], dtype=float16), array([3.951e-04, 1.788e-06, 9.995e-01], dtype=float16), array([3.440e-04, 1.800e-05, 9.995e-01], dtype=float16), array([6.026e-05, 1.073e-06, 1.000e+00], dtype=float16), array([5.584e-04, 3.493e-05, 9.995e-01], dtype=float16), array([3.564e-05, 1.013e-05, 1.000e+00], dtype=float16), array([1.6775e-03, 7.7367e-05, 9.9805e-01], dtype=float16), array([6.48e-05, 1.19e-06, 1.00e+00], dtype=float16), array([1.396e-04, 9.298e-06, 1.000e+00], dtype=float16), array([1.960e-04, 1.388e-04, 9.995e-01], dtype=float16), array([3.340e-04, 5.203e-05, 9.995e-01], dtype=float16), array([2.670e-05, 2.737e-03, 9.971e-01], dtype=float16), array([3.066e-04, 1.121e-05, 9.995e-01], dtype=float16), array([3.440e-04, 2.533e-05, 9.995e-01], dtype=float16), array([5.19e-05, 4.10e-05, 1.00e+00], dtype=float16), array([3.21e-05, 4.17e-07, 1.00e+00], dtype=float16), array([2.68e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.89e-05, 2.80e-06, 1.00e+00], dtype=float16), array([0.003986, 0.00123 , 0.9946  ], dtype=float16), array([5.444e-03, 2.283e-05, 9.946e-01], dtype=float16), array([3.312e-04, 1.562e-05, 9.995e-01], dtype=float16), array([7.2e-05, 3.8e-06, 1.0e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  2
---------------------------------------------------
Extracting Features for Prompt  3  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.334e-04, 6.616e-06, 1.000e+00], dtype=float16), array([7.284e-05, 9.894e-06, 1.000e+00], dtype=float16), array([5.884e-04, 8.512e-05, 9.995e-01], dtype=float16), array([4.035e-05, 1.264e-05, 1.000e+00], dtype=float16), array([3.605e-04, 2.897e-05, 9.995e-01], dtype=float16), array([1.768e-04, 2.289e-05, 1.000e+00], dtype=float16), array([2.156e-04, 2.146e-06, 1.000e+00], dtype=float16), array([5.066e-05, 2.027e-06, 1.000e+00], dtype=float16), array([6.074e-05, 1.431e-05, 1.000e+00], dtype=float16), array([9.97e-04, 2.19e-05, 9.99e-01], dtype=float16), array([1.526e-05, 2.563e-06, 1.000e+00], dtype=float16), array([0.002497, 0.01338 , 0.984   ], dtype=float16), array([0.01897 , 0.011734, 0.969   ], dtype=float16), array([2.165e-04, 1.093e-04, 9.995e-01], dtype=float16), array([3.059e-04, 2.092e-05, 9.995e-01], dtype=float16), array([1.173e-04, 6.557e-07, 1.000e+00], dtype=float16), array([2.323e-04, 2.044e-05, 9.995e-01], dtype=float16), array([1.683e-04, 1.490e-06, 1.000e+00], dtype=float16), array([2.07e-05, 7.03e-06, 1.00e+00], dtype=float16), array([5.031e-04, 2.068e-05, 9.995e-01], dtype=float16), array([2.011e-04, 1.049e-05, 1.000e+00], dtype=float16), array([4.79e-05, 4.35e-06, 1.00e+00], dtype=float16), array([3.606e-05, 5.662e-06, 1.000e+00], dtype=float16), array([8.094e-05, 8.881e-06, 1.000e+00], dtype=float16), array([4.023e-05, 3.821e-05, 1.000e+00], dtype=float16), array([1.778e-02, 3.273e-04, 9.819e-01], dtype=float16), array([1.895e-05, 5.424e-06, 1.000e+00], dtype=float16), array([1.108e-04, 6.658e-05, 1.000e+00], dtype=float16), array([6.218e-04, 2.700e-01, 7.295e-01], dtype=float16), array([1.56e-05, 7.15e-07, 1.00e+00], dtype=float16), array([5.469e-04, 1.508e-05, 9.995e-01], dtype=float16), array([5.7e-06, 5.4e-07, 1.0e+00], dtype=float16), array([5.364e-04, 2.697e-03, 9.966e-01], dtype=float16), array([1.342e-04, 9.537e-07, 1.000e+00], dtype=float16), array([2.475e-04, 1.252e-06, 9.995e-01], dtype=float16), array([2.074e-04, 2.527e-05, 1.000e+00], dtype=float16), array([0.1614 , 0.8228 , 0.01598], dtype=float16), array([4.215e-03, 2.176e-05, 9.956e-01], dtype=float16), array([1.066e-03, 6.776e-04, 9.980e-01], dtype=float16), array([9.66e-06, 2.38e-07, 1.00e+00], dtype=float16), array([5.999e-04, 1.297e-04, 9.995e-01], dtype=float16), array([2.763e-04, 1.192e-05, 9.995e-01], dtype=float16), array([7.963e-04, 2.420e-05, 9.990e-01], dtype=float16), array([2.486e-05, 2.801e-06, 1.000e+00], dtype=float16), array([1.105e-03, 3.451e-05, 9.990e-01], dtype=float16), array([7.086e-04, 2.503e-06, 9.995e-01], dtype=float16), array([4.756e-05, 1.609e-06, 1.000e+00], dtype=float16), array([5.274e-04, 2.190e-03, 9.971e-01], dtype=float16), array([3.e-07, 2.e-07, 1.e+00], dtype=float16), array([3.626e-04, 5.543e-06, 9.995e-01], dtype=float16), array([0.3374 , 0.652  , 0.01094], dtype=float16), array([0.001812, 0.007965, 0.99    ], dtype=float16), array([0.01522, 0.01999, 0.965  ], dtype=float16), array([1.0805e-03, 8.8358e-04, 9.9805e-01], dtype=float16), array([1.074e-04, 2.646e-05, 1.000e+00], dtype=float16), array([1.693e-04, 1.913e-05, 1.000e+00], dtype=float16), array([0.010025, 0.03513 , 0.955   ], dtype=float16), array([1.28e-05, 1.13e-06, 1.00e+00], dtype=float16), array([5.474e-04, 5.603e-06, 9.995e-01], dtype=float16), array([2.247e-04, 6.258e-06, 1.000e+00], dtype=float16), array([1.503e-04, 2.503e-06, 1.000e+00], dtype=float16), array([0.02725  , 0.0014515, 0.971    ], dtype=float16), array([0.06223, 0.05145, 0.886  ], dtype=float16), array([2.937e-04, 2.265e-05, 9.995e-01], dtype=float16), array([1.913e-05, 5.960e-06, 1.000e+00], dtype=float16), array([4.83e-06, 3.04e-06, 1.00e+00], dtype=float16), array([1.157e-04, 9.656e-06, 1.000e+00], dtype=float16), array([0.001046, 0.003887, 0.995   ], dtype=float16), array([0.001772, 0.001984, 0.996   ], dtype=float16), array([8.3e-06, 1.5e-06, 1.0e+00], dtype=float16), array([1.158e-02, 5.990e-05, 9.883e-01], dtype=float16), array([2.295e-05, 8.583e-06, 1.000e+00], dtype=float16), array([1.497e-04, 2.265e-06, 1.000e+00], dtype=float16), array([2.44e-06, 4.17e-07, 1.00e+00], dtype=float16), array([1.44e-04, 7.39e-06, 1.00e+00], dtype=float16), array([9.06e-06, 4.41e-06, 1.00e+00], dtype=float16), array([1.690e-04, 1.025e-05, 1.000e+00], dtype=float16), array([3.145e-03, 8.888e-04, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5763e-05, 1.0000e+00], dtype=float16), array([0.08386 , 0.003172, 0.913   ], dtype=float16), array([1.72e-05, 1.31e-06, 1.00e+00], dtype=float16), array([7.546e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.072e-04, 4.351e-06, 1.000e+00], dtype=float16), array([3.386e-05, 2.027e-06, 1.000e+00], dtype=float16), array([0.01016, 0.00961, 0.9805 ], dtype=float16), array([7.45e-06, 2.21e-06, 1.00e+00], dtype=float16), array([3.22e-05, 7.75e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.431e-06, 1.000e+00], dtype=float16), array([1.034e-03, 2.813e-05, 9.990e-01], dtype=float16), array([1.276e-04, 3.994e-06, 1.000e+00], dtype=float16), array([2.63e-05, 1.07e-06, 1.00e+00], dtype=float16), array([1.298e-04, 1.729e-06, 1.000e+00], dtype=float16), array([1.746e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.569e-03, 7.515e-04, 9.976e-01], dtype=float16), array([3.433e-05, 3.576e-07, 1.000e+00], dtype=float16), array([0.0686, 0.0521, 0.8794], dtype=float16), array([4.053e-05, 2.503e-06, 1.000e+00], dtype=float16), array([1.0073e-04, 1.6749e-05, 1.0000e+00], dtype=float16), array([2.393e-04, 8.076e-05, 9.995e-01], dtype=float16), array([3.419e-04, 6.604e-05, 9.995e-01], dtype=float16), array([2.02e-05, 1.25e-06, 1.00e+00], dtype=float16), array([3.054e-04, 1.168e-05, 9.995e-01], dtype=float16), array([4.685e-05, 3.576e-06, 1.000e+00], dtype=float16), array([6.71e-05, 8.61e-05, 1.00e+00], dtype=float16), array([2.260e-04, 8.482e-05, 9.995e-01], dtype=float16), array([2.627e-04, 1.913e-05, 9.995e-01], dtype=float16), array([2.68e-05, 2.80e-06, 1.00e+00], dtype=float16), array([7.86e-05, 2.62e-06, 1.00e+00], dtype=float16), array([1.18e-05, 1.73e-06, 1.00e+00], dtype=float16), array([8.04e-04, 2.38e-05, 9.99e-01], dtype=float16), array([5.65e-05, 9.48e-06, 1.00e+00], dtype=float16), array([1.878e-05, 1.097e-05, 1.000e+00], dtype=float16), array([6.194e-04, 2.408e-05, 9.995e-01], dtype=float16), array([4.189e-04, 8.702e-06, 9.995e-01], dtype=float16), array([0.0117  , 0.001235, 0.987   ], dtype=float16), array([6.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([1.264e-05, 1.788e-07, 1.000e+00], dtype=float16), array([4.4e-06, 7.2e-07, 1.0e+00], dtype=float16), array([1.406e-03, 5.823e-05, 9.985e-01], dtype=float16), array([1.53e-04, 1.81e-05, 1.00e+00], dtype=float16), array([2.307e-05, 1.073e-06, 1.000e+00], dtype=float16), array([3.982e-05, 2.986e-05, 1.000e+00], dtype=float16), array([4.27e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.707e-03, 1.639e-04, 9.951e-01], dtype=float16), array([2.601e-04, 2.533e-05, 9.995e-01], dtype=float16), array([3.867e-04, 1.615e-04, 9.995e-01], dtype=float16), array([4.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([4.15e-05, 5.60e-06, 1.00e+00], dtype=float16), array([3.011e-04, 1.782e-05, 9.995e-01], dtype=float16), array([8.e-05, 8.e-07, 1.e+00], dtype=float16), array([2.542e-04, 2.211e-05, 9.995e-01], dtype=float16), array([1.434e-04, 6.974e-06, 1.000e+00], dtype=float16), array([5.064e-04, 4.345e-05, 9.995e-01], dtype=float16), array([3.574e-04, 1.860e-05, 9.995e-01], dtype=float16), array([1.6e-04, 5.4e-07, 1.0e+00], dtype=float16), array([1.5335e-03, 1.2469e-04, 9.9854e-01], dtype=float16), array([1.79e-05, 6.56e-07, 1.00e+00], dtype=float16), array([1.777e-04, 1.907e-06, 1.000e+00], dtype=float16), array([2.797e-04, 1.901e-05, 9.995e-01], dtype=float16), array([1.432e-03, 2.152e-05, 9.985e-01], dtype=float16), array([2.87e-05, 2.86e-06, 1.00e+00], dtype=float16), array([9.19e-05, 3.87e-06, 1.00e+00], dtype=float16), array([8.58e-06, 3.34e-06, 1.00e+00], dtype=float16), array([2.927e-05, 7.153e-07, 1.000e+00], dtype=float16), array([0.00378, 0.00596, 0.99   ], dtype=float16), array([7.067e-04, 2.325e-05, 9.995e-01], dtype=float16), array([5.037e-05, 1.556e-05, 1.000e+00], dtype=float16), array([0.0634, 0.5986, 0.338 ], dtype=float16), array([0.007  , 0.12024, 0.8726 ], dtype=float16), array([7.254e-05, 2.205e-06, 1.000e+00], dtype=float16), array([5.54e-06, 2.68e-06, 1.00e+00], dtype=float16), array([7.933e-05, 2.086e-06, 1.000e+00], dtype=float16), array([3.58e-05, 2.38e-07, 1.00e+00], dtype=float16), array([3.87e-05, 3.81e-06, 1.00e+00], dtype=float16), array([1.118e-03, 9.942e-05, 9.990e-01], dtype=float16), array([2.378e-01, 3.459e-04, 7.617e-01], dtype=float16), array([3.181e-03, 8.285e-06, 9.966e-01], dtype=float16), array([1.843e-03, 8.345e-07, 9.980e-01], dtype=float16), array([4.010e-02, 4.661e-04, 9.595e-01], dtype=float16), array([2.02e-05, 2.15e-06, 1.00e+00], dtype=float16), array([3.191e-03, 6.872e-05, 9.966e-01], dtype=float16), array([0.00827 , 0.001407, 0.99    ], dtype=float16), array([7.367e-05, 1.645e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.29e-05, 2.38e-07, 1.00e+00], dtype=float16), array([2.474e-05, 8.523e-06, 1.000e+00], dtype=float16), array([1.32e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.816e-05, 9.656e-06, 1.000e+00], dtype=float16), array([7.067e-04, 1.127e-05, 9.995e-01], dtype=float16), array([5.760e-04, 3.517e-06, 9.995e-01], dtype=float16), array([2.165e-03, 2.718e-05, 9.980e-01], dtype=float16), array([3.684e-05, 1.121e-05, 1.000e+00], dtype=float16), array([7.734e-04, 4.232e-06, 9.990e-01], dtype=float16), array([8.583e-05, 5.722e-06, 1.000e+00], dtype=float16), array([2.384e-05, 1.609e-06, 1.000e+00], dtype=float16), array([1.35e-04, 4.85e-05, 1.00e+00], dtype=float16), array([2.174e-04, 3.517e-06, 1.000e+00], dtype=float16), array([1.041e-04, 5.484e-06, 1.000e+00], dtype=float16), array([2.241e-04, 4.172e-07, 1.000e+00], dtype=float16), array([3.779e-05, 4.196e-05, 1.000e+00], dtype=float16), array([1.498e-03, 3.121e-04, 9.980e-01], dtype=float16), array([1.59e-05, 9.72e-06, 1.00e+00], dtype=float16), array([1.1325e-04, 3.4928e-05, 1.0000e+00], dtype=float16), array([9.03e-05, 3.24e-05, 1.00e+00], dtype=float16), array([5.679e-04, 5.841e-06, 9.995e-01], dtype=float16), array([3.264e-04, 1.270e-05, 9.995e-01], dtype=float16), array([0.03882, 0.2043 , 0.757  ], dtype=float16), array([2.74e-06, 8.94e-07, 1.00e+00], dtype=float16), array([6.952e-04, 9.918e-04, 9.985e-01], dtype=float16), array([2.426e-05, 2.980e-07, 1.000e+00], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([5.4e-05, 7.1e-06, 1.0e+00], dtype=float16), array([7.668e-04, 2.587e-05, 9.990e-01], dtype=float16), array([2.26e-06, 2.98e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.848e-05, 1.000e+00], dtype=float16), array([3.386e-04, 4.965e-05, 9.995e-01], dtype=float16), array([8.135e-04, 8.345e-05, 9.990e-01], dtype=float16), array([1.472e-02, 5.925e-05, 9.854e-01], dtype=float16), array([7.34e-05, 4.98e-05, 1.00e+00], dtype=float16), array([1.70e-04, 9.83e-06, 1.00e+00], dtype=float16), array([1.053e-04, 2.092e-05, 1.000e+00], dtype=float16), array([7.33e-06, 1.85e-06, 1.00e+00], dtype=float16), array([4.49e-05, 3.52e-06, 1.00e+00], dtype=float16), array([5.012e-04, 6.258e-06, 9.995e-01], dtype=float16), array([3.672e-05, 1.365e-05, 1.000e+00], dtype=float16), array([4.9e-06, 3.6e-07, 1.0e+00], dtype=float16), array([2.03e-05, 6.56e-06, 1.00e+00], dtype=float16), array([1.915e-02, 5.298e-04, 9.805e-01], dtype=float16), array([2.867e-03, 1.121e-05, 9.971e-01], dtype=float16), array([7.486e-05, 4.947e-05, 1.000e+00], dtype=float16), array([2.209e-03, 1.612e-04, 9.976e-01], dtype=float16), array([3.169e-04, 6.801e-05, 9.995e-01], dtype=float16), array([2.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([6.23e-05, 1.91e-06, 1.00e+00], dtype=float16), array([1.491e-04, 4.768e-07, 1.000e+00], dtype=float16), array([2.277e-05, 6.557e-07, 1.000e+00], dtype=float16), array([1.4e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.989e-03, 1.490e-04, 9.927e-01], dtype=float16), array([8.15e-04, 9.18e-06, 9.99e-01], dtype=float16), array([7.615e-04, 5.895e-05, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.215, 0.039, 0.746], dtype=float16), array([6.74e-06, 1.19e-07, 1.00e+00], dtype=float16), array([6.658e-05, 2.843e-05, 1.000e+00], dtype=float16), array([1.1635e-04, 7.8678e-06, 1.0000e+00], dtype=float16), array([9.146e-04, 1.371e-05, 9.990e-01], dtype=float16), array([1.248e-04, 1.252e-06, 1.000e+00], dtype=float16), array([0.0109 , 0.02061, 0.9683 ], dtype=float16), array([7.224e-05, 8.059e-05, 1.000e+00], dtype=float16), array([9.459e-05, 1.234e-05, 1.000e+00], dtype=float16), array([4.116e-03, 2.037e-04, 9.956e-01], dtype=float16), array([1.35e-05, 4.77e-07, 1.00e+00], dtype=float16), array([2.080e-05, 1.264e-05, 1.000e+00], dtype=float16), array([1.6680e-03, 1.1754e-04, 9.9805e-01], dtype=float16), array([0.11176, 0.672  , 0.2163 ], dtype=float16), array([1.788e-04, 1.782e-05, 1.000e+00], dtype=float16), array([7.87e-06, 4.17e-07, 1.00e+00], dtype=float16), array([4.3e-05, 1.0e-06, 1.0e+00], dtype=float16), array([3.034e-05, 2.444e-06, 1.000e+00], dtype=float16), array([7.51e-06, 2.26e-06, 1.00e+00], dtype=float16), array([0.002886, 0.015076, 0.982   ], dtype=float16), array([5.215e-03, 2.766e-04, 9.946e-01], dtype=float16), array([9.209e-03, 5.764e-05, 9.907e-01], dtype=float16), array([8.988e-04, 2.664e-05, 9.990e-01], dtype=float16), array([1.205e-04, 1.365e-05, 1.000e+00], dtype=float16), array([0.0556  , 0.001326, 0.943   ], dtype=float16), array([3.641e-04, 7.570e-06, 9.995e-01], dtype=float16), array([1.881e-04, 1.073e-06, 1.000e+00], dtype=float16), array([8.459e-04, 3.517e-05, 9.990e-01], dtype=float16), array([3.32e-05, 1.49e-06, 1.00e+00], dtype=float16), array([2.60e-05, 1.33e-05, 1.00e+00], dtype=float16), array([3.464e-03, 4.786e-05, 9.966e-01], dtype=float16), array([1.325e-03, 1.026e-04, 9.985e-01], dtype=float16), array([3.838e-03, 2.323e-04, 9.961e-01], dtype=float16), array([2.84e-05, 3.70e-06, 1.00e+00], dtype=float16), array([8.976e-05, 7.629e-06, 1.000e+00], dtype=float16), array([3.951e-04, 1.788e-06, 9.995e-01], dtype=float16), array([3.440e-04, 1.800e-05, 9.995e-01], dtype=float16), array([6.026e-05, 1.073e-06, 1.000e+00], dtype=float16), array([5.584e-04, 3.493e-05, 9.995e-01], dtype=float16), array([3.564e-05, 1.013e-05, 1.000e+00], dtype=float16), array([1.6775e-03, 7.7367e-05, 9.9805e-01], dtype=float16), array([6.48e-05, 1.19e-06, 1.00e+00], dtype=float16), array([1.396e-04, 9.298e-06, 1.000e+00], dtype=float16), array([1.960e-04, 1.388e-04, 9.995e-01], dtype=float16), array([3.340e-04, 5.203e-05, 9.995e-01], dtype=float16), array([2.670e-05, 2.737e-03, 9.971e-01], dtype=float16), array([3.066e-04, 1.121e-05, 9.995e-01], dtype=float16), array([3.440e-04, 2.533e-05, 9.995e-01], dtype=float16), array([5.19e-05, 4.10e-05, 1.00e+00], dtype=float16), array([3.21e-05, 4.17e-07, 1.00e+00], dtype=float16), array([2.68e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.89e-05, 2.80e-06, 1.00e+00], dtype=float16), array([0.003986, 0.00123 , 0.9946  ], dtype=float16), array([5.444e-03, 2.283e-05, 9.946e-01], dtype=float16), array([3.312e-04, 1.562e-05, 9.995e-01], dtype=float16), array([7.2e-05, 3.8e-06, 1.0e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  3
---------------------------------------------------
Extracting Features for Prompt  4  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.334e-04, 6.616e-06, 1.000e+00], dtype=float16), array([7.284e-05, 9.894e-06, 1.000e+00], dtype=float16), array([5.884e-04, 8.512e-05, 9.995e-01], dtype=float16), array([4.035e-05, 1.264e-05, 1.000e+00], dtype=float16), array([3.605e-04, 2.897e-05, 9.995e-01], dtype=float16), array([1.768e-04, 2.289e-05, 1.000e+00], dtype=float16), array([2.156e-04, 2.146e-06, 1.000e+00], dtype=float16), array([5.066e-05, 2.027e-06, 1.000e+00], dtype=float16), array([6.074e-05, 1.431e-05, 1.000e+00], dtype=float16), array([9.97e-04, 2.19e-05, 9.99e-01], dtype=float16), array([1.526e-05, 2.563e-06, 1.000e+00], dtype=float16), array([0.002497, 0.01338 , 0.984   ], dtype=float16), array([0.01897 , 0.011734, 0.969   ], dtype=float16), array([2.165e-04, 1.093e-04, 9.995e-01], dtype=float16), array([3.059e-04, 2.092e-05, 9.995e-01], dtype=float16), array([1.173e-04, 6.557e-07, 1.000e+00], dtype=float16), array([2.323e-04, 2.044e-05, 9.995e-01], dtype=float16), array([1.683e-04, 1.490e-06, 1.000e+00], dtype=float16), array([2.07e-05, 7.03e-06, 1.00e+00], dtype=float16), array([5.031e-04, 2.068e-05, 9.995e-01], dtype=float16), array([2.011e-04, 1.049e-05, 1.000e+00], dtype=float16), array([4.79e-05, 4.35e-06, 1.00e+00], dtype=float16), array([3.606e-05, 5.662e-06, 1.000e+00], dtype=float16), array([8.094e-05, 8.881e-06, 1.000e+00], dtype=float16), array([4.023e-05, 3.821e-05, 1.000e+00], dtype=float16), array([1.778e-02, 3.273e-04, 9.819e-01], dtype=float16), array([1.895e-05, 5.424e-06, 1.000e+00], dtype=float16), array([1.108e-04, 6.658e-05, 1.000e+00], dtype=float16), array([6.218e-04, 2.700e-01, 7.295e-01], dtype=float16), array([1.56e-05, 7.15e-07, 1.00e+00], dtype=float16), array([5.469e-04, 1.508e-05, 9.995e-01], dtype=float16), array([5.7e-06, 5.4e-07, 1.0e+00], dtype=float16), array([5.364e-04, 2.697e-03, 9.966e-01], dtype=float16), array([1.342e-04, 9.537e-07, 1.000e+00], dtype=float16), array([2.475e-04, 1.252e-06, 9.995e-01], dtype=float16), array([2.074e-04, 2.527e-05, 1.000e+00], dtype=float16), array([0.1614 , 0.8228 , 0.01598], dtype=float16), array([4.215e-03, 2.176e-05, 9.956e-01], dtype=float16), array([1.066e-03, 6.776e-04, 9.980e-01], dtype=float16), array([9.66e-06, 2.38e-07, 1.00e+00], dtype=float16), array([5.999e-04, 1.297e-04, 9.995e-01], dtype=float16), array([2.763e-04, 1.192e-05, 9.995e-01], dtype=float16), array([7.963e-04, 2.420e-05, 9.990e-01], dtype=float16), array([2.486e-05, 2.801e-06, 1.000e+00], dtype=float16), array([1.105e-03, 3.451e-05, 9.990e-01], dtype=float16), array([7.086e-04, 2.503e-06, 9.995e-01], dtype=float16), array([4.756e-05, 1.609e-06, 1.000e+00], dtype=float16), array([5.274e-04, 2.190e-03, 9.971e-01], dtype=float16), array([3.e-07, 2.e-07, 1.e+00], dtype=float16), array([3.626e-04, 5.543e-06, 9.995e-01], dtype=float16), array([0.3374 , 0.652  , 0.01094], dtype=float16), array([0.001812, 0.007965, 0.99    ], dtype=float16), array([0.01522, 0.01999, 0.965  ], dtype=float16), array([1.0805e-03, 8.8358e-04, 9.9805e-01], dtype=float16), array([1.074e-04, 2.646e-05, 1.000e+00], dtype=float16), array([1.693e-04, 1.913e-05, 1.000e+00], dtype=float16), array([0.010025, 0.03513 , 0.955   ], dtype=float16), array([1.28e-05, 1.13e-06, 1.00e+00], dtype=float16), array([5.474e-04, 5.603e-06, 9.995e-01], dtype=float16), array([2.247e-04, 6.258e-06, 1.000e+00], dtype=float16), array([1.503e-04, 2.503e-06, 1.000e+00], dtype=float16), array([0.02725  , 0.0014515, 0.971    ], dtype=float16), array([0.06223, 0.05145, 0.886  ], dtype=float16), array([2.937e-04, 2.265e-05, 9.995e-01], dtype=float16), array([1.913e-05, 5.960e-06, 1.000e+00], dtype=float16), array([4.83e-06, 3.04e-06, 1.00e+00], dtype=float16), array([1.157e-04, 9.656e-06, 1.000e+00], dtype=float16), array([0.001046, 0.003887, 0.995   ], dtype=float16), array([0.001772, 0.001984, 0.996   ], dtype=float16), array([8.3e-06, 1.5e-06, 1.0e+00], dtype=float16), array([1.158e-02, 5.990e-05, 9.883e-01], dtype=float16), array([2.295e-05, 8.583e-06, 1.000e+00], dtype=float16), array([1.497e-04, 2.265e-06, 1.000e+00], dtype=float16), array([2.44e-06, 4.17e-07, 1.00e+00], dtype=float16), array([1.44e-04, 7.39e-06, 1.00e+00], dtype=float16), array([9.06e-06, 4.41e-06, 1.00e+00], dtype=float16), array([1.690e-04, 1.025e-05, 1.000e+00], dtype=float16), array([3.145e-03, 8.888e-04, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5763e-05, 1.0000e+00], dtype=float16), array([0.08386 , 0.003172, 0.913   ], dtype=float16), array([1.72e-05, 1.31e-06, 1.00e+00], dtype=float16), array([7.546e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.072e-04, 4.351e-06, 1.000e+00], dtype=float16), array([3.386e-05, 2.027e-06, 1.000e+00], dtype=float16), array([0.01016, 0.00961, 0.9805 ], dtype=float16), array([7.45e-06, 2.21e-06, 1.00e+00], dtype=float16), array([3.22e-05, 7.75e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.431e-06, 1.000e+00], dtype=float16), array([1.034e-03, 2.813e-05, 9.990e-01], dtype=float16), array([1.276e-04, 3.994e-06, 1.000e+00], dtype=float16), array([2.63e-05, 1.07e-06, 1.00e+00], dtype=float16), array([1.298e-04, 1.729e-06, 1.000e+00], dtype=float16), array([1.746e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.569e-03, 7.515e-04, 9.976e-01], dtype=float16), array([3.433e-05, 3.576e-07, 1.000e+00], dtype=float16), array([0.0686, 0.0521, 0.8794], dtype=float16), array([4.053e-05, 2.503e-06, 1.000e+00], dtype=float16), array([1.0073e-04, 1.6749e-05, 1.0000e+00], dtype=float16), array([2.393e-04, 8.076e-05, 9.995e-01], dtype=float16), array([3.419e-04, 6.604e-05, 9.995e-01], dtype=float16), array([2.02e-05, 1.25e-06, 1.00e+00], dtype=float16), array([3.054e-04, 1.168e-05, 9.995e-01], dtype=float16), array([4.685e-05, 3.576e-06, 1.000e+00], dtype=float16), array([6.71e-05, 8.61e-05, 1.00e+00], dtype=float16), array([2.260e-04, 8.482e-05, 9.995e-01], dtype=float16), array([2.627e-04, 1.913e-05, 9.995e-01], dtype=float16), array([2.68e-05, 2.80e-06, 1.00e+00], dtype=float16), array([7.86e-05, 2.62e-06, 1.00e+00], dtype=float16), array([1.18e-05, 1.73e-06, 1.00e+00], dtype=float16), array([8.04e-04, 2.38e-05, 9.99e-01], dtype=float16), array([5.65e-05, 9.48e-06, 1.00e+00], dtype=float16), array([1.878e-05, 1.097e-05, 1.000e+00], dtype=float16), array([6.194e-04, 2.408e-05, 9.995e-01], dtype=float16), array([4.189e-04, 8.702e-06, 9.995e-01], dtype=float16), array([0.0117  , 0.001235, 0.987   ], dtype=float16), array([6.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([1.264e-05, 1.788e-07, 1.000e+00], dtype=float16), array([4.4e-06, 7.2e-07, 1.0e+00], dtype=float16), array([1.406e-03, 5.823e-05, 9.985e-01], dtype=float16), array([1.53e-04, 1.81e-05, 1.00e+00], dtype=float16), array([2.307e-05, 1.073e-06, 1.000e+00], dtype=float16), array([3.982e-05, 2.986e-05, 1.000e+00], dtype=float16), array([4.27e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.707e-03, 1.639e-04, 9.951e-01], dtype=float16), array([2.601e-04, 2.533e-05, 9.995e-01], dtype=float16), array([3.867e-04, 1.615e-04, 9.995e-01], dtype=float16), array([4.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([4.15e-05, 5.60e-06, 1.00e+00], dtype=float16), array([3.011e-04, 1.782e-05, 9.995e-01], dtype=float16), array([8.e-05, 8.e-07, 1.e+00], dtype=float16), array([2.542e-04, 2.211e-05, 9.995e-01], dtype=float16), array([1.434e-04, 6.974e-06, 1.000e+00], dtype=float16), array([5.064e-04, 4.345e-05, 9.995e-01], dtype=float16), array([3.574e-04, 1.860e-05, 9.995e-01], dtype=float16), array([1.6e-04, 5.4e-07, 1.0e+00], dtype=float16), array([1.5335e-03, 1.2469e-04, 9.9854e-01], dtype=float16), array([1.79e-05, 6.56e-07, 1.00e+00], dtype=float16), array([1.777e-04, 1.907e-06, 1.000e+00], dtype=float16), array([2.797e-04, 1.901e-05, 9.995e-01], dtype=float16), array([1.432e-03, 2.152e-05, 9.985e-01], dtype=float16), array([2.87e-05, 2.86e-06, 1.00e+00], dtype=float16), array([9.19e-05, 3.87e-06, 1.00e+00], dtype=float16), array([8.58e-06, 3.34e-06, 1.00e+00], dtype=float16), array([2.927e-05, 7.153e-07, 1.000e+00], dtype=float16), array([0.00378, 0.00596, 0.99   ], dtype=float16), array([7.067e-04, 2.325e-05, 9.995e-01], dtype=float16), array([5.037e-05, 1.556e-05, 1.000e+00], dtype=float16), array([0.0634, 0.5986, 0.338 ], dtype=float16), array([0.007  , 0.12024, 0.8726 ], dtype=float16), array([7.254e-05, 2.205e-06, 1.000e+00], dtype=float16), array([5.54e-06, 2.68e-06, 1.00e+00], dtype=float16), array([7.933e-05, 2.086e-06, 1.000e+00], dtype=float16), array([3.58e-05, 2.38e-07, 1.00e+00], dtype=float16), array([3.87e-05, 3.81e-06, 1.00e+00], dtype=float16), array([1.118e-03, 9.942e-05, 9.990e-01], dtype=float16), array([2.378e-01, 3.459e-04, 7.617e-01], dtype=float16), array([3.181e-03, 8.285e-06, 9.966e-01], dtype=float16), array([1.843e-03, 8.345e-07, 9.980e-01], dtype=float16), array([4.010e-02, 4.661e-04, 9.595e-01], dtype=float16), array([2.02e-05, 2.15e-06, 1.00e+00], dtype=float16), array([3.191e-03, 6.872e-05, 9.966e-01], dtype=float16), array([0.00827 , 0.001407, 0.99    ], dtype=float16), array([7.367e-05, 1.645e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.29e-05, 2.38e-07, 1.00e+00], dtype=float16), array([2.474e-05, 8.523e-06, 1.000e+00], dtype=float16), array([1.32e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.816e-05, 9.656e-06, 1.000e+00], dtype=float16), array([7.067e-04, 1.127e-05, 9.995e-01], dtype=float16), array([5.760e-04, 3.517e-06, 9.995e-01], dtype=float16), array([2.165e-03, 2.718e-05, 9.980e-01], dtype=float16), array([3.684e-05, 1.121e-05, 1.000e+00], dtype=float16), array([7.734e-04, 4.232e-06, 9.990e-01], dtype=float16), array([8.583e-05, 5.722e-06, 1.000e+00], dtype=float16), array([2.384e-05, 1.609e-06, 1.000e+00], dtype=float16), array([1.35e-04, 4.85e-05, 1.00e+00], dtype=float16), array([2.174e-04, 3.517e-06, 1.000e+00], dtype=float16), array([1.041e-04, 5.484e-06, 1.000e+00], dtype=float16), array([2.241e-04, 4.172e-07, 1.000e+00], dtype=float16), array([3.779e-05, 4.196e-05, 1.000e+00], dtype=float16), array([1.498e-03, 3.121e-04, 9.980e-01], dtype=float16), array([1.59e-05, 9.72e-06, 1.00e+00], dtype=float16), array([1.1325e-04, 3.4928e-05, 1.0000e+00], dtype=float16), array([9.03e-05, 3.24e-05, 1.00e+00], dtype=float16), array([5.679e-04, 5.841e-06, 9.995e-01], dtype=float16), array([3.264e-04, 1.270e-05, 9.995e-01], dtype=float16), array([0.03882, 0.2043 , 0.757  ], dtype=float16), array([2.74e-06, 8.94e-07, 1.00e+00], dtype=float16), array([6.952e-04, 9.918e-04, 9.985e-01], dtype=float16), array([2.426e-05, 2.980e-07, 1.000e+00], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([5.4e-05, 7.1e-06, 1.0e+00], dtype=float16), array([7.668e-04, 2.587e-05, 9.990e-01], dtype=float16), array([2.26e-06, 2.98e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.848e-05, 1.000e+00], dtype=float16), array([3.386e-04, 4.965e-05, 9.995e-01], dtype=float16), array([8.135e-04, 8.345e-05, 9.990e-01], dtype=float16), array([1.472e-02, 5.925e-05, 9.854e-01], dtype=float16), array([7.34e-05, 4.98e-05, 1.00e+00], dtype=float16), array([1.70e-04, 9.83e-06, 1.00e+00], dtype=float16), array([1.053e-04, 2.092e-05, 1.000e+00], dtype=float16), array([7.33e-06, 1.85e-06, 1.00e+00], dtype=float16), array([4.49e-05, 3.52e-06, 1.00e+00], dtype=float16), array([5.012e-04, 6.258e-06, 9.995e-01], dtype=float16), array([3.672e-05, 1.365e-05, 1.000e+00], dtype=float16), array([4.9e-06, 3.6e-07, 1.0e+00], dtype=float16), array([2.03e-05, 6.56e-06, 1.00e+00], dtype=float16), array([1.915e-02, 5.298e-04, 9.805e-01], dtype=float16), array([2.867e-03, 1.121e-05, 9.971e-01], dtype=float16), array([7.486e-05, 4.947e-05, 1.000e+00], dtype=float16), array([2.209e-03, 1.612e-04, 9.976e-01], dtype=float16), array([3.169e-04, 6.801e-05, 9.995e-01], dtype=float16), array([2.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([6.23e-05, 1.91e-06, 1.00e+00], dtype=float16), array([1.491e-04, 4.768e-07, 1.000e+00], dtype=float16), array([2.277e-05, 6.557e-07, 1.000e+00], dtype=float16), array([1.4e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.989e-03, 1.490e-04, 9.927e-01], dtype=float16), array([8.15e-04, 9.18e-06, 9.99e-01], dtype=float16), array([7.615e-04, 5.895e-05, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.215, 0.039, 0.746], dtype=float16), array([6.74e-06, 1.19e-07, 1.00e+00], dtype=float16), array([6.658e-05, 2.843e-05, 1.000e+00], dtype=float16), array([1.1635e-04, 7.8678e-06, 1.0000e+00], dtype=float16), array([9.146e-04, 1.371e-05, 9.990e-01], dtype=float16), array([1.248e-04, 1.252e-06, 1.000e+00], dtype=float16), array([0.0109 , 0.02061, 0.9683 ], dtype=float16), array([7.224e-05, 8.059e-05, 1.000e+00], dtype=float16), array([9.459e-05, 1.234e-05, 1.000e+00], dtype=float16), array([4.116e-03, 2.037e-04, 9.956e-01], dtype=float16), array([1.35e-05, 4.77e-07, 1.00e+00], dtype=float16), array([2.080e-05, 1.264e-05, 1.000e+00], dtype=float16), array([1.6680e-03, 1.1754e-04, 9.9805e-01], dtype=float16), array([0.11176, 0.672  , 0.2163 ], dtype=float16), array([1.788e-04, 1.782e-05, 1.000e+00], dtype=float16), array([7.87e-06, 4.17e-07, 1.00e+00], dtype=float16), array([4.3e-05, 1.0e-06, 1.0e+00], dtype=float16), array([3.034e-05, 2.444e-06, 1.000e+00], dtype=float16), array([7.51e-06, 2.26e-06, 1.00e+00], dtype=float16), array([0.002886, 0.015076, 0.982   ], dtype=float16), array([5.215e-03, 2.766e-04, 9.946e-01], dtype=float16), array([9.209e-03, 5.764e-05, 9.907e-01], dtype=float16), array([8.988e-04, 2.664e-05, 9.990e-01], dtype=float16), array([1.205e-04, 1.365e-05, 1.000e+00], dtype=float16), array([0.0556  , 0.001326, 0.943   ], dtype=float16), array([3.641e-04, 7.570e-06, 9.995e-01], dtype=float16), array([1.881e-04, 1.073e-06, 1.000e+00], dtype=float16), array([8.459e-04, 3.517e-05, 9.990e-01], dtype=float16), array([3.32e-05, 1.49e-06, 1.00e+00], dtype=float16), array([2.60e-05, 1.33e-05, 1.00e+00], dtype=float16), array([3.464e-03, 4.786e-05, 9.966e-01], dtype=float16), array([1.325e-03, 1.026e-04, 9.985e-01], dtype=float16), array([3.838e-03, 2.323e-04, 9.961e-01], dtype=float16), array([2.84e-05, 3.70e-06, 1.00e+00], dtype=float16), array([8.976e-05, 7.629e-06, 1.000e+00], dtype=float16), array([3.951e-04, 1.788e-06, 9.995e-01], dtype=float16), array([3.440e-04, 1.800e-05, 9.995e-01], dtype=float16), array([6.026e-05, 1.073e-06, 1.000e+00], dtype=float16), array([5.584e-04, 3.493e-05, 9.995e-01], dtype=float16), array([3.564e-05, 1.013e-05, 1.000e+00], dtype=float16), array([1.6775e-03, 7.7367e-05, 9.9805e-01], dtype=float16), array([6.48e-05, 1.19e-06, 1.00e+00], dtype=float16), array([1.396e-04, 9.298e-06, 1.000e+00], dtype=float16), array([1.960e-04, 1.388e-04, 9.995e-01], dtype=float16), array([3.340e-04, 5.203e-05, 9.995e-01], dtype=float16), array([2.670e-05, 2.737e-03, 9.971e-01], dtype=float16), array([3.066e-04, 1.121e-05, 9.995e-01], dtype=float16), array([3.440e-04, 2.533e-05, 9.995e-01], dtype=float16), array([5.19e-05, 4.10e-05, 1.00e+00], dtype=float16), array([3.21e-05, 4.17e-07, 1.00e+00], dtype=float16), array([2.68e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.89e-05, 2.80e-06, 1.00e+00], dtype=float16), array([0.003986, 0.00123 , 0.9946  ], dtype=float16), array([5.444e-03, 2.283e-05, 9.946e-01], dtype=float16), array([3.312e-04, 1.562e-05, 9.995e-01], dtype=float16), array([7.2e-05, 3.8e-06, 1.0e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  4
---------------------------------------------------
Extracting Features for Prompt  5  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.334e-04, 6.616e-06, 1.000e+00], dtype=float16), array([7.284e-05, 9.894e-06, 1.000e+00], dtype=float16), array([5.884e-04, 8.512e-05, 9.995e-01], dtype=float16), array([4.035e-05, 1.264e-05, 1.000e+00], dtype=float16), array([3.605e-04, 2.897e-05, 9.995e-01], dtype=float16), array([1.768e-04, 2.289e-05, 1.000e+00], dtype=float16), array([2.156e-04, 2.146e-06, 1.000e+00], dtype=float16), array([5.066e-05, 2.027e-06, 1.000e+00], dtype=float16), array([6.074e-05, 1.431e-05, 1.000e+00], dtype=float16), array([9.97e-04, 2.19e-05, 9.99e-01], dtype=float16), array([1.526e-05, 2.563e-06, 1.000e+00], dtype=float16), array([0.002497, 0.01338 , 0.984   ], dtype=float16), array([0.01897 , 0.011734, 0.969   ], dtype=float16), array([2.165e-04, 1.093e-04, 9.995e-01], dtype=float16), array([3.059e-04, 2.092e-05, 9.995e-01], dtype=float16), array([1.173e-04, 6.557e-07, 1.000e+00], dtype=float16), array([2.323e-04, 2.044e-05, 9.995e-01], dtype=float16), array([1.683e-04, 1.490e-06, 1.000e+00], dtype=float16), array([2.07e-05, 7.03e-06, 1.00e+00], dtype=float16), array([5.031e-04, 2.068e-05, 9.995e-01], dtype=float16), array([2.011e-04, 1.049e-05, 1.000e+00], dtype=float16), array([4.79e-05, 4.35e-06, 1.00e+00], dtype=float16), array([3.606e-05, 5.662e-06, 1.000e+00], dtype=float16), array([8.094e-05, 8.881e-06, 1.000e+00], dtype=float16), array([4.023e-05, 3.821e-05, 1.000e+00], dtype=float16), array([1.778e-02, 3.273e-04, 9.819e-01], dtype=float16), array([1.895e-05, 5.424e-06, 1.000e+00], dtype=float16), array([1.108e-04, 6.658e-05, 1.000e+00], dtype=float16), array([6.218e-04, 2.700e-01, 7.295e-01], dtype=float16), array([1.56e-05, 7.15e-07, 1.00e+00], dtype=float16), array([5.469e-04, 1.508e-05, 9.995e-01], dtype=float16), array([5.7e-06, 5.4e-07, 1.0e+00], dtype=float16), array([5.364e-04, 2.697e-03, 9.966e-01], dtype=float16), array([1.342e-04, 9.537e-07, 1.000e+00], dtype=float16), array([2.475e-04, 1.252e-06, 9.995e-01], dtype=float16), array([2.074e-04, 2.527e-05, 1.000e+00], dtype=float16), array([0.1614 , 0.8228 , 0.01598], dtype=float16), array([4.215e-03, 2.176e-05, 9.956e-01], dtype=float16), array([1.066e-03, 6.776e-04, 9.980e-01], dtype=float16), array([9.66e-06, 2.38e-07, 1.00e+00], dtype=float16), array([5.999e-04, 1.297e-04, 9.995e-01], dtype=float16), array([2.763e-04, 1.192e-05, 9.995e-01], dtype=float16), array([7.963e-04, 2.420e-05, 9.990e-01], dtype=float16), array([2.486e-05, 2.801e-06, 1.000e+00], dtype=float16), array([1.105e-03, 3.451e-05, 9.990e-01], dtype=float16), array([7.086e-04, 2.503e-06, 9.995e-01], dtype=float16), array([4.756e-05, 1.609e-06, 1.000e+00], dtype=float16), array([5.274e-04, 2.190e-03, 9.971e-01], dtype=float16), array([3.e-07, 2.e-07, 1.e+00], dtype=float16), array([3.626e-04, 5.543e-06, 9.995e-01], dtype=float16), array([0.3374 , 0.652  , 0.01094], dtype=float16), array([0.001812, 0.007965, 0.99    ], dtype=float16), array([0.01522, 0.01999, 0.965  ], dtype=float16), array([1.0805e-03, 8.8358e-04, 9.9805e-01], dtype=float16), array([1.074e-04, 2.646e-05, 1.000e+00], dtype=float16), array([1.693e-04, 1.913e-05, 1.000e+00], dtype=float16), array([0.010025, 0.03513 , 0.955   ], dtype=float16), array([1.28e-05, 1.13e-06, 1.00e+00], dtype=float16), array([5.474e-04, 5.603e-06, 9.995e-01], dtype=float16), array([2.247e-04, 6.258e-06, 1.000e+00], dtype=float16), array([1.503e-04, 2.503e-06, 1.000e+00], dtype=float16), array([0.02725  , 0.0014515, 0.971    ], dtype=float16), array([0.06223, 0.05145, 0.886  ], dtype=float16), array([2.937e-04, 2.265e-05, 9.995e-01], dtype=float16), array([1.913e-05, 5.960e-06, 1.000e+00], dtype=float16), array([4.83e-06, 3.04e-06, 1.00e+00], dtype=float16), array([1.157e-04, 9.656e-06, 1.000e+00], dtype=float16), array([0.001046, 0.003887, 0.995   ], dtype=float16), array([0.001772, 0.001984, 0.996   ], dtype=float16), array([8.3e-06, 1.5e-06, 1.0e+00], dtype=float16), array([1.158e-02, 5.990e-05, 9.883e-01], dtype=float16), array([2.295e-05, 8.583e-06, 1.000e+00], dtype=float16), array([1.497e-04, 2.265e-06, 1.000e+00], dtype=float16), array([2.44e-06, 4.17e-07, 1.00e+00], dtype=float16), array([1.44e-04, 7.39e-06, 1.00e+00], dtype=float16), array([9.06e-06, 4.41e-06, 1.00e+00], dtype=float16), array([1.690e-04, 1.025e-05, 1.000e+00], dtype=float16), array([3.145e-03, 8.888e-04, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5763e-05, 1.0000e+00], dtype=float16), array([0.08386 , 0.003172, 0.913   ], dtype=float16), array([1.72e-05, 1.31e-06, 1.00e+00], dtype=float16), array([7.546e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.072e-04, 4.351e-06, 1.000e+00], dtype=float16), array([3.386e-05, 2.027e-06, 1.000e+00], dtype=float16), array([0.01016, 0.00961, 0.9805 ], dtype=float16), array([7.45e-06, 2.21e-06, 1.00e+00], dtype=float16), array([3.22e-05, 7.75e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.431e-06, 1.000e+00], dtype=float16), array([1.034e-03, 2.813e-05, 9.990e-01], dtype=float16), array([1.276e-04, 3.994e-06, 1.000e+00], dtype=float16), array([2.63e-05, 1.07e-06, 1.00e+00], dtype=float16), array([1.298e-04, 1.729e-06, 1.000e+00], dtype=float16), array([1.746e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.569e-03, 7.515e-04, 9.976e-01], dtype=float16), array([3.433e-05, 3.576e-07, 1.000e+00], dtype=float16), array([0.0686, 0.0521, 0.8794], dtype=float16), array([4.053e-05, 2.503e-06, 1.000e+00], dtype=float16), array([1.0073e-04, 1.6749e-05, 1.0000e+00], dtype=float16), array([2.393e-04, 8.076e-05, 9.995e-01], dtype=float16), array([3.419e-04, 6.604e-05, 9.995e-01], dtype=float16), array([2.02e-05, 1.25e-06, 1.00e+00], dtype=float16), array([3.054e-04, 1.168e-05, 9.995e-01], dtype=float16), array([4.685e-05, 3.576e-06, 1.000e+00], dtype=float16), array([6.71e-05, 8.61e-05, 1.00e+00], dtype=float16), array([2.260e-04, 8.482e-05, 9.995e-01], dtype=float16), array([2.627e-04, 1.913e-05, 9.995e-01], dtype=float16), array([2.68e-05, 2.80e-06, 1.00e+00], dtype=float16), array([7.86e-05, 2.62e-06, 1.00e+00], dtype=float16), array([1.18e-05, 1.73e-06, 1.00e+00], dtype=float16), array([8.04e-04, 2.38e-05, 9.99e-01], dtype=float16), array([5.65e-05, 9.48e-06, 1.00e+00], dtype=float16), array([1.878e-05, 1.097e-05, 1.000e+00], dtype=float16), array([6.194e-04, 2.408e-05, 9.995e-01], dtype=float16), array([4.189e-04, 8.702e-06, 9.995e-01], dtype=float16), array([0.0117  , 0.001235, 0.987   ], dtype=float16), array([6.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([1.264e-05, 1.788e-07, 1.000e+00], dtype=float16), array([4.4e-06, 7.2e-07, 1.0e+00], dtype=float16), array([1.406e-03, 5.823e-05, 9.985e-01], dtype=float16), array([1.53e-04, 1.81e-05, 1.00e+00], dtype=float16), array([2.307e-05, 1.073e-06, 1.000e+00], dtype=float16), array([3.982e-05, 2.986e-05, 1.000e+00], dtype=float16), array([4.27e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.707e-03, 1.639e-04, 9.951e-01], dtype=float16), array([2.601e-04, 2.533e-05, 9.995e-01], dtype=float16), array([3.867e-04, 1.615e-04, 9.995e-01], dtype=float16), array([4.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([4.15e-05, 5.60e-06, 1.00e+00], dtype=float16), array([3.011e-04, 1.782e-05, 9.995e-01], dtype=float16), array([8.e-05, 8.e-07, 1.e+00], dtype=float16), array([2.542e-04, 2.211e-05, 9.995e-01], dtype=float16), array([1.434e-04, 6.974e-06, 1.000e+00], dtype=float16), array([5.064e-04, 4.345e-05, 9.995e-01], dtype=float16), array([3.574e-04, 1.860e-05, 9.995e-01], dtype=float16), array([1.6e-04, 5.4e-07, 1.0e+00], dtype=float16), array([1.5335e-03, 1.2469e-04, 9.9854e-01], dtype=float16), array([1.79e-05, 6.56e-07, 1.00e+00], dtype=float16), array([1.777e-04, 1.907e-06, 1.000e+00], dtype=float16), array([2.797e-04, 1.901e-05, 9.995e-01], dtype=float16), array([1.432e-03, 2.152e-05, 9.985e-01], dtype=float16), array([2.87e-05, 2.86e-06, 1.00e+00], dtype=float16), array([9.19e-05, 3.87e-06, 1.00e+00], dtype=float16), array([8.58e-06, 3.34e-06, 1.00e+00], dtype=float16), array([2.927e-05, 7.153e-07, 1.000e+00], dtype=float16), array([0.00378, 0.00596, 0.99   ], dtype=float16), array([7.067e-04, 2.325e-05, 9.995e-01], dtype=float16), array([5.037e-05, 1.556e-05, 1.000e+00], dtype=float16), array([0.0634, 0.5986, 0.338 ], dtype=float16), array([0.007  , 0.12024, 0.8726 ], dtype=float16), array([7.254e-05, 2.205e-06, 1.000e+00], dtype=float16), array([5.54e-06, 2.68e-06, 1.00e+00], dtype=float16), array([7.933e-05, 2.086e-06, 1.000e+00], dtype=float16), array([3.58e-05, 2.38e-07, 1.00e+00], dtype=float16), array([3.87e-05, 3.81e-06, 1.00e+00], dtype=float16), array([1.118e-03, 9.942e-05, 9.990e-01], dtype=float16), array([2.378e-01, 3.459e-04, 7.617e-01], dtype=float16), array([3.181e-03, 8.285e-06, 9.966e-01], dtype=float16), array([1.843e-03, 8.345e-07, 9.980e-01], dtype=float16), array([4.010e-02, 4.661e-04, 9.595e-01], dtype=float16), array([2.02e-05, 2.15e-06, 1.00e+00], dtype=float16), array([3.191e-03, 6.872e-05, 9.966e-01], dtype=float16), array([0.00827 , 0.001407, 0.99    ], dtype=float16), array([7.367e-05, 1.645e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.29e-05, 2.38e-07, 1.00e+00], dtype=float16), array([2.474e-05, 8.523e-06, 1.000e+00], dtype=float16), array([1.32e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.816e-05, 9.656e-06, 1.000e+00], dtype=float16), array([7.067e-04, 1.127e-05, 9.995e-01], dtype=float16), array([5.760e-04, 3.517e-06, 9.995e-01], dtype=float16), array([2.165e-03, 2.718e-05, 9.980e-01], dtype=float16), array([3.684e-05, 1.121e-05, 1.000e+00], dtype=float16), array([7.734e-04, 4.232e-06, 9.990e-01], dtype=float16), array([8.583e-05, 5.722e-06, 1.000e+00], dtype=float16), array([2.384e-05, 1.609e-06, 1.000e+00], dtype=float16), array([1.35e-04, 4.85e-05, 1.00e+00], dtype=float16), array([2.174e-04, 3.517e-06, 1.000e+00], dtype=float16), array([1.041e-04, 5.484e-06, 1.000e+00], dtype=float16), array([2.241e-04, 4.172e-07, 1.000e+00], dtype=float16), array([3.779e-05, 4.196e-05, 1.000e+00], dtype=float16), array([1.498e-03, 3.121e-04, 9.980e-01], dtype=float16), array([1.59e-05, 9.72e-06, 1.00e+00], dtype=float16), array([1.1325e-04, 3.4928e-05, 1.0000e+00], dtype=float16), array([9.03e-05, 3.24e-05, 1.00e+00], dtype=float16), array([5.679e-04, 5.841e-06, 9.995e-01], dtype=float16), array([3.264e-04, 1.270e-05, 9.995e-01], dtype=float16), array([0.03882, 0.2043 , 0.757  ], dtype=float16), array([2.74e-06, 8.94e-07, 1.00e+00], dtype=float16), array([6.952e-04, 9.918e-04, 9.985e-01], dtype=float16), array([2.426e-05, 2.980e-07, 1.000e+00], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([5.4e-05, 7.1e-06, 1.0e+00], dtype=float16), array([7.668e-04, 2.587e-05, 9.990e-01], dtype=float16), array([2.26e-06, 2.98e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.848e-05, 1.000e+00], dtype=float16), array([3.386e-04, 4.965e-05, 9.995e-01], dtype=float16), array([8.135e-04, 8.345e-05, 9.990e-01], dtype=float16), array([1.472e-02, 5.925e-05, 9.854e-01], dtype=float16), array([7.34e-05, 4.98e-05, 1.00e+00], dtype=float16), array([1.70e-04, 9.83e-06, 1.00e+00], dtype=float16), array([1.053e-04, 2.092e-05, 1.000e+00], dtype=float16), array([7.33e-06, 1.85e-06, 1.00e+00], dtype=float16), array([4.49e-05, 3.52e-06, 1.00e+00], dtype=float16), array([5.012e-04, 6.258e-06, 9.995e-01], dtype=float16), array([3.672e-05, 1.365e-05, 1.000e+00], dtype=float16), array([4.9e-06, 3.6e-07, 1.0e+00], dtype=float16), array([2.03e-05, 6.56e-06, 1.00e+00], dtype=float16), array([1.915e-02, 5.298e-04, 9.805e-01], dtype=float16), array([2.867e-03, 1.121e-05, 9.971e-01], dtype=float16), array([7.486e-05, 4.947e-05, 1.000e+00], dtype=float16), array([2.209e-03, 1.612e-04, 9.976e-01], dtype=float16), array([3.169e-04, 6.801e-05, 9.995e-01], dtype=float16), array([2.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([6.23e-05, 1.91e-06, 1.00e+00], dtype=float16), array([1.491e-04, 4.768e-07, 1.000e+00], dtype=float16), array([2.277e-05, 6.557e-07, 1.000e+00], dtype=float16), array([1.4e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.989e-03, 1.490e-04, 9.927e-01], dtype=float16), array([8.15e-04, 9.18e-06, 9.99e-01], dtype=float16), array([7.615e-04, 5.895e-05, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.215, 0.039, 0.746], dtype=float16), array([6.74e-06, 1.19e-07, 1.00e+00], dtype=float16), array([6.658e-05, 2.843e-05, 1.000e+00], dtype=float16), array([1.1635e-04, 7.8678e-06, 1.0000e+00], dtype=float16), array([9.146e-04, 1.371e-05, 9.990e-01], dtype=float16), array([1.248e-04, 1.252e-06, 1.000e+00], dtype=float16), array([0.0109 , 0.02061, 0.9683 ], dtype=float16), array([7.224e-05, 8.059e-05, 1.000e+00], dtype=float16), array([9.459e-05, 1.234e-05, 1.000e+00], dtype=float16), array([4.116e-03, 2.037e-04, 9.956e-01], dtype=float16), array([1.35e-05, 4.77e-07, 1.00e+00], dtype=float16), array([2.080e-05, 1.264e-05, 1.000e+00], dtype=float16), array([1.6680e-03, 1.1754e-04, 9.9805e-01], dtype=float16), array([0.11176, 0.672  , 0.2163 ], dtype=float16), array([1.788e-04, 1.782e-05, 1.000e+00], dtype=float16), array([7.87e-06, 4.17e-07, 1.00e+00], dtype=float16), array([4.3e-05, 1.0e-06, 1.0e+00], dtype=float16), array([3.034e-05, 2.444e-06, 1.000e+00], dtype=float16), array([7.51e-06, 2.26e-06, 1.00e+00], dtype=float16), array([0.002886, 0.015076, 0.982   ], dtype=float16), array([5.215e-03, 2.766e-04, 9.946e-01], dtype=float16), array([9.209e-03, 5.764e-05, 9.907e-01], dtype=float16), array([8.988e-04, 2.664e-05, 9.990e-01], dtype=float16), array([1.205e-04, 1.365e-05, 1.000e+00], dtype=float16), array([0.0556  , 0.001326, 0.943   ], dtype=float16), array([3.641e-04, 7.570e-06, 9.995e-01], dtype=float16), array([1.881e-04, 1.073e-06, 1.000e+00], dtype=float16), array([8.459e-04, 3.517e-05, 9.990e-01], dtype=float16), array([3.32e-05, 1.49e-06, 1.00e+00], dtype=float16), array([2.60e-05, 1.33e-05, 1.00e+00], dtype=float16), array([3.464e-03, 4.786e-05, 9.966e-01], dtype=float16), array([1.325e-03, 1.026e-04, 9.985e-01], dtype=float16), array([3.838e-03, 2.323e-04, 9.961e-01], dtype=float16), array([2.84e-05, 3.70e-06, 1.00e+00], dtype=float16), array([8.976e-05, 7.629e-06, 1.000e+00], dtype=float16), array([3.951e-04, 1.788e-06, 9.995e-01], dtype=float16), array([3.440e-04, 1.800e-05, 9.995e-01], dtype=float16), array([6.026e-05, 1.073e-06, 1.000e+00], dtype=float16), array([5.584e-04, 3.493e-05, 9.995e-01], dtype=float16), array([3.564e-05, 1.013e-05, 1.000e+00], dtype=float16), array([1.6775e-03, 7.7367e-05, 9.9805e-01], dtype=float16), array([6.48e-05, 1.19e-06, 1.00e+00], dtype=float16), array([1.396e-04, 9.298e-06, 1.000e+00], dtype=float16), array([1.960e-04, 1.388e-04, 9.995e-01], dtype=float16), array([3.340e-04, 5.203e-05, 9.995e-01], dtype=float16), array([2.670e-05, 2.737e-03, 9.971e-01], dtype=float16), array([3.066e-04, 1.121e-05, 9.995e-01], dtype=float16), array([3.440e-04, 2.533e-05, 9.995e-01], dtype=float16), array([5.19e-05, 4.10e-05, 1.00e+00], dtype=float16), array([3.21e-05, 4.17e-07, 1.00e+00], dtype=float16), array([2.68e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.89e-05, 2.80e-06, 1.00e+00], dtype=float16), array([0.003986, 0.00123 , 0.9946  ], dtype=float16), array([5.444e-03, 2.283e-05, 9.946e-01], dtype=float16), array([3.312e-04, 1.562e-05, 9.995e-01], dtype=float16), array([7.2e-05, 3.8e-06, 1.0e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  5
---------------------------------------------------
Extracting Features for Prompt  6  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.334e-04, 6.616e-06, 1.000e+00], dtype=float16), array([7.284e-05, 9.894e-06, 1.000e+00], dtype=float16), array([5.884e-04, 8.512e-05, 9.995e-01], dtype=float16), array([4.035e-05, 1.264e-05, 1.000e+00], dtype=float16), array([3.605e-04, 2.897e-05, 9.995e-01], dtype=float16), array([1.768e-04, 2.289e-05, 1.000e+00], dtype=float16), array([2.156e-04, 2.146e-06, 1.000e+00], dtype=float16), array([5.066e-05, 2.027e-06, 1.000e+00], dtype=float16), array([6.074e-05, 1.431e-05, 1.000e+00], dtype=float16), array([9.97e-04, 2.19e-05, 9.99e-01], dtype=float16), array([1.526e-05, 2.563e-06, 1.000e+00], dtype=float16), array([0.002497, 0.01338 , 0.984   ], dtype=float16), array([0.01897 , 0.011734, 0.969   ], dtype=float16), array([2.165e-04, 1.093e-04, 9.995e-01], dtype=float16), array([3.059e-04, 2.092e-05, 9.995e-01], dtype=float16), array([1.173e-04, 6.557e-07, 1.000e+00], dtype=float16), array([2.323e-04, 2.044e-05, 9.995e-01], dtype=float16), array([1.683e-04, 1.490e-06, 1.000e+00], dtype=float16), array([2.07e-05, 7.03e-06, 1.00e+00], dtype=float16), array([5.031e-04, 2.068e-05, 9.995e-01], dtype=float16), array([2.011e-04, 1.049e-05, 1.000e+00], dtype=float16), array([4.79e-05, 4.35e-06, 1.00e+00], dtype=float16), array([3.606e-05, 5.662e-06, 1.000e+00], dtype=float16), array([8.094e-05, 8.881e-06, 1.000e+00], dtype=float16), array([4.023e-05, 3.821e-05, 1.000e+00], dtype=float16), array([1.778e-02, 3.273e-04, 9.819e-01], dtype=float16), array([1.895e-05, 5.424e-06, 1.000e+00], dtype=float16), array([1.108e-04, 6.658e-05, 1.000e+00], dtype=float16), array([6.218e-04, 2.700e-01, 7.295e-01], dtype=float16), array([1.56e-05, 7.15e-07, 1.00e+00], dtype=float16), array([5.469e-04, 1.508e-05, 9.995e-01], dtype=float16), array([5.7e-06, 5.4e-07, 1.0e+00], dtype=float16), array([5.364e-04, 2.697e-03, 9.966e-01], dtype=float16), array([1.342e-04, 9.537e-07, 1.000e+00], dtype=float16), array([2.475e-04, 1.252e-06, 9.995e-01], dtype=float16), array([2.074e-04, 2.527e-05, 1.000e+00], dtype=float16), array([0.1614 , 0.8228 , 0.01598], dtype=float16), array([4.215e-03, 2.176e-05, 9.956e-01], dtype=float16), array([1.066e-03, 6.776e-04, 9.980e-01], dtype=float16), array([9.66e-06, 2.38e-07, 1.00e+00], dtype=float16), array([5.999e-04, 1.297e-04, 9.995e-01], dtype=float16), array([2.763e-04, 1.192e-05, 9.995e-01], dtype=float16), array([7.963e-04, 2.420e-05, 9.990e-01], dtype=float16), array([2.486e-05, 2.801e-06, 1.000e+00], dtype=float16), array([1.105e-03, 3.451e-05, 9.990e-01], dtype=float16), array([7.086e-04, 2.503e-06, 9.995e-01], dtype=float16), array([4.756e-05, 1.609e-06, 1.000e+00], dtype=float16), array([5.274e-04, 2.190e-03, 9.971e-01], dtype=float16), array([3.e-07, 2.e-07, 1.e+00], dtype=float16), array([3.626e-04, 5.543e-06, 9.995e-01], dtype=float16), array([0.3374 , 0.652  , 0.01094], dtype=float16), array([0.001812, 0.007965, 0.99    ], dtype=float16), array([0.01522, 0.01999, 0.965  ], dtype=float16), array([1.0805e-03, 8.8358e-04, 9.9805e-01], dtype=float16), array([1.074e-04, 2.646e-05, 1.000e+00], dtype=float16), array([1.693e-04, 1.913e-05, 1.000e+00], dtype=float16), array([0.010025, 0.03513 , 0.955   ], dtype=float16), array([1.28e-05, 1.13e-06, 1.00e+00], dtype=float16), array([5.474e-04, 5.603e-06, 9.995e-01], dtype=float16), array([2.247e-04, 6.258e-06, 1.000e+00], dtype=float16), array([1.503e-04, 2.503e-06, 1.000e+00], dtype=float16), array([0.02725  , 0.0014515, 0.971    ], dtype=float16), array([0.06223, 0.05145, 0.886  ], dtype=float16), array([2.937e-04, 2.265e-05, 9.995e-01], dtype=float16), array([1.913e-05, 5.960e-06, 1.000e+00], dtype=float16), array([4.83e-06, 3.04e-06, 1.00e+00], dtype=float16), array([1.157e-04, 9.656e-06, 1.000e+00], dtype=float16), array([0.001046, 0.003887, 0.995   ], dtype=float16), array([0.001772, 0.001984, 0.996   ], dtype=float16), array([8.3e-06, 1.5e-06, 1.0e+00], dtype=float16), array([1.158e-02, 5.990e-05, 9.883e-01], dtype=float16), array([2.295e-05, 8.583e-06, 1.000e+00], dtype=float16), array([1.497e-04, 2.265e-06, 1.000e+00], dtype=float16), array([2.44e-06, 4.17e-07, 1.00e+00], dtype=float16), array([1.44e-04, 7.39e-06, 1.00e+00], dtype=float16), array([9.06e-06, 4.41e-06, 1.00e+00], dtype=float16), array([1.690e-04, 1.025e-05, 1.000e+00], dtype=float16), array([3.145e-03, 8.888e-04, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5763e-05, 1.0000e+00], dtype=float16), array([0.08386 , 0.003172, 0.913   ], dtype=float16), array([1.72e-05, 1.31e-06, 1.00e+00], dtype=float16), array([7.546e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.072e-04, 4.351e-06, 1.000e+00], dtype=float16), array([3.386e-05, 2.027e-06, 1.000e+00], dtype=float16), array([0.01016, 0.00961, 0.9805 ], dtype=float16), array([7.45e-06, 2.21e-06, 1.00e+00], dtype=float16), array([3.22e-05, 7.75e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.431e-06, 1.000e+00], dtype=float16), array([1.034e-03, 2.813e-05, 9.990e-01], dtype=float16), array([1.276e-04, 3.994e-06, 1.000e+00], dtype=float16), array([2.63e-05, 1.07e-06, 1.00e+00], dtype=float16), array([1.298e-04, 1.729e-06, 1.000e+00], dtype=float16), array([1.746e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.569e-03, 7.515e-04, 9.976e-01], dtype=float16), array([3.433e-05, 3.576e-07, 1.000e+00], dtype=float16), array([0.0686, 0.0521, 0.8794], dtype=float16), array([4.053e-05, 2.503e-06, 1.000e+00], dtype=float16), array([1.0073e-04, 1.6749e-05, 1.0000e+00], dtype=float16), array([2.393e-04, 8.076e-05, 9.995e-01], dtype=float16), array([3.419e-04, 6.604e-05, 9.995e-01], dtype=float16), array([2.02e-05, 1.25e-06, 1.00e+00], dtype=float16), array([3.054e-04, 1.168e-05, 9.995e-01], dtype=float16), array([4.685e-05, 3.576e-06, 1.000e+00], dtype=float16), array([6.71e-05, 8.61e-05, 1.00e+00], dtype=float16), array([2.260e-04, 8.482e-05, 9.995e-01], dtype=float16), array([2.627e-04, 1.913e-05, 9.995e-01], dtype=float16), array([2.68e-05, 2.80e-06, 1.00e+00], dtype=float16), array([7.86e-05, 2.62e-06, 1.00e+00], dtype=float16), array([1.18e-05, 1.73e-06, 1.00e+00], dtype=float16), array([8.04e-04, 2.38e-05, 9.99e-01], dtype=float16), array([5.65e-05, 9.48e-06, 1.00e+00], dtype=float16), array([1.878e-05, 1.097e-05, 1.000e+00], dtype=float16), array([6.194e-04, 2.408e-05, 9.995e-01], dtype=float16), array([4.189e-04, 8.702e-06, 9.995e-01], dtype=float16), array([0.0117  , 0.001235, 0.987   ], dtype=float16), array([6.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([1.264e-05, 1.788e-07, 1.000e+00], dtype=float16), array([4.4e-06, 7.2e-07, 1.0e+00], dtype=float16), array([1.406e-03, 5.823e-05, 9.985e-01], dtype=float16), array([1.53e-04, 1.81e-05, 1.00e+00], dtype=float16), array([2.307e-05, 1.073e-06, 1.000e+00], dtype=float16), array([3.982e-05, 2.986e-05, 1.000e+00], dtype=float16), array([4.27e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.707e-03, 1.639e-04, 9.951e-01], dtype=float16), array([2.601e-04, 2.533e-05, 9.995e-01], dtype=float16), array([3.867e-04, 1.615e-04, 9.995e-01], dtype=float16), array([4.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([4.15e-05, 5.60e-06, 1.00e+00], dtype=float16), array([3.011e-04, 1.782e-05, 9.995e-01], dtype=float16), array([8.e-05, 8.e-07, 1.e+00], dtype=float16), array([2.542e-04, 2.211e-05, 9.995e-01], dtype=float16), array([1.434e-04, 6.974e-06, 1.000e+00], dtype=float16), array([5.064e-04, 4.345e-05, 9.995e-01], dtype=float16), array([3.574e-04, 1.860e-05, 9.995e-01], dtype=float16), array([1.6e-04, 5.4e-07, 1.0e+00], dtype=float16), array([1.5335e-03, 1.2469e-04, 9.9854e-01], dtype=float16), array([1.79e-05, 6.56e-07, 1.00e+00], dtype=float16), array([1.777e-04, 1.907e-06, 1.000e+00], dtype=float16), array([2.797e-04, 1.901e-05, 9.995e-01], dtype=float16), array([1.432e-03, 2.152e-05, 9.985e-01], dtype=float16), array([2.87e-05, 2.86e-06, 1.00e+00], dtype=float16), array([9.19e-05, 3.87e-06, 1.00e+00], dtype=float16), array([8.58e-06, 3.34e-06, 1.00e+00], dtype=float16), array([2.927e-05, 7.153e-07, 1.000e+00], dtype=float16), array([0.00378, 0.00596, 0.99   ], dtype=float16), array([7.067e-04, 2.325e-05, 9.995e-01], dtype=float16), array([5.037e-05, 1.556e-05, 1.000e+00], dtype=float16), array([0.0634, 0.5986, 0.338 ], dtype=float16), array([0.007  , 0.12024, 0.8726 ], dtype=float16), array([7.254e-05, 2.205e-06, 1.000e+00], dtype=float16), array([5.54e-06, 2.68e-06, 1.00e+00], dtype=float16), array([7.933e-05, 2.086e-06, 1.000e+00], dtype=float16), array([3.58e-05, 2.38e-07, 1.00e+00], dtype=float16), array([3.87e-05, 3.81e-06, 1.00e+00], dtype=float16), array([1.118e-03, 9.942e-05, 9.990e-01], dtype=float16), array([2.378e-01, 3.459e-04, 7.617e-01], dtype=float16), array([3.181e-03, 8.285e-06, 9.966e-01], dtype=float16), array([1.843e-03, 8.345e-07, 9.980e-01], dtype=float16), array([4.010e-02, 4.661e-04, 9.595e-01], dtype=float16), array([2.02e-05, 2.15e-06, 1.00e+00], dtype=float16), array([3.191e-03, 6.872e-05, 9.966e-01], dtype=float16), array([0.00827 , 0.001407, 0.99    ], dtype=float16), array([7.367e-05, 1.645e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.29e-05, 2.38e-07, 1.00e+00], dtype=float16), array([2.474e-05, 8.523e-06, 1.000e+00], dtype=float16), array([1.32e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.816e-05, 9.656e-06, 1.000e+00], dtype=float16), array([7.067e-04, 1.127e-05, 9.995e-01], dtype=float16), array([5.760e-04, 3.517e-06, 9.995e-01], dtype=float16), array([2.165e-03, 2.718e-05, 9.980e-01], dtype=float16), array([3.684e-05, 1.121e-05, 1.000e+00], dtype=float16), array([7.734e-04, 4.232e-06, 9.990e-01], dtype=float16), array([8.583e-05, 5.722e-06, 1.000e+00], dtype=float16), array([2.384e-05, 1.609e-06, 1.000e+00], dtype=float16), array([1.35e-04, 4.85e-05, 1.00e+00], dtype=float16), array([2.174e-04, 3.517e-06, 1.000e+00], dtype=float16), array([1.041e-04, 5.484e-06, 1.000e+00], dtype=float16), array([2.241e-04, 4.172e-07, 1.000e+00], dtype=float16), array([3.779e-05, 4.196e-05, 1.000e+00], dtype=float16), array([1.498e-03, 3.121e-04, 9.980e-01], dtype=float16), array([1.59e-05, 9.72e-06, 1.00e+00], dtype=float16), array([1.1325e-04, 3.4928e-05, 1.0000e+00], dtype=float16), array([9.03e-05, 3.24e-05, 1.00e+00], dtype=float16), array([5.679e-04, 5.841e-06, 9.995e-01], dtype=float16), array([3.264e-04, 1.270e-05, 9.995e-01], dtype=float16), array([0.03882, 0.2043 , 0.757  ], dtype=float16), array([2.74e-06, 8.94e-07, 1.00e+00], dtype=float16), array([6.952e-04, 9.918e-04, 9.985e-01], dtype=float16), array([2.426e-05, 2.980e-07, 1.000e+00], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([5.4e-05, 7.1e-06, 1.0e+00], dtype=float16), array([7.668e-04, 2.587e-05, 9.990e-01], dtype=float16), array([2.26e-06, 2.98e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.848e-05, 1.000e+00], dtype=float16), array([3.386e-04, 4.965e-05, 9.995e-01], dtype=float16), array([8.135e-04, 8.345e-05, 9.990e-01], dtype=float16), array([1.472e-02, 5.925e-05, 9.854e-01], dtype=float16), array([7.34e-05, 4.98e-05, 1.00e+00], dtype=float16), array([1.70e-04, 9.83e-06, 1.00e+00], dtype=float16), array([1.053e-04, 2.092e-05, 1.000e+00], dtype=float16), array([7.33e-06, 1.85e-06, 1.00e+00], dtype=float16), array([4.49e-05, 3.52e-06, 1.00e+00], dtype=float16), array([5.012e-04, 6.258e-06, 9.995e-01], dtype=float16), array([3.672e-05, 1.365e-05, 1.000e+00], dtype=float16), array([4.9e-06, 3.6e-07, 1.0e+00], dtype=float16), array([2.03e-05, 6.56e-06, 1.00e+00], dtype=float16), array([1.915e-02, 5.298e-04, 9.805e-01], dtype=float16), array([2.867e-03, 1.121e-05, 9.971e-01], dtype=float16), array([7.486e-05, 4.947e-05, 1.000e+00], dtype=float16), array([2.209e-03, 1.612e-04, 9.976e-01], dtype=float16), array([3.169e-04, 6.801e-05, 9.995e-01], dtype=float16), array([2.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([6.23e-05, 1.91e-06, 1.00e+00], dtype=float16), array([1.491e-04, 4.768e-07, 1.000e+00], dtype=float16), array([2.277e-05, 6.557e-07, 1.000e+00], dtype=float16), array([1.4e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.989e-03, 1.490e-04, 9.927e-01], dtype=float16), array([8.15e-04, 9.18e-06, 9.99e-01], dtype=float16), array([7.615e-04, 5.895e-05, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.215, 0.039, 0.746], dtype=float16), array([6.74e-06, 1.19e-07, 1.00e+00], dtype=float16), array([6.658e-05, 2.843e-05, 1.000e+00], dtype=float16), array([1.1635e-04, 7.8678e-06, 1.0000e+00], dtype=float16), array([9.146e-04, 1.371e-05, 9.990e-01], dtype=float16), array([1.248e-04, 1.252e-06, 1.000e+00], dtype=float16), array([0.0109 , 0.02061, 0.9683 ], dtype=float16), array([7.224e-05, 8.059e-05, 1.000e+00], dtype=float16), array([9.459e-05, 1.234e-05, 1.000e+00], dtype=float16), array([4.116e-03, 2.037e-04, 9.956e-01], dtype=float16), array([1.35e-05, 4.77e-07, 1.00e+00], dtype=float16), array([2.080e-05, 1.264e-05, 1.000e+00], dtype=float16), array([1.6680e-03, 1.1754e-04, 9.9805e-01], dtype=float16), array([0.11176, 0.672  , 0.2163 ], dtype=float16), array([1.788e-04, 1.782e-05, 1.000e+00], dtype=float16), array([7.87e-06, 4.17e-07, 1.00e+00], dtype=float16), array([4.3e-05, 1.0e-06, 1.0e+00], dtype=float16), array([3.034e-05, 2.444e-06, 1.000e+00], dtype=float16), array([7.51e-06, 2.26e-06, 1.00e+00], dtype=float16), array([0.002886, 0.015076, 0.982   ], dtype=float16), array([5.215e-03, 2.766e-04, 9.946e-01], dtype=float16), array([9.209e-03, 5.764e-05, 9.907e-01], dtype=float16), array([8.988e-04, 2.664e-05, 9.990e-01], dtype=float16), array([1.205e-04, 1.365e-05, 1.000e+00], dtype=float16), array([0.0556  , 0.001326, 0.943   ], dtype=float16), array([3.641e-04, 7.570e-06, 9.995e-01], dtype=float16), array([1.881e-04, 1.073e-06, 1.000e+00], dtype=float16), array([8.459e-04, 3.517e-05, 9.990e-01], dtype=float16), array([3.32e-05, 1.49e-06, 1.00e+00], dtype=float16), array([2.60e-05, 1.33e-05, 1.00e+00], dtype=float16), array([3.464e-03, 4.786e-05, 9.966e-01], dtype=float16), array([1.325e-03, 1.026e-04, 9.985e-01], dtype=float16), array([3.838e-03, 2.323e-04, 9.961e-01], dtype=float16), array([2.84e-05, 3.70e-06, 1.00e+00], dtype=float16), array([8.976e-05, 7.629e-06, 1.000e+00], dtype=float16), array([3.951e-04, 1.788e-06, 9.995e-01], dtype=float16), array([3.440e-04, 1.800e-05, 9.995e-01], dtype=float16), array([6.026e-05, 1.073e-06, 1.000e+00], dtype=float16), array([5.584e-04, 3.493e-05, 9.995e-01], dtype=float16), array([3.564e-05, 1.013e-05, 1.000e+00], dtype=float16), array([1.6775e-03, 7.7367e-05, 9.9805e-01], dtype=float16), array([6.48e-05, 1.19e-06, 1.00e+00], dtype=float16), array([1.396e-04, 9.298e-06, 1.000e+00], dtype=float16), array([1.960e-04, 1.388e-04, 9.995e-01], dtype=float16), array([3.340e-04, 5.203e-05, 9.995e-01], dtype=float16), array([2.670e-05, 2.737e-03, 9.971e-01], dtype=float16), array([3.066e-04, 1.121e-05, 9.995e-01], dtype=float16), array([3.440e-04, 2.533e-05, 9.995e-01], dtype=float16), array([5.19e-05, 4.10e-05, 1.00e+00], dtype=float16), array([3.21e-05, 4.17e-07, 1.00e+00], dtype=float16), array([2.68e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.89e-05, 2.80e-06, 1.00e+00], dtype=float16), array([0.003986, 0.00123 , 0.9946  ], dtype=float16), array([5.444e-03, 2.283e-05, 9.946e-01], dtype=float16), array([3.312e-04, 1.562e-05, 9.995e-01], dtype=float16), array([7.2e-05, 3.8e-06, 1.0e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  6
---------------------------------------------------
Extracting Features for Prompt  7  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.334e-04, 6.616e-06, 1.000e+00], dtype=float16), array([7.284e-05, 9.894e-06, 1.000e+00], dtype=float16), array([5.884e-04, 8.512e-05, 9.995e-01], dtype=float16), array([4.035e-05, 1.264e-05, 1.000e+00], dtype=float16), array([3.605e-04, 2.897e-05, 9.995e-01], dtype=float16), array([1.768e-04, 2.289e-05, 1.000e+00], dtype=float16), array([2.156e-04, 2.146e-06, 1.000e+00], dtype=float16), array([5.066e-05, 2.027e-06, 1.000e+00], dtype=float16), array([6.074e-05, 1.431e-05, 1.000e+00], dtype=float16), array([9.97e-04, 2.19e-05, 9.99e-01], dtype=float16), array([1.526e-05, 2.563e-06, 1.000e+00], dtype=float16), array([0.002497, 0.01338 , 0.984   ], dtype=float16), array([0.01897 , 0.011734, 0.969   ], dtype=float16), array([2.165e-04, 1.093e-04, 9.995e-01], dtype=float16), array([3.059e-04, 2.092e-05, 9.995e-01], dtype=float16), array([1.173e-04, 6.557e-07, 1.000e+00], dtype=float16), array([2.323e-04, 2.044e-05, 9.995e-01], dtype=float16), array([1.683e-04, 1.490e-06, 1.000e+00], dtype=float16), array([2.07e-05, 7.03e-06, 1.00e+00], dtype=float16), array([5.031e-04, 2.068e-05, 9.995e-01], dtype=float16), array([2.011e-04, 1.049e-05, 1.000e+00], dtype=float16), array([4.79e-05, 4.35e-06, 1.00e+00], dtype=float16), array([3.606e-05, 5.662e-06, 1.000e+00], dtype=float16), array([8.094e-05, 8.881e-06, 1.000e+00], dtype=float16), array([4.023e-05, 3.821e-05, 1.000e+00], dtype=float16), array([1.778e-02, 3.273e-04, 9.819e-01], dtype=float16), array([1.895e-05, 5.424e-06, 1.000e+00], dtype=float16), array([1.108e-04, 6.658e-05, 1.000e+00], dtype=float16), array([6.218e-04, 2.700e-01, 7.295e-01], dtype=float16), array([1.56e-05, 7.15e-07, 1.00e+00], dtype=float16), array([5.469e-04, 1.508e-05, 9.995e-01], dtype=float16), array([5.7e-06, 5.4e-07, 1.0e+00], dtype=float16), array([5.364e-04, 2.697e-03, 9.966e-01], dtype=float16), array([1.342e-04, 9.537e-07, 1.000e+00], dtype=float16), array([2.475e-04, 1.252e-06, 9.995e-01], dtype=float16), array([2.074e-04, 2.527e-05, 1.000e+00], dtype=float16), array([0.1614 , 0.8228 , 0.01598], dtype=float16), array([4.215e-03, 2.176e-05, 9.956e-01], dtype=float16), array([1.066e-03, 6.776e-04, 9.980e-01], dtype=float16), array([9.66e-06, 2.38e-07, 1.00e+00], dtype=float16), array([5.999e-04, 1.297e-04, 9.995e-01], dtype=float16), array([2.763e-04, 1.192e-05, 9.995e-01], dtype=float16), array([7.963e-04, 2.420e-05, 9.990e-01], dtype=float16), array([2.486e-05, 2.801e-06, 1.000e+00], dtype=float16), array([1.105e-03, 3.451e-05, 9.990e-01], dtype=float16), array([7.086e-04, 2.503e-06, 9.995e-01], dtype=float16), array([4.756e-05, 1.609e-06, 1.000e+00], dtype=float16), array([5.274e-04, 2.190e-03, 9.971e-01], dtype=float16), array([3.e-07, 2.e-07, 1.e+00], dtype=float16), array([3.626e-04, 5.543e-06, 9.995e-01], dtype=float16), array([0.3374 , 0.652  , 0.01094], dtype=float16), array([0.001812, 0.007965, 0.99    ], dtype=float16), array([0.01522, 0.01999, 0.965  ], dtype=float16), array([1.0805e-03, 8.8358e-04, 9.9805e-01], dtype=float16), array([1.074e-04, 2.646e-05, 1.000e+00], dtype=float16), array([1.693e-04, 1.913e-05, 1.000e+00], dtype=float16), array([0.010025, 0.03513 , 0.955   ], dtype=float16), array([1.28e-05, 1.13e-06, 1.00e+00], dtype=float16), array([5.474e-04, 5.603e-06, 9.995e-01], dtype=float16), array([2.247e-04, 6.258e-06, 1.000e+00], dtype=float16), array([1.503e-04, 2.503e-06, 1.000e+00], dtype=float16), array([0.02725  , 0.0014515, 0.971    ], dtype=float16), array([0.06223, 0.05145, 0.886  ], dtype=float16), array([2.937e-04, 2.265e-05, 9.995e-01], dtype=float16), array([1.913e-05, 5.960e-06, 1.000e+00], dtype=float16), array([4.83e-06, 3.04e-06, 1.00e+00], dtype=float16), array([1.157e-04, 9.656e-06, 1.000e+00], dtype=float16), array([0.001046, 0.003887, 0.995   ], dtype=float16), array([0.001772, 0.001984, 0.996   ], dtype=float16), array([8.3e-06, 1.5e-06, 1.0e+00], dtype=float16), array([1.158e-02, 5.990e-05, 9.883e-01], dtype=float16), array([2.295e-05, 8.583e-06, 1.000e+00], dtype=float16), array([1.497e-04, 2.265e-06, 1.000e+00], dtype=float16), array([2.44e-06, 4.17e-07, 1.00e+00], dtype=float16), array([1.44e-04, 7.39e-06, 1.00e+00], dtype=float16), array([9.06e-06, 4.41e-06, 1.00e+00], dtype=float16), array([1.690e-04, 1.025e-05, 1.000e+00], dtype=float16), array([3.145e-03, 8.888e-04, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5763e-05, 1.0000e+00], dtype=float16), array([0.08386 , 0.003172, 0.913   ], dtype=float16), array([1.72e-05, 1.31e-06, 1.00e+00], dtype=float16), array([7.546e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.072e-04, 4.351e-06, 1.000e+00], dtype=float16), array([3.386e-05, 2.027e-06, 1.000e+00], dtype=float16), array([0.01016, 0.00961, 0.9805 ], dtype=float16), array([7.45e-06, 2.21e-06, 1.00e+00], dtype=float16), array([3.22e-05, 7.75e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.431e-06, 1.000e+00], dtype=float16), array([1.034e-03, 2.813e-05, 9.990e-01], dtype=float16), array([1.276e-04, 3.994e-06, 1.000e+00], dtype=float16), array([2.63e-05, 1.07e-06, 1.00e+00], dtype=float16), array([1.298e-04, 1.729e-06, 1.000e+00], dtype=float16), array([1.746e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.569e-03, 7.515e-04, 9.976e-01], dtype=float16), array([3.433e-05, 3.576e-07, 1.000e+00], dtype=float16), array([0.0686, 0.0521, 0.8794], dtype=float16), array([4.053e-05, 2.503e-06, 1.000e+00], dtype=float16), array([1.0073e-04, 1.6749e-05, 1.0000e+00], dtype=float16), array([2.393e-04, 8.076e-05, 9.995e-01], dtype=float16), array([3.419e-04, 6.604e-05, 9.995e-01], dtype=float16), array([2.02e-05, 1.25e-06, 1.00e+00], dtype=float16), array([3.054e-04, 1.168e-05, 9.995e-01], dtype=float16), array([4.685e-05, 3.576e-06, 1.000e+00], dtype=float16), array([6.71e-05, 8.61e-05, 1.00e+00], dtype=float16), array([2.260e-04, 8.482e-05, 9.995e-01], dtype=float16), array([2.627e-04, 1.913e-05, 9.995e-01], dtype=float16), array([2.68e-05, 2.80e-06, 1.00e+00], dtype=float16), array([7.86e-05, 2.62e-06, 1.00e+00], dtype=float16), array([1.18e-05, 1.73e-06, 1.00e+00], dtype=float16), array([8.04e-04, 2.38e-05, 9.99e-01], dtype=float16), array([5.65e-05, 9.48e-06, 1.00e+00], dtype=float16), array([1.878e-05, 1.097e-05, 1.000e+00], dtype=float16), array([6.194e-04, 2.408e-05, 9.995e-01], dtype=float16), array([4.189e-04, 8.702e-06, 9.995e-01], dtype=float16), array([0.0117  , 0.001235, 0.987   ], dtype=float16), array([6.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([1.264e-05, 1.788e-07, 1.000e+00], dtype=float16), array([4.4e-06, 7.2e-07, 1.0e+00], dtype=float16), array([1.406e-03, 5.823e-05, 9.985e-01], dtype=float16), array([1.53e-04, 1.81e-05, 1.00e+00], dtype=float16), array([2.307e-05, 1.073e-06, 1.000e+00], dtype=float16), array([3.982e-05, 2.986e-05, 1.000e+00], dtype=float16), array([4.27e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.707e-03, 1.639e-04, 9.951e-01], dtype=float16), array([2.601e-04, 2.533e-05, 9.995e-01], dtype=float16), array([3.867e-04, 1.615e-04, 9.995e-01], dtype=float16), array([4.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([4.15e-05, 5.60e-06, 1.00e+00], dtype=float16), array([3.011e-04, 1.782e-05, 9.995e-01], dtype=float16), array([8.e-05, 8.e-07, 1.e+00], dtype=float16), array([2.542e-04, 2.211e-05, 9.995e-01], dtype=float16), array([1.434e-04, 6.974e-06, 1.000e+00], dtype=float16), array([5.064e-04, 4.345e-05, 9.995e-01], dtype=float16), array([3.574e-04, 1.860e-05, 9.995e-01], dtype=float16), array([1.6e-04, 5.4e-07, 1.0e+00], dtype=float16), array([1.5335e-03, 1.2469e-04, 9.9854e-01], dtype=float16), array([1.79e-05, 6.56e-07, 1.00e+00], dtype=float16), array([1.777e-04, 1.907e-06, 1.000e+00], dtype=float16), array([2.797e-04, 1.901e-05, 9.995e-01], dtype=float16), array([1.432e-03, 2.152e-05, 9.985e-01], dtype=float16), array([2.87e-05, 2.86e-06, 1.00e+00], dtype=float16), array([9.19e-05, 3.87e-06, 1.00e+00], dtype=float16), array([8.58e-06, 3.34e-06, 1.00e+00], dtype=float16), array([2.927e-05, 7.153e-07, 1.000e+00], dtype=float16), array([0.00378, 0.00596, 0.99   ], dtype=float16), array([7.067e-04, 2.325e-05, 9.995e-01], dtype=float16), array([5.037e-05, 1.556e-05, 1.000e+00], dtype=float16), array([0.0634, 0.5986, 0.338 ], dtype=float16), array([0.007  , 0.12024, 0.8726 ], dtype=float16), array([7.254e-05, 2.205e-06, 1.000e+00], dtype=float16), array([5.54e-06, 2.68e-06, 1.00e+00], dtype=float16), array([7.933e-05, 2.086e-06, 1.000e+00], dtype=float16), array([3.58e-05, 2.38e-07, 1.00e+00], dtype=float16), array([3.87e-05, 3.81e-06, 1.00e+00], dtype=float16), array([1.118e-03, 9.942e-05, 9.990e-01], dtype=float16), array([2.378e-01, 3.459e-04, 7.617e-01], dtype=float16), array([3.181e-03, 8.285e-06, 9.966e-01], dtype=float16), array([1.843e-03, 8.345e-07, 9.980e-01], dtype=float16), array([4.010e-02, 4.661e-04, 9.595e-01], dtype=float16), array([2.02e-05, 2.15e-06, 1.00e+00], dtype=float16), array([3.191e-03, 6.872e-05, 9.966e-01], dtype=float16), array([0.00827 , 0.001407, 0.99    ], dtype=float16), array([7.367e-05, 1.645e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.29e-05, 2.38e-07, 1.00e+00], dtype=float16), array([2.474e-05, 8.523e-06, 1.000e+00], dtype=float16), array([1.32e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.816e-05, 9.656e-06, 1.000e+00], dtype=float16), array([7.067e-04, 1.127e-05, 9.995e-01], dtype=float16), array([5.760e-04, 3.517e-06, 9.995e-01], dtype=float16), array([2.165e-03, 2.718e-05, 9.980e-01], dtype=float16), array([3.684e-05, 1.121e-05, 1.000e+00], dtype=float16), array([7.734e-04, 4.232e-06, 9.990e-01], dtype=float16), array([8.583e-05, 5.722e-06, 1.000e+00], dtype=float16), array([2.384e-05, 1.609e-06, 1.000e+00], dtype=float16), array([1.35e-04, 4.85e-05, 1.00e+00], dtype=float16), array([2.174e-04, 3.517e-06, 1.000e+00], dtype=float16), array([1.041e-04, 5.484e-06, 1.000e+00], dtype=float16), array([2.241e-04, 4.172e-07, 1.000e+00], dtype=float16), array([3.779e-05, 4.196e-05, 1.000e+00], dtype=float16), array([1.498e-03, 3.121e-04, 9.980e-01], dtype=float16), array([1.59e-05, 9.72e-06, 1.00e+00], dtype=float16), array([1.1325e-04, 3.4928e-05, 1.0000e+00], dtype=float16), array([9.03e-05, 3.24e-05, 1.00e+00], dtype=float16), array([5.679e-04, 5.841e-06, 9.995e-01], dtype=float16), array([3.264e-04, 1.270e-05, 9.995e-01], dtype=float16), array([0.03882, 0.2043 , 0.757  ], dtype=float16), array([2.74e-06, 8.94e-07, 1.00e+00], dtype=float16), array([6.952e-04, 9.918e-04, 9.985e-01], dtype=float16), array([2.426e-05, 2.980e-07, 1.000e+00], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([5.4e-05, 7.1e-06, 1.0e+00], dtype=float16), array([7.668e-04, 2.587e-05, 9.990e-01], dtype=float16), array([2.26e-06, 2.98e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.848e-05, 1.000e+00], dtype=float16), array([3.386e-04, 4.965e-05, 9.995e-01], dtype=float16), array([8.135e-04, 8.345e-05, 9.990e-01], dtype=float16), array([1.472e-02, 5.925e-05, 9.854e-01], dtype=float16), array([7.34e-05, 4.98e-05, 1.00e+00], dtype=float16), array([1.70e-04, 9.83e-06, 1.00e+00], dtype=float16), array([1.053e-04, 2.092e-05, 1.000e+00], dtype=float16), array([7.33e-06, 1.85e-06, 1.00e+00], dtype=float16), array([4.49e-05, 3.52e-06, 1.00e+00], dtype=float16), array([5.012e-04, 6.258e-06, 9.995e-01], dtype=float16), array([3.672e-05, 1.365e-05, 1.000e+00], dtype=float16), array([4.9e-06, 3.6e-07, 1.0e+00], dtype=float16), array([2.03e-05, 6.56e-06, 1.00e+00], dtype=float16), array([1.915e-02, 5.298e-04, 9.805e-01], dtype=float16), array([2.867e-03, 1.121e-05, 9.971e-01], dtype=float16), array([7.486e-05, 4.947e-05, 1.000e+00], dtype=float16), array([2.209e-03, 1.612e-04, 9.976e-01], dtype=float16), array([3.169e-04, 6.801e-05, 9.995e-01], dtype=float16), array([2.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([6.23e-05, 1.91e-06, 1.00e+00], dtype=float16), array([1.491e-04, 4.768e-07, 1.000e+00], dtype=float16), array([2.277e-05, 6.557e-07, 1.000e+00], dtype=float16), array([1.4e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.989e-03, 1.490e-04, 9.927e-01], dtype=float16), array([8.15e-04, 9.18e-06, 9.99e-01], dtype=float16), array([7.615e-04, 5.895e-05, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.215, 0.039, 0.746], dtype=float16), array([6.74e-06, 1.19e-07, 1.00e+00], dtype=float16), array([6.658e-05, 2.843e-05, 1.000e+00], dtype=float16), array([1.1635e-04, 7.8678e-06, 1.0000e+00], dtype=float16), array([9.146e-04, 1.371e-05, 9.990e-01], dtype=float16), array([1.248e-04, 1.252e-06, 1.000e+00], dtype=float16), array([0.0109 , 0.02061, 0.9683 ], dtype=float16), array([7.224e-05, 8.059e-05, 1.000e+00], dtype=float16), array([9.459e-05, 1.234e-05, 1.000e+00], dtype=float16), array([4.116e-03, 2.037e-04, 9.956e-01], dtype=float16), array([1.35e-05, 4.77e-07, 1.00e+00], dtype=float16), array([2.080e-05, 1.264e-05, 1.000e+00], dtype=float16), array([1.6680e-03, 1.1754e-04, 9.9805e-01], dtype=float16), array([0.11176, 0.672  , 0.2163 ], dtype=float16), array([1.788e-04, 1.782e-05, 1.000e+00], dtype=float16), array([7.87e-06, 4.17e-07, 1.00e+00], dtype=float16), array([4.3e-05, 1.0e-06, 1.0e+00], dtype=float16), array([3.034e-05, 2.444e-06, 1.000e+00], dtype=float16), array([7.51e-06, 2.26e-06, 1.00e+00], dtype=float16), array([0.002886, 0.015076, 0.982   ], dtype=float16), array([5.215e-03, 2.766e-04, 9.946e-01], dtype=float16), array([9.209e-03, 5.764e-05, 9.907e-01], dtype=float16), array([8.988e-04, 2.664e-05, 9.990e-01], dtype=float16), array([1.205e-04, 1.365e-05, 1.000e+00], dtype=float16), array([0.0556  , 0.001326, 0.943   ], dtype=float16), array([3.641e-04, 7.570e-06, 9.995e-01], dtype=float16), array([1.881e-04, 1.073e-06, 1.000e+00], dtype=float16), array([8.459e-04, 3.517e-05, 9.990e-01], dtype=float16), array([3.32e-05, 1.49e-06, 1.00e+00], dtype=float16), array([2.60e-05, 1.33e-05, 1.00e+00], dtype=float16), array([3.464e-03, 4.786e-05, 9.966e-01], dtype=float16), array([1.325e-03, 1.026e-04, 9.985e-01], dtype=float16), array([3.838e-03, 2.323e-04, 9.961e-01], dtype=float16), array([2.84e-05, 3.70e-06, 1.00e+00], dtype=float16), array([8.976e-05, 7.629e-06, 1.000e+00], dtype=float16), array([3.951e-04, 1.788e-06, 9.995e-01], dtype=float16), array([3.440e-04, 1.800e-05, 9.995e-01], dtype=float16), array([6.026e-05, 1.073e-06, 1.000e+00], dtype=float16), array([5.584e-04, 3.493e-05, 9.995e-01], dtype=float16), array([3.564e-05, 1.013e-05, 1.000e+00], dtype=float16), array([1.6775e-03, 7.7367e-05, 9.9805e-01], dtype=float16), array([6.48e-05, 1.19e-06, 1.00e+00], dtype=float16), array([1.396e-04, 9.298e-06, 1.000e+00], dtype=float16), array([1.960e-04, 1.388e-04, 9.995e-01], dtype=float16), array([3.340e-04, 5.203e-05, 9.995e-01], dtype=float16), array([2.670e-05, 2.737e-03, 9.971e-01], dtype=float16), array([3.066e-04, 1.121e-05, 9.995e-01], dtype=float16), array([3.440e-04, 2.533e-05, 9.995e-01], dtype=float16), array([5.19e-05, 4.10e-05, 1.00e+00], dtype=float16), array([3.21e-05, 4.17e-07, 1.00e+00], dtype=float16), array([2.68e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.89e-05, 2.80e-06, 1.00e+00], dtype=float16), array([0.003986, 0.00123 , 0.9946  ], dtype=float16), array([5.444e-03, 2.283e-05, 9.946e-01], dtype=float16), array([3.312e-04, 1.562e-05, 9.995e-01], dtype=float16), array([7.2e-05, 3.8e-06, 1.0e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  7
---------------------------------------------------
Extracting Features for Prompt  8  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.334e-04, 6.616e-06, 1.000e+00], dtype=float16), array([7.284e-05, 9.894e-06, 1.000e+00], dtype=float16), array([5.884e-04, 8.512e-05, 9.995e-01], dtype=float16), array([4.035e-05, 1.264e-05, 1.000e+00], dtype=float16), array([3.605e-04, 2.897e-05, 9.995e-01], dtype=float16), array([1.768e-04, 2.289e-05, 1.000e+00], dtype=float16), array([2.156e-04, 2.146e-06, 1.000e+00], dtype=float16), array([5.066e-05, 2.027e-06, 1.000e+00], dtype=float16), array([6.074e-05, 1.431e-05, 1.000e+00], dtype=float16), array([9.97e-04, 2.19e-05, 9.99e-01], dtype=float16), array([1.526e-05, 2.563e-06, 1.000e+00], dtype=float16), array([0.002497, 0.01338 , 0.984   ], dtype=float16), array([0.01897 , 0.011734, 0.969   ], dtype=float16), array([2.165e-04, 1.093e-04, 9.995e-01], dtype=float16), array([3.059e-04, 2.092e-05, 9.995e-01], dtype=float16), array([1.173e-04, 6.557e-07, 1.000e+00], dtype=float16), array([2.323e-04, 2.044e-05, 9.995e-01], dtype=float16), array([1.683e-04, 1.490e-06, 1.000e+00], dtype=float16), array([2.07e-05, 7.03e-06, 1.00e+00], dtype=float16), array([5.031e-04, 2.068e-05, 9.995e-01], dtype=float16), array([2.011e-04, 1.049e-05, 1.000e+00], dtype=float16), array([4.79e-05, 4.35e-06, 1.00e+00], dtype=float16), array([3.606e-05, 5.662e-06, 1.000e+00], dtype=float16), array([8.094e-05, 8.881e-06, 1.000e+00], dtype=float16), array([4.023e-05, 3.821e-05, 1.000e+00], dtype=float16), array([1.778e-02, 3.273e-04, 9.819e-01], dtype=float16), array([1.895e-05, 5.424e-06, 1.000e+00], dtype=float16), array([1.108e-04, 6.658e-05, 1.000e+00], dtype=float16), array([6.218e-04, 2.700e-01, 7.295e-01], dtype=float16), array([1.56e-05, 7.15e-07, 1.00e+00], dtype=float16), array([5.469e-04, 1.508e-05, 9.995e-01], dtype=float16), array([5.7e-06, 5.4e-07, 1.0e+00], dtype=float16), array([5.364e-04, 2.697e-03, 9.966e-01], dtype=float16), array([1.342e-04, 9.537e-07, 1.000e+00], dtype=float16), array([2.475e-04, 1.252e-06, 9.995e-01], dtype=float16), array([2.074e-04, 2.527e-05, 1.000e+00], dtype=float16), array([0.1614 , 0.8228 , 0.01598], dtype=float16), array([4.215e-03, 2.176e-05, 9.956e-01], dtype=float16), array([1.066e-03, 6.776e-04, 9.980e-01], dtype=float16), array([9.66e-06, 2.38e-07, 1.00e+00], dtype=float16), array([5.999e-04, 1.297e-04, 9.995e-01], dtype=float16), array([2.763e-04, 1.192e-05, 9.995e-01], dtype=float16), array([7.963e-04, 2.420e-05, 9.990e-01], dtype=float16), array([2.486e-05, 2.801e-06, 1.000e+00], dtype=float16), array([1.105e-03, 3.451e-05, 9.990e-01], dtype=float16), array([7.086e-04, 2.503e-06, 9.995e-01], dtype=float16), array([4.756e-05, 1.609e-06, 1.000e+00], dtype=float16), array([5.274e-04, 2.190e-03, 9.971e-01], dtype=float16), array([3.e-07, 2.e-07, 1.e+00], dtype=float16), array([3.626e-04, 5.543e-06, 9.995e-01], dtype=float16), array([0.3374 , 0.652  , 0.01094], dtype=float16), array([0.001812, 0.007965, 0.99    ], dtype=float16), array([0.01522, 0.01999, 0.965  ], dtype=float16), array([1.0805e-03, 8.8358e-04, 9.9805e-01], dtype=float16), array([1.074e-04, 2.646e-05, 1.000e+00], dtype=float16), array([1.693e-04, 1.913e-05, 1.000e+00], dtype=float16), array([0.010025, 0.03513 , 0.955   ], dtype=float16), array([1.28e-05, 1.13e-06, 1.00e+00], dtype=float16), array([5.474e-04, 5.603e-06, 9.995e-01], dtype=float16), array([2.247e-04, 6.258e-06, 1.000e+00], dtype=float16), array([1.503e-04, 2.503e-06, 1.000e+00], dtype=float16), array([0.02725  , 0.0014515, 0.971    ], dtype=float16), array([0.06223, 0.05145, 0.886  ], dtype=float16), array([2.937e-04, 2.265e-05, 9.995e-01], dtype=float16), array([1.913e-05, 5.960e-06, 1.000e+00], dtype=float16), array([4.83e-06, 3.04e-06, 1.00e+00], dtype=float16), array([1.157e-04, 9.656e-06, 1.000e+00], dtype=float16), array([0.001046, 0.003887, 0.995   ], dtype=float16), array([0.001772, 0.001984, 0.996   ], dtype=float16), array([8.3e-06, 1.5e-06, 1.0e+00], dtype=float16), array([1.158e-02, 5.990e-05, 9.883e-01], dtype=float16), array([2.295e-05, 8.583e-06, 1.000e+00], dtype=float16), array([1.497e-04, 2.265e-06, 1.000e+00], dtype=float16), array([2.44e-06, 4.17e-07, 1.00e+00], dtype=float16), array([1.44e-04, 7.39e-06, 1.00e+00], dtype=float16), array([9.06e-06, 4.41e-06, 1.00e+00], dtype=float16), array([1.690e-04, 1.025e-05, 1.000e+00], dtype=float16), array([3.145e-03, 8.888e-04, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5763e-05, 1.0000e+00], dtype=float16), array([0.08386 , 0.003172, 0.913   ], dtype=float16), array([1.72e-05, 1.31e-06, 1.00e+00], dtype=float16), array([7.546e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.072e-04, 4.351e-06, 1.000e+00], dtype=float16), array([3.386e-05, 2.027e-06, 1.000e+00], dtype=float16), array([0.01016, 0.00961, 0.9805 ], dtype=float16), array([7.45e-06, 2.21e-06, 1.00e+00], dtype=float16), array([3.22e-05, 7.75e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.431e-06, 1.000e+00], dtype=float16), array([1.034e-03, 2.813e-05, 9.990e-01], dtype=float16), array([1.276e-04, 3.994e-06, 1.000e+00], dtype=float16), array([2.63e-05, 1.07e-06, 1.00e+00], dtype=float16), array([1.298e-04, 1.729e-06, 1.000e+00], dtype=float16), array([1.746e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.569e-03, 7.515e-04, 9.976e-01], dtype=float16), array([3.433e-05, 3.576e-07, 1.000e+00], dtype=float16), array([0.0686, 0.0521, 0.8794], dtype=float16), array([4.053e-05, 2.503e-06, 1.000e+00], dtype=float16), array([1.0073e-04, 1.6749e-05, 1.0000e+00], dtype=float16), array([2.393e-04, 8.076e-05, 9.995e-01], dtype=float16), array([3.419e-04, 6.604e-05, 9.995e-01], dtype=float16), array([2.02e-05, 1.25e-06, 1.00e+00], dtype=float16), array([3.054e-04, 1.168e-05, 9.995e-01], dtype=float16), array([4.685e-05, 3.576e-06, 1.000e+00], dtype=float16), array([6.71e-05, 8.61e-05, 1.00e+00], dtype=float16), array([2.260e-04, 8.482e-05, 9.995e-01], dtype=float16), array([2.627e-04, 1.913e-05, 9.995e-01], dtype=float16), array([2.68e-05, 2.80e-06, 1.00e+00], dtype=float16), array([7.86e-05, 2.62e-06, 1.00e+00], dtype=float16), array([1.18e-05, 1.73e-06, 1.00e+00], dtype=float16), array([8.04e-04, 2.38e-05, 9.99e-01], dtype=float16), array([5.65e-05, 9.48e-06, 1.00e+00], dtype=float16), array([1.878e-05, 1.097e-05, 1.000e+00], dtype=float16), array([6.194e-04, 2.408e-05, 9.995e-01], dtype=float16), array([4.189e-04, 8.702e-06, 9.995e-01], dtype=float16), array([0.0117  , 0.001235, 0.987   ], dtype=float16), array([6.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([1.264e-05, 1.788e-07, 1.000e+00], dtype=float16), array([4.4e-06, 7.2e-07, 1.0e+00], dtype=float16), array([1.406e-03, 5.823e-05, 9.985e-01], dtype=float16), array([1.53e-04, 1.81e-05, 1.00e+00], dtype=float16), array([2.307e-05, 1.073e-06, 1.000e+00], dtype=float16), array([3.982e-05, 2.986e-05, 1.000e+00], dtype=float16), array([4.27e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.707e-03, 1.639e-04, 9.951e-01], dtype=float16), array([2.601e-04, 2.533e-05, 9.995e-01], dtype=float16), array([3.867e-04, 1.615e-04, 9.995e-01], dtype=float16), array([4.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([4.15e-05, 5.60e-06, 1.00e+00], dtype=float16), array([3.011e-04, 1.782e-05, 9.995e-01], dtype=float16), array([8.e-05, 8.e-07, 1.e+00], dtype=float16), array([2.542e-04, 2.211e-05, 9.995e-01], dtype=float16), array([1.434e-04, 6.974e-06, 1.000e+00], dtype=float16), array([5.064e-04, 4.345e-05, 9.995e-01], dtype=float16), array([3.574e-04, 1.860e-05, 9.995e-01], dtype=float16), array([1.6e-04, 5.4e-07, 1.0e+00], dtype=float16), array([1.5335e-03, 1.2469e-04, 9.9854e-01], dtype=float16), array([1.79e-05, 6.56e-07, 1.00e+00], dtype=float16), array([1.777e-04, 1.907e-06, 1.000e+00], dtype=float16), array([2.797e-04, 1.901e-05, 9.995e-01], dtype=float16), array([1.432e-03, 2.152e-05, 9.985e-01], dtype=float16), array([2.87e-05, 2.86e-06, 1.00e+00], dtype=float16), array([9.19e-05, 3.87e-06, 1.00e+00], dtype=float16), array([8.58e-06, 3.34e-06, 1.00e+00], dtype=float16), array([2.927e-05, 7.153e-07, 1.000e+00], dtype=float16), array([0.00378, 0.00596, 0.99   ], dtype=float16), array([7.067e-04, 2.325e-05, 9.995e-01], dtype=float16), array([5.037e-05, 1.556e-05, 1.000e+00], dtype=float16), array([0.0634, 0.5986, 0.338 ], dtype=float16), array([0.007  , 0.12024, 0.8726 ], dtype=float16), array([7.254e-05, 2.205e-06, 1.000e+00], dtype=float16), array([5.54e-06, 2.68e-06, 1.00e+00], dtype=float16), array([7.933e-05, 2.086e-06, 1.000e+00], dtype=float16), array([3.58e-05, 2.38e-07, 1.00e+00], dtype=float16), array([3.87e-05, 3.81e-06, 1.00e+00], dtype=float16), array([1.118e-03, 9.942e-05, 9.990e-01], dtype=float16), array([2.378e-01, 3.459e-04, 7.617e-01], dtype=float16), array([3.181e-03, 8.285e-06, 9.966e-01], dtype=float16), array([1.843e-03, 8.345e-07, 9.980e-01], dtype=float16), array([4.010e-02, 4.661e-04, 9.595e-01], dtype=float16), array([2.02e-05, 2.15e-06, 1.00e+00], dtype=float16), array([3.191e-03, 6.872e-05, 9.966e-01], dtype=float16), array([0.00827 , 0.001407, 0.99    ], dtype=float16), array([7.367e-05, 1.645e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.29e-05, 2.38e-07, 1.00e+00], dtype=float16), array([2.474e-05, 8.523e-06, 1.000e+00], dtype=float16), array([1.32e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.816e-05, 9.656e-06, 1.000e+00], dtype=float16), array([7.067e-04, 1.127e-05, 9.995e-01], dtype=float16), array([5.760e-04, 3.517e-06, 9.995e-01], dtype=float16), array([2.165e-03, 2.718e-05, 9.980e-01], dtype=float16), array([3.684e-05, 1.121e-05, 1.000e+00], dtype=float16), array([7.734e-04, 4.232e-06, 9.990e-01], dtype=float16), array([8.583e-05, 5.722e-06, 1.000e+00], dtype=float16), array([2.384e-05, 1.609e-06, 1.000e+00], dtype=float16), array([1.35e-04, 4.85e-05, 1.00e+00], dtype=float16), array([2.174e-04, 3.517e-06, 1.000e+00], dtype=float16), array([1.041e-04, 5.484e-06, 1.000e+00], dtype=float16), array([2.241e-04, 4.172e-07, 1.000e+00], dtype=float16), array([3.779e-05, 4.196e-05, 1.000e+00], dtype=float16), array([1.498e-03, 3.121e-04, 9.980e-01], dtype=float16), array([1.59e-05, 9.72e-06, 1.00e+00], dtype=float16), array([1.1325e-04, 3.4928e-05, 1.0000e+00], dtype=float16), array([9.03e-05, 3.24e-05, 1.00e+00], dtype=float16), array([5.679e-04, 5.841e-06, 9.995e-01], dtype=float16), array([3.264e-04, 1.270e-05, 9.995e-01], dtype=float16), array([0.03882, 0.2043 , 0.757  ], dtype=float16), array([2.74e-06, 8.94e-07, 1.00e+00], dtype=float16), array([6.952e-04, 9.918e-04, 9.985e-01], dtype=float16), array([2.426e-05, 2.980e-07, 1.000e+00], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([5.4e-05, 7.1e-06, 1.0e+00], dtype=float16), array([7.668e-04, 2.587e-05, 9.990e-01], dtype=float16), array([2.26e-06, 2.98e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.848e-05, 1.000e+00], dtype=float16), array([3.386e-04, 4.965e-05, 9.995e-01], dtype=float16), array([8.135e-04, 8.345e-05, 9.990e-01], dtype=float16), array([1.472e-02, 5.925e-05, 9.854e-01], dtype=float16), array([7.34e-05, 4.98e-05, 1.00e+00], dtype=float16), array([1.70e-04, 9.83e-06, 1.00e+00], dtype=float16), array([1.053e-04, 2.092e-05, 1.000e+00], dtype=float16), array([7.33e-06, 1.85e-06, 1.00e+00], dtype=float16), array([4.49e-05, 3.52e-06, 1.00e+00], dtype=float16), array([5.012e-04, 6.258e-06, 9.995e-01], dtype=float16), array([3.672e-05, 1.365e-05, 1.000e+00], dtype=float16), array([4.9e-06, 3.6e-07, 1.0e+00], dtype=float16), array([2.03e-05, 6.56e-06, 1.00e+00], dtype=float16), array([1.915e-02, 5.298e-04, 9.805e-01], dtype=float16), array([2.867e-03, 1.121e-05, 9.971e-01], dtype=float16), array([7.486e-05, 4.947e-05, 1.000e+00], dtype=float16), array([2.209e-03, 1.612e-04, 9.976e-01], dtype=float16), array([3.169e-04, 6.801e-05, 9.995e-01], dtype=float16), array([2.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([6.23e-05, 1.91e-06, 1.00e+00], dtype=float16), array([1.491e-04, 4.768e-07, 1.000e+00], dtype=float16), array([2.277e-05, 6.557e-07, 1.000e+00], dtype=float16), array([1.4e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.989e-03, 1.490e-04, 9.927e-01], dtype=float16), array([8.15e-04, 9.18e-06, 9.99e-01], dtype=float16), array([7.615e-04, 5.895e-05, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.215, 0.039, 0.746], dtype=float16), array([6.74e-06, 1.19e-07, 1.00e+00], dtype=float16), array([6.658e-05, 2.843e-05, 1.000e+00], dtype=float16), array([1.1635e-04, 7.8678e-06, 1.0000e+00], dtype=float16), array([9.146e-04, 1.371e-05, 9.990e-01], dtype=float16), array([1.248e-04, 1.252e-06, 1.000e+00], dtype=float16), array([0.0109 , 0.02061, 0.9683 ], dtype=float16), array([7.224e-05, 8.059e-05, 1.000e+00], dtype=float16), array([9.459e-05, 1.234e-05, 1.000e+00], dtype=float16), array([4.116e-03, 2.037e-04, 9.956e-01], dtype=float16), array([1.35e-05, 4.77e-07, 1.00e+00], dtype=float16), array([2.080e-05, 1.264e-05, 1.000e+00], dtype=float16), array([1.6680e-03, 1.1754e-04, 9.9805e-01], dtype=float16), array([0.11176, 0.672  , 0.2163 ], dtype=float16), array([1.788e-04, 1.782e-05, 1.000e+00], dtype=float16), array([7.87e-06, 4.17e-07, 1.00e+00], dtype=float16), array([4.3e-05, 1.0e-06, 1.0e+00], dtype=float16), array([3.034e-05, 2.444e-06, 1.000e+00], dtype=float16), array([7.51e-06, 2.26e-06, 1.00e+00], dtype=float16), array([0.002886, 0.015076, 0.982   ], dtype=float16), array([5.215e-03, 2.766e-04, 9.946e-01], dtype=float16), array([9.209e-03, 5.764e-05, 9.907e-01], dtype=float16), array([8.988e-04, 2.664e-05, 9.990e-01], dtype=float16), array([1.205e-04, 1.365e-05, 1.000e+00], dtype=float16), array([0.0556  , 0.001326, 0.943   ], dtype=float16), array([3.641e-04, 7.570e-06, 9.995e-01], dtype=float16), array([1.881e-04, 1.073e-06, 1.000e+00], dtype=float16), array([8.459e-04, 3.517e-05, 9.990e-01], dtype=float16), array([3.32e-05, 1.49e-06, 1.00e+00], dtype=float16), array([2.60e-05, 1.33e-05, 1.00e+00], dtype=float16), array([3.464e-03, 4.786e-05, 9.966e-01], dtype=float16), array([1.325e-03, 1.026e-04, 9.985e-01], dtype=float16), array([3.838e-03, 2.323e-04, 9.961e-01], dtype=float16), array([2.84e-05, 3.70e-06, 1.00e+00], dtype=float16), array([8.976e-05, 7.629e-06, 1.000e+00], dtype=float16), array([3.951e-04, 1.788e-06, 9.995e-01], dtype=float16), array([3.440e-04, 1.800e-05, 9.995e-01], dtype=float16), array([6.026e-05, 1.073e-06, 1.000e+00], dtype=float16), array([5.584e-04, 3.493e-05, 9.995e-01], dtype=float16), array([3.564e-05, 1.013e-05, 1.000e+00], dtype=float16), array([1.6775e-03, 7.7367e-05, 9.9805e-01], dtype=float16), array([6.48e-05, 1.19e-06, 1.00e+00], dtype=float16), array([1.396e-04, 9.298e-06, 1.000e+00], dtype=float16), array([1.960e-04, 1.388e-04, 9.995e-01], dtype=float16), array([3.340e-04, 5.203e-05, 9.995e-01], dtype=float16), array([2.670e-05, 2.737e-03, 9.971e-01], dtype=float16), array([3.066e-04, 1.121e-05, 9.995e-01], dtype=float16), array([3.440e-04, 2.533e-05, 9.995e-01], dtype=float16), array([5.19e-05, 4.10e-05, 1.00e+00], dtype=float16), array([3.21e-05, 4.17e-07, 1.00e+00], dtype=float16), array([2.68e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.89e-05, 2.80e-06, 1.00e+00], dtype=float16), array([0.003986, 0.00123 , 0.9946  ], dtype=float16), array([5.444e-03, 2.283e-05, 9.946e-01], dtype=float16), array([3.312e-04, 1.562e-05, 9.995e-01], dtype=float16), array([7.2e-05, 3.8e-06, 1.0e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  8
---------------------------------------------------
Extracting Features for Prompt  9  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.334e-04, 6.616e-06, 1.000e+00], dtype=float16), array([7.284e-05, 9.894e-06, 1.000e+00], dtype=float16), array([5.884e-04, 8.512e-05, 9.995e-01], dtype=float16), array([4.035e-05, 1.264e-05, 1.000e+00], dtype=float16), array([3.605e-04, 2.897e-05, 9.995e-01], dtype=float16), array([1.768e-04, 2.289e-05, 1.000e+00], dtype=float16), array([2.156e-04, 2.146e-06, 1.000e+00], dtype=float16), array([5.066e-05, 2.027e-06, 1.000e+00], dtype=float16), array([6.074e-05, 1.431e-05, 1.000e+00], dtype=float16), array([9.97e-04, 2.19e-05, 9.99e-01], dtype=float16), array([1.526e-05, 2.563e-06, 1.000e+00], dtype=float16), array([0.002497, 0.01338 , 0.984   ], dtype=float16), array([0.01897 , 0.011734, 0.969   ], dtype=float16), array([2.165e-04, 1.093e-04, 9.995e-01], dtype=float16), array([3.059e-04, 2.092e-05, 9.995e-01], dtype=float16), array([1.173e-04, 6.557e-07, 1.000e+00], dtype=float16), array([2.323e-04, 2.044e-05, 9.995e-01], dtype=float16), array([1.683e-04, 1.490e-06, 1.000e+00], dtype=float16), array([2.07e-05, 7.03e-06, 1.00e+00], dtype=float16), array([5.031e-04, 2.068e-05, 9.995e-01], dtype=float16), array([2.011e-04, 1.049e-05, 1.000e+00], dtype=float16), array([4.79e-05, 4.35e-06, 1.00e+00], dtype=float16), array([3.606e-05, 5.662e-06, 1.000e+00], dtype=float16), array([8.094e-05, 8.881e-06, 1.000e+00], dtype=float16), array([4.023e-05, 3.821e-05, 1.000e+00], dtype=float16), array([1.778e-02, 3.273e-04, 9.819e-01], dtype=float16), array([1.895e-05, 5.424e-06, 1.000e+00], dtype=float16), array([1.108e-04, 6.658e-05, 1.000e+00], dtype=float16), array([6.218e-04, 2.700e-01, 7.295e-01], dtype=float16), array([1.56e-05, 7.15e-07, 1.00e+00], dtype=float16), array([5.469e-04, 1.508e-05, 9.995e-01], dtype=float16), array([5.7e-06, 5.4e-07, 1.0e+00], dtype=float16), array([5.364e-04, 2.697e-03, 9.966e-01], dtype=float16), array([1.342e-04, 9.537e-07, 1.000e+00], dtype=float16), array([2.475e-04, 1.252e-06, 9.995e-01], dtype=float16), array([2.074e-04, 2.527e-05, 1.000e+00], dtype=float16), array([0.1614 , 0.8228 , 0.01598], dtype=float16), array([4.215e-03, 2.176e-05, 9.956e-01], dtype=float16), array([1.066e-03, 6.776e-04, 9.980e-01], dtype=float16), array([9.66e-06, 2.38e-07, 1.00e+00], dtype=float16), array([5.999e-04, 1.297e-04, 9.995e-01], dtype=float16), array([2.763e-04, 1.192e-05, 9.995e-01], dtype=float16), array([7.963e-04, 2.420e-05, 9.990e-01], dtype=float16), array([2.486e-05, 2.801e-06, 1.000e+00], dtype=float16), array([1.105e-03, 3.451e-05, 9.990e-01], dtype=float16), array([7.086e-04, 2.503e-06, 9.995e-01], dtype=float16), array([4.756e-05, 1.609e-06, 1.000e+00], dtype=float16), array([5.274e-04, 2.190e-03, 9.971e-01], dtype=float16), array([3.e-07, 2.e-07, 1.e+00], dtype=float16), array([3.626e-04, 5.543e-06, 9.995e-01], dtype=float16), array([0.3374 , 0.652  , 0.01094], dtype=float16), array([0.001812, 0.007965, 0.99    ], dtype=float16), array([0.01522, 0.01999, 0.965  ], dtype=float16), array([1.0805e-03, 8.8358e-04, 9.9805e-01], dtype=float16), array([1.074e-04, 2.646e-05, 1.000e+00], dtype=float16), array([1.693e-04, 1.913e-05, 1.000e+00], dtype=float16), array([0.010025, 0.03513 , 0.955   ], dtype=float16), array([1.28e-05, 1.13e-06, 1.00e+00], dtype=float16), array([5.474e-04, 5.603e-06, 9.995e-01], dtype=float16), array([2.247e-04, 6.258e-06, 1.000e+00], dtype=float16), array([1.503e-04, 2.503e-06, 1.000e+00], dtype=float16), array([0.02725  , 0.0014515, 0.971    ], dtype=float16), array([0.06223, 0.05145, 0.886  ], dtype=float16), array([2.937e-04, 2.265e-05, 9.995e-01], dtype=float16), array([1.913e-05, 5.960e-06, 1.000e+00], dtype=float16), array([4.83e-06, 3.04e-06, 1.00e+00], dtype=float16), array([1.157e-04, 9.656e-06, 1.000e+00], dtype=float16), array([0.001046, 0.003887, 0.995   ], dtype=float16), array([0.001772, 0.001984, 0.996   ], dtype=float16), array([8.3e-06, 1.5e-06, 1.0e+00], dtype=float16), array([1.158e-02, 5.990e-05, 9.883e-01], dtype=float16), array([2.295e-05, 8.583e-06, 1.000e+00], dtype=float16), array([1.497e-04, 2.265e-06, 1.000e+00], dtype=float16), array([2.44e-06, 4.17e-07, 1.00e+00], dtype=float16), array([1.44e-04, 7.39e-06, 1.00e+00], dtype=float16), array([9.06e-06, 4.41e-06, 1.00e+00], dtype=float16), array([1.690e-04, 1.025e-05, 1.000e+00], dtype=float16), array([3.145e-03, 8.888e-04, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5763e-05, 1.0000e+00], dtype=float16), array([0.08386 , 0.003172, 0.913   ], dtype=float16), array([1.72e-05, 1.31e-06, 1.00e+00], dtype=float16), array([7.546e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.072e-04, 4.351e-06, 1.000e+00], dtype=float16), array([3.386e-05, 2.027e-06, 1.000e+00], dtype=float16), array([0.01016, 0.00961, 0.9805 ], dtype=float16), array([7.45e-06, 2.21e-06, 1.00e+00], dtype=float16), array([3.22e-05, 7.75e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.431e-06, 1.000e+00], dtype=float16), array([1.034e-03, 2.813e-05, 9.990e-01], dtype=float16), array([1.276e-04, 3.994e-06, 1.000e+00], dtype=float16), array([2.63e-05, 1.07e-06, 1.00e+00], dtype=float16), array([1.298e-04, 1.729e-06, 1.000e+00], dtype=float16), array([1.746e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.569e-03, 7.515e-04, 9.976e-01], dtype=float16), array([3.433e-05, 3.576e-07, 1.000e+00], dtype=float16), array([0.0686, 0.0521, 0.8794], dtype=float16), array([4.053e-05, 2.503e-06, 1.000e+00], dtype=float16), array([1.0073e-04, 1.6749e-05, 1.0000e+00], dtype=float16), array([2.393e-04, 8.076e-05, 9.995e-01], dtype=float16), array([3.419e-04, 6.604e-05, 9.995e-01], dtype=float16), array([2.02e-05, 1.25e-06, 1.00e+00], dtype=float16), array([3.054e-04, 1.168e-05, 9.995e-01], dtype=float16), array([4.685e-05, 3.576e-06, 1.000e+00], dtype=float16), array([6.71e-05, 8.61e-05, 1.00e+00], dtype=float16), array([2.260e-04, 8.482e-05, 9.995e-01], dtype=float16), array([2.627e-04, 1.913e-05, 9.995e-01], dtype=float16), array([2.68e-05, 2.80e-06, 1.00e+00], dtype=float16), array([7.86e-05, 2.62e-06, 1.00e+00], dtype=float16), array([1.18e-05, 1.73e-06, 1.00e+00], dtype=float16), array([8.04e-04, 2.38e-05, 9.99e-01], dtype=float16), array([5.65e-05, 9.48e-06, 1.00e+00], dtype=float16), array([1.878e-05, 1.097e-05, 1.000e+00], dtype=float16), array([6.194e-04, 2.408e-05, 9.995e-01], dtype=float16), array([4.189e-04, 8.702e-06, 9.995e-01], dtype=float16), array([0.0117  , 0.001235, 0.987   ], dtype=float16), array([6.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([1.264e-05, 1.788e-07, 1.000e+00], dtype=float16), array([4.4e-06, 7.2e-07, 1.0e+00], dtype=float16), array([1.406e-03, 5.823e-05, 9.985e-01], dtype=float16), array([1.53e-04, 1.81e-05, 1.00e+00], dtype=float16), array([2.307e-05, 1.073e-06, 1.000e+00], dtype=float16), array([3.982e-05, 2.986e-05, 1.000e+00], dtype=float16), array([4.27e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.707e-03, 1.639e-04, 9.951e-01], dtype=float16), array([2.601e-04, 2.533e-05, 9.995e-01], dtype=float16), array([3.867e-04, 1.615e-04, 9.995e-01], dtype=float16), array([4.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([4.15e-05, 5.60e-06, 1.00e+00], dtype=float16), array([3.011e-04, 1.782e-05, 9.995e-01], dtype=float16), array([8.e-05, 8.e-07, 1.e+00], dtype=float16), array([2.542e-04, 2.211e-05, 9.995e-01], dtype=float16), array([1.434e-04, 6.974e-06, 1.000e+00], dtype=float16), array([5.064e-04, 4.345e-05, 9.995e-01], dtype=float16), array([3.574e-04, 1.860e-05, 9.995e-01], dtype=float16), array([1.6e-04, 5.4e-07, 1.0e+00], dtype=float16), array([1.5335e-03, 1.2469e-04, 9.9854e-01], dtype=float16), array([1.79e-05, 6.56e-07, 1.00e+00], dtype=float16), array([1.777e-04, 1.907e-06, 1.000e+00], dtype=float16), array([2.797e-04, 1.901e-05, 9.995e-01], dtype=float16), array([1.432e-03, 2.152e-05, 9.985e-01], dtype=float16), array([2.87e-05, 2.86e-06, 1.00e+00], dtype=float16), array([9.19e-05, 3.87e-06, 1.00e+00], dtype=float16), array([8.58e-06, 3.34e-06, 1.00e+00], dtype=float16), array([2.927e-05, 7.153e-07, 1.000e+00], dtype=float16), array([0.00378, 0.00596, 0.99   ], dtype=float16), array([7.067e-04, 2.325e-05, 9.995e-01], dtype=float16), array([5.037e-05, 1.556e-05, 1.000e+00], dtype=float16), array([0.0634, 0.5986, 0.338 ], dtype=float16), array([0.007  , 0.12024, 0.8726 ], dtype=float16), array([7.254e-05, 2.205e-06, 1.000e+00], dtype=float16), array([5.54e-06, 2.68e-06, 1.00e+00], dtype=float16), array([7.933e-05, 2.086e-06, 1.000e+00], dtype=float16), array([3.58e-05, 2.38e-07, 1.00e+00], dtype=float16), array([3.87e-05, 3.81e-06, 1.00e+00], dtype=float16), array([1.118e-03, 9.942e-05, 9.990e-01], dtype=float16), array([2.378e-01, 3.459e-04, 7.617e-01], dtype=float16), array([3.181e-03, 8.285e-06, 9.966e-01], dtype=float16), array([1.843e-03, 8.345e-07, 9.980e-01], dtype=float16), array([4.010e-02, 4.661e-04, 9.595e-01], dtype=float16), array([2.02e-05, 2.15e-06, 1.00e+00], dtype=float16), array([3.191e-03, 6.872e-05, 9.966e-01], dtype=float16), array([0.00827 , 0.001407, 0.99    ], dtype=float16), array([7.367e-05, 1.645e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.29e-05, 2.38e-07, 1.00e+00], dtype=float16), array([2.474e-05, 8.523e-06, 1.000e+00], dtype=float16), array([1.32e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.816e-05, 9.656e-06, 1.000e+00], dtype=float16), array([7.067e-04, 1.127e-05, 9.995e-01], dtype=float16), array([5.760e-04, 3.517e-06, 9.995e-01], dtype=float16), array([2.165e-03, 2.718e-05, 9.980e-01], dtype=float16), array([3.684e-05, 1.121e-05, 1.000e+00], dtype=float16), array([7.734e-04, 4.232e-06, 9.990e-01], dtype=float16), array([8.583e-05, 5.722e-06, 1.000e+00], dtype=float16), array([2.384e-05, 1.609e-06, 1.000e+00], dtype=float16), array([1.35e-04, 4.85e-05, 1.00e+00], dtype=float16), array([2.174e-04, 3.517e-06, 1.000e+00], dtype=float16), array([1.041e-04, 5.484e-06, 1.000e+00], dtype=float16), array([2.241e-04, 4.172e-07, 1.000e+00], dtype=float16), array([3.779e-05, 4.196e-05, 1.000e+00], dtype=float16), array([1.498e-03, 3.121e-04, 9.980e-01], dtype=float16), array([1.59e-05, 9.72e-06, 1.00e+00], dtype=float16), array([1.1325e-04, 3.4928e-05, 1.0000e+00], dtype=float16), array([9.03e-05, 3.24e-05, 1.00e+00], dtype=float16), array([5.679e-04, 5.841e-06, 9.995e-01], dtype=float16), array([3.264e-04, 1.270e-05, 9.995e-01], dtype=float16), array([0.03882, 0.2043 , 0.757  ], dtype=float16), array([2.74e-06, 8.94e-07, 1.00e+00], dtype=float16), array([6.952e-04, 9.918e-04, 9.985e-01], dtype=float16), array([2.426e-05, 2.980e-07, 1.000e+00], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([5.4e-05, 7.1e-06, 1.0e+00], dtype=float16), array([7.668e-04, 2.587e-05, 9.990e-01], dtype=float16), array([2.26e-06, 2.98e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.848e-05, 1.000e+00], dtype=float16), array([3.386e-04, 4.965e-05, 9.995e-01], dtype=float16), array([8.135e-04, 8.345e-05, 9.990e-01], dtype=float16), array([1.472e-02, 5.925e-05, 9.854e-01], dtype=float16), array([7.34e-05, 4.98e-05, 1.00e+00], dtype=float16), array([1.70e-04, 9.83e-06, 1.00e+00], dtype=float16), array([1.053e-04, 2.092e-05, 1.000e+00], dtype=float16), array([7.33e-06, 1.85e-06, 1.00e+00], dtype=float16), array([4.49e-05, 3.52e-06, 1.00e+00], dtype=float16), array([5.012e-04, 6.258e-06, 9.995e-01], dtype=float16), array([3.672e-05, 1.365e-05, 1.000e+00], dtype=float16), array([4.9e-06, 3.6e-07, 1.0e+00], dtype=float16), array([2.03e-05, 6.56e-06, 1.00e+00], dtype=float16), array([1.915e-02, 5.298e-04, 9.805e-01], dtype=float16), array([2.867e-03, 1.121e-05, 9.971e-01], dtype=float16), array([7.486e-05, 4.947e-05, 1.000e+00], dtype=float16), array([2.209e-03, 1.612e-04, 9.976e-01], dtype=float16), array([3.169e-04, 6.801e-05, 9.995e-01], dtype=float16), array([2.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([6.23e-05, 1.91e-06, 1.00e+00], dtype=float16), array([1.491e-04, 4.768e-07, 1.000e+00], dtype=float16), array([2.277e-05, 6.557e-07, 1.000e+00], dtype=float16), array([1.4e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.989e-03, 1.490e-04, 9.927e-01], dtype=float16), array([8.15e-04, 9.18e-06, 9.99e-01], dtype=float16), array([7.615e-04, 5.895e-05, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.215, 0.039, 0.746], dtype=float16), array([6.74e-06, 1.19e-07, 1.00e+00], dtype=float16), array([6.658e-05, 2.843e-05, 1.000e+00], dtype=float16), array([1.1635e-04, 7.8678e-06, 1.0000e+00], dtype=float16), array([9.146e-04, 1.371e-05, 9.990e-01], dtype=float16), array([1.248e-04, 1.252e-06, 1.000e+00], dtype=float16), array([0.0109 , 0.02061, 0.9683 ], dtype=float16), array([7.224e-05, 8.059e-05, 1.000e+00], dtype=float16), array([9.459e-05, 1.234e-05, 1.000e+00], dtype=float16), array([4.116e-03, 2.037e-04, 9.956e-01], dtype=float16), array([1.35e-05, 4.77e-07, 1.00e+00], dtype=float16), array([2.080e-05, 1.264e-05, 1.000e+00], dtype=float16), array([1.6680e-03, 1.1754e-04, 9.9805e-01], dtype=float16), array([0.11176, 0.672  , 0.2163 ], dtype=float16), array([1.788e-04, 1.782e-05, 1.000e+00], dtype=float16), array([7.87e-06, 4.17e-07, 1.00e+00], dtype=float16), array([4.3e-05, 1.0e-06, 1.0e+00], dtype=float16), array([3.034e-05, 2.444e-06, 1.000e+00], dtype=float16), array([7.51e-06, 2.26e-06, 1.00e+00], dtype=float16), array([0.002886, 0.015076, 0.982   ], dtype=float16), array([5.215e-03, 2.766e-04, 9.946e-01], dtype=float16), array([9.209e-03, 5.764e-05, 9.907e-01], dtype=float16), array([8.988e-04, 2.664e-05, 9.990e-01], dtype=float16), array([1.205e-04, 1.365e-05, 1.000e+00], dtype=float16), array([0.0556  , 0.001326, 0.943   ], dtype=float16), array([3.641e-04, 7.570e-06, 9.995e-01], dtype=float16), array([1.881e-04, 1.073e-06, 1.000e+00], dtype=float16), array([8.459e-04, 3.517e-05, 9.990e-01], dtype=float16), array([3.32e-05, 1.49e-06, 1.00e+00], dtype=float16), array([2.60e-05, 1.33e-05, 1.00e+00], dtype=float16), array([3.464e-03, 4.786e-05, 9.966e-01], dtype=float16), array([1.325e-03, 1.026e-04, 9.985e-01], dtype=float16), array([3.838e-03, 2.323e-04, 9.961e-01], dtype=float16), array([2.84e-05, 3.70e-06, 1.00e+00], dtype=float16), array([8.976e-05, 7.629e-06, 1.000e+00], dtype=float16), array([3.951e-04, 1.788e-06, 9.995e-01], dtype=float16), array([3.440e-04, 1.800e-05, 9.995e-01], dtype=float16), array([6.026e-05, 1.073e-06, 1.000e+00], dtype=float16), array([5.584e-04, 3.493e-05, 9.995e-01], dtype=float16), array([3.564e-05, 1.013e-05, 1.000e+00], dtype=float16), array([1.6775e-03, 7.7367e-05, 9.9805e-01], dtype=float16), array([6.48e-05, 1.19e-06, 1.00e+00], dtype=float16), array([1.396e-04, 9.298e-06, 1.000e+00], dtype=float16), array([1.960e-04, 1.388e-04, 9.995e-01], dtype=float16), array([3.340e-04, 5.203e-05, 9.995e-01], dtype=float16), array([2.670e-05, 2.737e-03, 9.971e-01], dtype=float16), array([3.066e-04, 1.121e-05, 9.995e-01], dtype=float16), array([3.440e-04, 2.533e-05, 9.995e-01], dtype=float16), array([5.19e-05, 4.10e-05, 1.00e+00], dtype=float16), array([3.21e-05, 4.17e-07, 1.00e+00], dtype=float16), array([2.68e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.89e-05, 2.80e-06, 1.00e+00], dtype=float16), array([0.003986, 0.00123 , 0.9946  ], dtype=float16), array([5.444e-03, 2.283e-05, 9.946e-01], dtype=float16), array([3.312e-04, 1.562e-05, 9.995e-01], dtype=float16), array([7.2e-05, 3.8e-06, 1.0e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  9
---------------------------------------------------
Extracting Features for Prompt  10  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.334e-04, 6.616e-06, 1.000e+00], dtype=float16), array([7.284e-05, 9.894e-06, 1.000e+00], dtype=float16), array([5.884e-04, 8.512e-05, 9.995e-01], dtype=float16), array([4.035e-05, 1.264e-05, 1.000e+00], dtype=float16), array([3.605e-04, 2.897e-05, 9.995e-01], dtype=float16), array([1.768e-04, 2.289e-05, 1.000e+00], dtype=float16), array([2.156e-04, 2.146e-06, 1.000e+00], dtype=float16), array([5.066e-05, 2.027e-06, 1.000e+00], dtype=float16), array([6.074e-05, 1.431e-05, 1.000e+00], dtype=float16), array([9.97e-04, 2.19e-05, 9.99e-01], dtype=float16), array([1.526e-05, 2.563e-06, 1.000e+00], dtype=float16), array([0.002497, 0.01338 , 0.984   ], dtype=float16), array([0.01897 , 0.011734, 0.969   ], dtype=float16), array([2.165e-04, 1.093e-04, 9.995e-01], dtype=float16), array([3.059e-04, 2.092e-05, 9.995e-01], dtype=float16), array([1.173e-04, 6.557e-07, 1.000e+00], dtype=float16), array([2.323e-04, 2.044e-05, 9.995e-01], dtype=float16), array([1.683e-04, 1.490e-06, 1.000e+00], dtype=float16), array([2.07e-05, 7.03e-06, 1.00e+00], dtype=float16), array([5.031e-04, 2.068e-05, 9.995e-01], dtype=float16), array([2.011e-04, 1.049e-05, 1.000e+00], dtype=float16), array([4.79e-05, 4.35e-06, 1.00e+00], dtype=float16), array([3.606e-05, 5.662e-06, 1.000e+00], dtype=float16), array([8.094e-05, 8.881e-06, 1.000e+00], dtype=float16), array([4.023e-05, 3.821e-05, 1.000e+00], dtype=float16), array([1.778e-02, 3.273e-04, 9.819e-01], dtype=float16), array([1.895e-05, 5.424e-06, 1.000e+00], dtype=float16), array([1.108e-04, 6.658e-05, 1.000e+00], dtype=float16), array([6.218e-04, 2.700e-01, 7.295e-01], dtype=float16), array([1.56e-05, 7.15e-07, 1.00e+00], dtype=float16), array([5.469e-04, 1.508e-05, 9.995e-01], dtype=float16), array([5.7e-06, 5.4e-07, 1.0e+00], dtype=float16), array([5.364e-04, 2.697e-03, 9.966e-01], dtype=float16), array([1.342e-04, 9.537e-07, 1.000e+00], dtype=float16), array([2.475e-04, 1.252e-06, 9.995e-01], dtype=float16), array([2.074e-04, 2.527e-05, 1.000e+00], dtype=float16), array([0.1614 , 0.8228 , 0.01598], dtype=float16), array([4.215e-03, 2.176e-05, 9.956e-01], dtype=float16), array([1.066e-03, 6.776e-04, 9.980e-01], dtype=float16), array([9.66e-06, 2.38e-07, 1.00e+00], dtype=float16), array([5.999e-04, 1.297e-04, 9.995e-01], dtype=float16), array([2.763e-04, 1.192e-05, 9.995e-01], dtype=float16), array([7.963e-04, 2.420e-05, 9.990e-01], dtype=float16), array([2.486e-05, 2.801e-06, 1.000e+00], dtype=float16), array([1.105e-03, 3.451e-05, 9.990e-01], dtype=float16), array([7.086e-04, 2.503e-06, 9.995e-01], dtype=float16), array([4.756e-05, 1.609e-06, 1.000e+00], dtype=float16), array([5.274e-04, 2.190e-03, 9.971e-01], dtype=float16), array([3.e-07, 2.e-07, 1.e+00], dtype=float16), array([3.626e-04, 5.543e-06, 9.995e-01], dtype=float16), array([0.3374 , 0.652  , 0.01094], dtype=float16), array([0.001812, 0.007965, 0.99    ], dtype=float16), array([0.01522, 0.01999, 0.965  ], dtype=float16), array([1.0805e-03, 8.8358e-04, 9.9805e-01], dtype=float16), array([1.074e-04, 2.646e-05, 1.000e+00], dtype=float16), array([1.693e-04, 1.913e-05, 1.000e+00], dtype=float16), array([0.010025, 0.03513 , 0.955   ], dtype=float16), array([1.28e-05, 1.13e-06, 1.00e+00], dtype=float16), array([5.474e-04, 5.603e-06, 9.995e-01], dtype=float16), array([2.247e-04, 6.258e-06, 1.000e+00], dtype=float16), array([1.503e-04, 2.503e-06, 1.000e+00], dtype=float16), array([0.02725  , 0.0014515, 0.971    ], dtype=float16), array([0.06223, 0.05145, 0.886  ], dtype=float16), array([2.937e-04, 2.265e-05, 9.995e-01], dtype=float16), array([1.913e-05, 5.960e-06, 1.000e+00], dtype=float16), array([4.83e-06, 3.04e-06, 1.00e+00], dtype=float16), array([1.157e-04, 9.656e-06, 1.000e+00], dtype=float16), array([0.001046, 0.003887, 0.995   ], dtype=float16), array([0.001772, 0.001984, 0.996   ], dtype=float16), array([8.3e-06, 1.5e-06, 1.0e+00], dtype=float16), array([1.158e-02, 5.990e-05, 9.883e-01], dtype=float16), array([2.295e-05, 8.583e-06, 1.000e+00], dtype=float16), array([1.497e-04, 2.265e-06, 1.000e+00], dtype=float16), array([2.44e-06, 4.17e-07, 1.00e+00], dtype=float16), array([1.44e-04, 7.39e-06, 1.00e+00], dtype=float16), array([9.06e-06, 4.41e-06, 1.00e+00], dtype=float16), array([1.690e-04, 1.025e-05, 1.000e+00], dtype=float16), array([3.145e-03, 8.888e-04, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5763e-05, 1.0000e+00], dtype=float16), array([0.08386 , 0.003172, 0.913   ], dtype=float16), array([1.72e-05, 1.31e-06, 1.00e+00], dtype=float16), array([7.546e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.072e-04, 4.351e-06, 1.000e+00], dtype=float16), array([3.386e-05, 2.027e-06, 1.000e+00], dtype=float16), array([0.01016, 0.00961, 0.9805 ], dtype=float16), array([7.45e-06, 2.21e-06, 1.00e+00], dtype=float16), array([3.22e-05, 7.75e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.431e-06, 1.000e+00], dtype=float16), array([1.034e-03, 2.813e-05, 9.990e-01], dtype=float16), array([1.276e-04, 3.994e-06, 1.000e+00], dtype=float16), array([2.63e-05, 1.07e-06, 1.00e+00], dtype=float16), array([1.298e-04, 1.729e-06, 1.000e+00], dtype=float16), array([1.746e-05, 1.669e-06, 1.000e+00], dtype=float16), array([1.569e-03, 7.515e-04, 9.976e-01], dtype=float16), array([3.433e-05, 3.576e-07, 1.000e+00], dtype=float16), array([0.0686, 0.0521, 0.8794], dtype=float16), array([4.053e-05, 2.503e-06, 1.000e+00], dtype=float16), array([1.0073e-04, 1.6749e-05, 1.0000e+00], dtype=float16), array([2.393e-04, 8.076e-05, 9.995e-01], dtype=float16), array([3.419e-04, 6.604e-05, 9.995e-01], dtype=float16), array([2.02e-05, 1.25e-06, 1.00e+00], dtype=float16), array([3.054e-04, 1.168e-05, 9.995e-01], dtype=float16), array([4.685e-05, 3.576e-06, 1.000e+00], dtype=float16), array([6.71e-05, 8.61e-05, 1.00e+00], dtype=float16), array([2.260e-04, 8.482e-05, 9.995e-01], dtype=float16), array([2.627e-04, 1.913e-05, 9.995e-01], dtype=float16), array([2.68e-05, 2.80e-06, 1.00e+00], dtype=float16), array([7.86e-05, 2.62e-06, 1.00e+00], dtype=float16), array([1.18e-05, 1.73e-06, 1.00e+00], dtype=float16), array([8.04e-04, 2.38e-05, 9.99e-01], dtype=float16), array([5.65e-05, 9.48e-06, 1.00e+00], dtype=float16), array([1.878e-05, 1.097e-05, 1.000e+00], dtype=float16), array([6.194e-04, 2.408e-05, 9.995e-01], dtype=float16), array([4.189e-04, 8.702e-06, 9.995e-01], dtype=float16), array([0.0117  , 0.001235, 0.987   ], dtype=float16), array([6.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([1.264e-05, 1.788e-07, 1.000e+00], dtype=float16), array([4.4e-06, 7.2e-07, 1.0e+00], dtype=float16), array([1.406e-03, 5.823e-05, 9.985e-01], dtype=float16), array([1.53e-04, 1.81e-05, 1.00e+00], dtype=float16), array([2.307e-05, 1.073e-06, 1.000e+00], dtype=float16), array([3.982e-05, 2.986e-05, 1.000e+00], dtype=float16), array([4.27e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.707e-03, 1.639e-04, 9.951e-01], dtype=float16), array([2.601e-04, 2.533e-05, 9.995e-01], dtype=float16), array([3.867e-04, 1.615e-04, 9.995e-01], dtype=float16), array([4.8e-06, 6.0e-07, 1.0e+00], dtype=float16), array([4.15e-05, 5.60e-06, 1.00e+00], dtype=float16), array([3.011e-04, 1.782e-05, 9.995e-01], dtype=float16), array([8.e-05, 8.e-07, 1.e+00], dtype=float16), array([2.542e-04, 2.211e-05, 9.995e-01], dtype=float16), array([1.434e-04, 6.974e-06, 1.000e+00], dtype=float16), array([5.064e-04, 4.345e-05, 9.995e-01], dtype=float16), array([3.574e-04, 1.860e-05, 9.995e-01], dtype=float16), array([1.6e-04, 5.4e-07, 1.0e+00], dtype=float16), array([1.5335e-03, 1.2469e-04, 9.9854e-01], dtype=float16), array([1.79e-05, 6.56e-07, 1.00e+00], dtype=float16), array([1.777e-04, 1.907e-06, 1.000e+00], dtype=float16), array([2.797e-04, 1.901e-05, 9.995e-01], dtype=float16), array([1.432e-03, 2.152e-05, 9.985e-01], dtype=float16), array([2.87e-05, 2.86e-06, 1.00e+00], dtype=float16), array([9.19e-05, 3.87e-06, 1.00e+00], dtype=float16), array([8.58e-06, 3.34e-06, 1.00e+00], dtype=float16), array([2.927e-05, 7.153e-07, 1.000e+00], dtype=float16), array([0.00378, 0.00596, 0.99   ], dtype=float16), array([7.067e-04, 2.325e-05, 9.995e-01], dtype=float16), array([5.037e-05, 1.556e-05, 1.000e+00], dtype=float16), array([0.0634, 0.5986, 0.338 ], dtype=float16), array([0.007  , 0.12024, 0.8726 ], dtype=float16), array([7.254e-05, 2.205e-06, 1.000e+00], dtype=float16), array([5.54e-06, 2.68e-06, 1.00e+00], dtype=float16), array([7.933e-05, 2.086e-06, 1.000e+00], dtype=float16), array([3.58e-05, 2.38e-07, 1.00e+00], dtype=float16), array([3.87e-05, 3.81e-06, 1.00e+00], dtype=float16), array([1.118e-03, 9.942e-05, 9.990e-01], dtype=float16), array([2.378e-01, 3.459e-04, 7.617e-01], dtype=float16), array([3.181e-03, 8.285e-06, 9.966e-01], dtype=float16), array([1.843e-03, 8.345e-07, 9.980e-01], dtype=float16), array([4.010e-02, 4.661e-04, 9.595e-01], dtype=float16), array([2.02e-05, 2.15e-06, 1.00e+00], dtype=float16), array([3.191e-03, 6.872e-05, 9.966e-01], dtype=float16), array([0.00827 , 0.001407, 0.99    ], dtype=float16), array([7.367e-05, 1.645e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.29e-05, 2.38e-07, 1.00e+00], dtype=float16), array([2.474e-05, 8.523e-06, 1.000e+00], dtype=float16), array([1.32e-05, 8.94e-07, 1.00e+00], dtype=float16), array([4.816e-05, 9.656e-06, 1.000e+00], dtype=float16), array([7.067e-04, 1.127e-05, 9.995e-01], dtype=float16), array([5.760e-04, 3.517e-06, 9.995e-01], dtype=float16), array([2.165e-03, 2.718e-05, 9.980e-01], dtype=float16), array([3.684e-05, 1.121e-05, 1.000e+00], dtype=float16), array([7.734e-04, 4.232e-06, 9.990e-01], dtype=float16), array([8.583e-05, 5.722e-06, 1.000e+00], dtype=float16), array([2.384e-05, 1.609e-06, 1.000e+00], dtype=float16), array([1.35e-04, 4.85e-05, 1.00e+00], dtype=float16), array([2.174e-04, 3.517e-06, 1.000e+00], dtype=float16), array([1.041e-04, 5.484e-06, 1.000e+00], dtype=float16), array([2.241e-04, 4.172e-07, 1.000e+00], dtype=float16), array([3.779e-05, 4.196e-05, 1.000e+00], dtype=float16), array([1.498e-03, 3.121e-04, 9.980e-01], dtype=float16), array([1.59e-05, 9.72e-06, 1.00e+00], dtype=float16), array([1.1325e-04, 3.4928e-05, 1.0000e+00], dtype=float16), array([9.03e-05, 3.24e-05, 1.00e+00], dtype=float16), array([5.679e-04, 5.841e-06, 9.995e-01], dtype=float16), array([3.264e-04, 1.270e-05, 9.995e-01], dtype=float16), array([0.03882, 0.2043 , 0.757  ], dtype=float16), array([2.74e-06, 8.94e-07, 1.00e+00], dtype=float16), array([6.952e-04, 9.918e-04, 9.985e-01], dtype=float16), array([2.426e-05, 2.980e-07, 1.000e+00], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([5.4e-05, 7.1e-06, 1.0e+00], dtype=float16), array([7.668e-04, 2.587e-05, 9.990e-01], dtype=float16), array([2.26e-06, 2.98e-07, 1.00e+00], dtype=float16), array([1.347e-04, 1.848e-05, 1.000e+00], dtype=float16), array([3.386e-04, 4.965e-05, 9.995e-01], dtype=float16), array([8.135e-04, 8.345e-05, 9.990e-01], dtype=float16), array([1.472e-02, 5.925e-05, 9.854e-01], dtype=float16), array([7.34e-05, 4.98e-05, 1.00e+00], dtype=float16), array([1.70e-04, 9.83e-06, 1.00e+00], dtype=float16), array([1.053e-04, 2.092e-05, 1.000e+00], dtype=float16), array([7.33e-06, 1.85e-06, 1.00e+00], dtype=float16), array([4.49e-05, 3.52e-06, 1.00e+00], dtype=float16), array([5.012e-04, 6.258e-06, 9.995e-01], dtype=float16), array([3.672e-05, 1.365e-05, 1.000e+00], dtype=float16), array([4.9e-06, 3.6e-07, 1.0e+00], dtype=float16), array([2.03e-05, 6.56e-06, 1.00e+00], dtype=float16), array([1.915e-02, 5.298e-04, 9.805e-01], dtype=float16), array([2.867e-03, 1.121e-05, 9.971e-01], dtype=float16), array([7.486e-05, 4.947e-05, 1.000e+00], dtype=float16), array([2.209e-03, 1.612e-04, 9.976e-01], dtype=float16), array([3.169e-04, 6.801e-05, 9.995e-01], dtype=float16), array([2.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([6.23e-05, 1.91e-06, 1.00e+00], dtype=float16), array([1.491e-04, 4.768e-07, 1.000e+00], dtype=float16), array([2.277e-05, 6.557e-07, 1.000e+00], dtype=float16), array([1.4e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.989e-03, 1.490e-04, 9.927e-01], dtype=float16), array([8.15e-04, 9.18e-06, 9.99e-01], dtype=float16), array([7.615e-04, 5.895e-05, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.215, 0.039, 0.746], dtype=float16), array([6.74e-06, 1.19e-07, 1.00e+00], dtype=float16), array([6.658e-05, 2.843e-05, 1.000e+00], dtype=float16), array([1.1635e-04, 7.8678e-06, 1.0000e+00], dtype=float16), array([9.146e-04, 1.371e-05, 9.990e-01], dtype=float16), array([1.248e-04, 1.252e-06, 1.000e+00], dtype=float16), array([0.0109 , 0.02061, 0.9683 ], dtype=float16), array([7.224e-05, 8.059e-05, 1.000e+00], dtype=float16), array([9.459e-05, 1.234e-05, 1.000e+00], dtype=float16), array([4.116e-03, 2.037e-04, 9.956e-01], dtype=float16), array([1.35e-05, 4.77e-07, 1.00e+00], dtype=float16), array([2.080e-05, 1.264e-05, 1.000e+00], dtype=float16), array([1.6680e-03, 1.1754e-04, 9.9805e-01], dtype=float16), array([0.11176, 0.672  , 0.2163 ], dtype=float16), array([1.788e-04, 1.782e-05, 1.000e+00], dtype=float16), array([7.87e-06, 4.17e-07, 1.00e+00], dtype=float16), array([4.3e-05, 1.0e-06, 1.0e+00], dtype=float16), array([3.034e-05, 2.444e-06, 1.000e+00], dtype=float16), array([7.51e-06, 2.26e-06, 1.00e+00], dtype=float16), array([0.002886, 0.015076, 0.982   ], dtype=float16), array([5.215e-03, 2.766e-04, 9.946e-01], dtype=float16), array([9.209e-03, 5.764e-05, 9.907e-01], dtype=float16), array([8.988e-04, 2.664e-05, 9.990e-01], dtype=float16), array([1.205e-04, 1.365e-05, 1.000e+00], dtype=float16), array([0.0556  , 0.001326, 0.943   ], dtype=float16), array([3.641e-04, 7.570e-06, 9.995e-01], dtype=float16), array([1.881e-04, 1.073e-06, 1.000e+00], dtype=float16), array([8.459e-04, 3.517e-05, 9.990e-01], dtype=float16), array([3.32e-05, 1.49e-06, 1.00e+00], dtype=float16), array([2.60e-05, 1.33e-05, 1.00e+00], dtype=float16), array([3.464e-03, 4.786e-05, 9.966e-01], dtype=float16), array([1.325e-03, 1.026e-04, 9.985e-01], dtype=float16), array([3.838e-03, 2.323e-04, 9.961e-01], dtype=float16), array([2.84e-05, 3.70e-06, 1.00e+00], dtype=float16), array([8.976e-05, 7.629e-06, 1.000e+00], dtype=float16), array([3.951e-04, 1.788e-06, 9.995e-01], dtype=float16), array([3.440e-04, 1.800e-05, 9.995e-01], dtype=float16), array([6.026e-05, 1.073e-06, 1.000e+00], dtype=float16), array([5.584e-04, 3.493e-05, 9.995e-01], dtype=float16), array([3.564e-05, 1.013e-05, 1.000e+00], dtype=float16), array([1.6775e-03, 7.7367e-05, 9.9805e-01], dtype=float16), array([6.48e-05, 1.19e-06, 1.00e+00], dtype=float16), array([1.396e-04, 9.298e-06, 1.000e+00], dtype=float16), array([1.960e-04, 1.388e-04, 9.995e-01], dtype=float16), array([3.340e-04, 5.203e-05, 9.995e-01], dtype=float16), array([2.670e-05, 2.737e-03, 9.971e-01], dtype=float16), array([3.066e-04, 1.121e-05, 9.995e-01], dtype=float16), array([3.440e-04, 2.533e-05, 9.995e-01], dtype=float16), array([5.19e-05, 4.10e-05, 1.00e+00], dtype=float16), array([3.21e-05, 4.17e-07, 1.00e+00], dtype=float16), array([2.68e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.89e-05, 2.80e-06, 1.00e+00], dtype=float16), array([0.003986, 0.00123 , 0.9946  ], dtype=float16), array([5.444e-03, 2.283e-05, 9.946e-01], dtype=float16), array([3.312e-04, 1.562e-05, 9.995e-01], dtype=float16), array([7.2e-05, 3.8e-06, 1.0e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  10
---------------------------------------------------
Training based on Prompt 1
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
RMSE for train:  6.283948225801506
MAE for train:  4.915291391923619
RMSE for dev:  5.871164599631972
MAE for dev:  4.721168424357923
RMSE for test:  7.095553189780709
MAE for test:  5.6341451441197075
weights:  {'1': 0.21181197324812653}
Training based on Prompt 2
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
RMSE for train:  6.283948225801506
MAE for train:  4.915291391923619
RMSE for dev:  5.871164599631972
MAE for dev:  4.721168424357923
RMSE for test:  7.095553189780709
MAE for test:  5.6341451441197075
weights:  {'1': 0.21181197324812653, '2': 0.21181197324812653}
Training based on Prompt 3
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
RMSE for train:  6.283948225801506
MAE for train:  4.915291391923619
RMSE for dev:  5.871164599631972
MAE for dev:  4.721168424357923
RMSE for test:  7.095553189780709
MAE for test:  5.6341451441197075
weights:  {'1': 0.21181197324812653, '2': 0.21181197324812653, '3': 0.21181197324812653}
Training based on Prompt 4
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
RMSE for train:  6.283948225801506
MAE for train:  4.915291391923619
RMSE for dev:  5.871164599631972
MAE for dev:  4.721168424357923
RMSE for test:  7.095553189780709
MAE for test:  5.6341451441197075
weights:  {'1': 0.21181197324812653, '2': 0.21181197324812653, '3': 0.21181197324812653, '4': 0.21181197324812653}
Training based on Prompt 5
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
RMSE for train:  6.283948225801506
MAE for train:  4.915291391923619
RMSE for dev:  5.871164599631972
MAE for dev:  4.721168424357923
RMSE for test:  7.095553189780709
MAE for test:  5.6341451441197075
weights:  {'1': 0.21181197324812653, '2': 0.21181197324812653, '3': 0.21181197324812653, '4': 0.21181197324812653, '5': 0.21181197324812653}
Training based on Prompt 6
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
RMSE for train:  6.283948225801506
MAE for train:  4.915291391923619
RMSE for dev:  5.871164599631972
MAE for dev:  4.721168424357923
RMSE for test:  7.095553189780709
MAE for test:  5.6341451441197075
weights:  {'1': 0.21181197324812653, '2': 0.21181197324812653, '3': 0.21181197324812653, '4': 0.21181197324812653, '5': 0.21181197324812653, '6': 0.21181197324812653}
Training based on Prompt 7
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
RMSE for train:  6.283948225801506
MAE for train:  4.915291391923619
RMSE for dev:  5.871164599631972
MAE for dev:  4.721168424357923
RMSE for test:  7.095553189780709
MAE for test:  5.6341451441197075
weights:  {'1': 0.21181197324812653, '2': 0.21181197324812653, '3': 0.21181197324812653, '4': 0.21181197324812653, '5': 0.21181197324812653, '6': 0.21181197324812653, '7': 0.21181197324812653}
Training based on Prompt 8
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
RMSE for train:  6.283948225801506
MAE for train:  4.915291391923619
RMSE for dev:  5.871164599631972
MAE for dev:  4.721168424357923
RMSE for test:  7.095553189780709
MAE for test:  5.6341451441197075
weights:  {'1': 0.21181197324812653, '2': 0.21181197324812653, '3': 0.21181197324812653, '4': 0.21181197324812653, '5': 0.21181197324812653, '6': 0.21181197324812653, '7': 0.21181197324812653, '8': 0.21181197324812653}
Training based on Prompt 9
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
RMSE for train:  6.283948225801506
MAE for train:  4.915291391923619
RMSE for dev:  5.871164599631972
MAE for dev:  4.721168424357923
RMSE for test:  7.095553189780709
MAE for test:  5.6341451441197075
weights:  {'1': 0.21181197324812653, '2': 0.21181197324812653, '3': 0.21181197324812653, '4': 0.21181197324812653, '5': 0.21181197324812653, '6': 0.21181197324812653, '7': 0.21181197324812653, '8': 0.21181197324812653, '9': 0.21181197324812653}
Training based on Prompt 10
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
RMSE for train:  6.283948225801506
MAE for train:  4.915291391923619
RMSE for dev:  5.871164599631972
MAE for dev:  4.721168424357923
RMSE for test:  7.095553189780709
MAE for test:  5.6341451441197075
weights:  {'1': 0.21181197324812653, '2': 0.21181197324812653, '3': 0.21181197324812653, '4': 0.21181197324812653, '5': 0.21181197324812653, '6': 0.21181197324812653, '7': 0.21181197324812653, '8': 0.21181197324812653, '9': 0.21181197324812653, '10': 0.21181197324812653}
prompt_number:  1
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  2
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  3
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  4
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  5
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  6
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  7
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  8
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  9
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  10
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
average_array_train.shape:  (163, 3)
average_array_dev.shape:  (56, 3)
average_array_test.shape:  (56, 3)
final y_train shape:  (163,)
RMSE for train:  5.905099068597116
MAE for train:  4.591976563056417
RMSE for dev:  5.857543550193448
MAE for dev:  4.649143765389182
RMSE for test:  7.391476449130567
MAE for test:  5.825598704309746
y_dev:  [ 2  3  0  8 11 20  8  5  7  9 11 16 11 12  9  9  2 16  0 17  2  9 11  0
 12  3  0  6  2 10  8  0 18  6 18  4  1  1  2  4  0  6  7  1  9  4 12  3
  1  4  0  3  0  7 19  0]
y_pred_dev:  [ 4.89895406  4.90067417  4.89901471  4.90327143  4.97860581  4.96398458
  5.1446738   4.902139    4.98910259  4.90715689  4.9002214   4.91476224
  4.92134267  4.90912876  4.92188029  4.90387044  5.08764666  4.89980181
  4.91168648  4.90907509  4.96323418  4.93739424 12.16571587  4.89788373
  5.03680713  4.90017674  4.89753293  4.90377795  4.98952576  4.89779251
  4.91316774  4.94067536  4.99766496  6.55027158  4.90817367  4.91652248
  4.91007843  4.89843298  4.90257365  4.95601176  4.90224383  4.89806761
  4.90007978  7.05607761  5.22658475  4.90831147  5.16002349  4.93931564
  4.89777746  4.90439362  4.91376396  4.90003895  4.8978663   5.70264335
  4.99388747  4.99073197]
y_test:  [ 5 13 12  2  5  7  0  3  4  6  0  0  9 22  2  0  0  0  2  9 15 16 19 17
 19  3  5  7 12  8 11 16 19  9  2  5  7  0  8 12  6 19 13 11  3 13 16 20
  0 10  7  0  7 15  1  3]
y_pred_test:  [20.61931249  4.89826654  4.90629681  4.91057859  5.00492672  4.91113459
  7.20647573  4.90969092  4.90846554  5.38154411  4.89903024  4.90047189
  5.09570988  1.01389415  4.91789132  4.89840385  4.90226989  4.90096958
  4.8984786   6.07352548  5.50689017  5.94375637  5.00390846  4.91135303
 10.73937615  4.94121556  4.91801255  4.99862337  4.90121508  4.90105219
  5.29633929  5.05771874  5.34997414  4.90081579  4.90769014  4.94425901
  4.9395729   4.90413451  4.96374319  4.90195656  5.09454368  4.90462359
  4.91319098  4.92995637  4.940322    5.06413362  4.93515111  4.93996352
  4.90535367  4.90103263  4.90072265  4.89973137  5.42502582  5.51823504
  4.93806358  4.9055688 ]
=== JOB_STATISTICS ===
=== current date     : Wed 14 Feb 2024 02:01:49 PM CET
= Job-ID             : 768736 on tinygpu
= Job-Name           : job_llama_fine_tuned_in_pipeline.sh
= Job-Command        : /home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline/job_llama_fine_tuned_in_pipeline.sh
= Initial workdir    : /home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline
= Queue/Partition    : a100
= Slurm account      : empk with QOS=normal
= Requested resources: cpu=32,mem=120000M,node=1,billing=32,gres/gpu=1,gres/gpu:a100=1 for 20:00:00
= Elapsed runtime    : 00:18:14
= Total RAM usage    : 1.2 GiB of requested 117 GiB (1.0%)   
= Node list          : tg093
= Subm/Elig/Start/End: 2024-02-14T13:43:33 / 2024-02-14T13:43:33 / 2024-02-14T13:43:34 / 2024-02-14T14:01:48
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           76.5G   104.9G   209.7G        N/A  45,103      500K   1,000K        N/A    
    /home/vault       1479.7G     0.0K     0.0K        N/A      74K     300K     450K        N/A    
    /home/woody        311.0G   500.0G   750.0G        N/A     314K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:81:00.0, 785759, 86 %, 49 %, 11228 MiB, 1078947 ms
