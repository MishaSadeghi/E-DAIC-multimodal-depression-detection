### Starting TaskPrologue of job 768599 on tg093 at Wed 14 Feb 2024 11:40:49 AM CET
Running on cores 0-31 with governor ondemand
Wed Feb 14 11:40:50 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:01:00.0 Off |                    0 |
| N/A   34C    P0              57W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
### Finished TaskPrologue

/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline
def_dev:      id  ...                                               text
0  300  ...   which will record your body. So I'll show you...
1  301  ...   Yeah, there's all sorts of different studies ...
2  306  ...   Okay, looks like we're good. But let's move a...
3  317  ...   Okay. How long is this? This is probably goin...
4  320  ...   Okay, everything looks good. Okay. Perfect. O...

[5 rows x 7 columns]
max PHQ score in df_train:  23
prompt_number:  1
prompt:   Your task is to read the following text which is an interview with a person and to summarize the key points that might be related to the depression of the person. Be concise and to the point.
prompt_number:  2
prompt:   Your task is to read the following text which is an interview with a person and to summarize the key points that might be related to the depression of the person. Be concise and to the point. It is very essential that you write your answer in the first-person perspective, as if the interviewee is narrating about himself or herself. 
prompt_number:  3
prompt:   After reading the interview, briefly summarize the main aspects that pertain to the person's depression. 
prompt_number:  4
prompt:   Based on the interview, highlight the key factors that might be indicative of the interviewee's depression. 
prompt_number:  5
prompt:   Your task is to summarize the interviewee's main points that could be linked to their depression. Keep it concise. 
prompt_number:  6
prompt:   After reading the interview, identify and summarize the main challenges or difficulties the interviewee faces that are indicative of depression. 
prompt_number:  7
prompt:   Based on the interview, provide a concise analysis of the interviewee's emotional state and behaviors that may indicate the presence of depression. 
prompt_number:  8
prompt:   Read the interview carefully and extract the most significant indicators of depression exhibited by the interviewee. Summarize them concisely. 
prompt_number:  9
prompt:   Your task is to analyze the interviewee's responses and highlight the key signs or symptoms of depression that are evident in the interview. 
prompt_number:  10
prompt:   Provide a brief summary of the interview, focusing on aspects that strongly suggest the presence of depression in the interviewee. 
Extracting Features for Prompt  1  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:33<00:00, 15.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:33<00:00, 16.98s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-02-14 12:09:23.317094: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
output:  [array([2.07e-05, 8.78e-03, 9.91e-01], dtype=float16), array([7.510e-06, 6.037e-04, 9.995e-01], dtype=float16), array([2.530e-04, 1.011e-02, 9.897e-01], dtype=float16), array([4.363e-04, 5.903e-04, 9.990e-01], dtype=float16), array([1.440e-04, 5.745e-03, 9.941e-01], dtype=float16), array([3.877e-04, 6.366e-02, 9.360e-01], dtype=float16), array([0.00927 , 0.001953, 0.989   ], dtype=float16), array([2.384e-06, 1.271e-04, 1.000e+00], dtype=float16), array([0.001191, 0.02646 , 0.972   ], dtype=float16), array([2.01e-05, 2.05e-05, 1.00e+00], dtype=float16), array([1.686e-04, 8.644e-03, 9.912e-01], dtype=float16), array([6.554e-03, 4.107e-05, 9.932e-01], dtype=float16), array([5.260e-04, 6.494e-01, 3.499e-01], dtype=float16), array([2.549e-04, 4.951e-03, 9.946e-01], dtype=float16), array([7.117e-05, 6.577e-01, 3.425e-01], dtype=float16), array([6.889e-03, 1.519e-04, 9.932e-01], dtype=float16), array([3.200e-04, 1.932e-04, 9.995e-01], dtype=float16), array([1.550e-06, 2.682e-03, 9.976e-01], dtype=float16), array([2.739e-04, 1.636e-02, 9.834e-01], dtype=float16), array([3.860e-01, 5.202e-04, 6.133e-01], dtype=float16), array([1.156e-05, 3.332e-05, 1.000e+00], dtype=float16), array([0.00754, 0.2837 , 0.7085 ], dtype=float16), array([2.575e-04, 6.353e-01, 3.647e-01], dtype=float16), array([3.595e-04, 3.617e-04, 9.995e-01], dtype=float16), array([0.001156, 0.745   , 0.2537  ], dtype=float16), array([3.054e-04, 7.858e-03, 9.917e-01], dtype=float16), array([7.766e-05, 6.073e-03, 9.937e-01], dtype=float16), array([8.464e-06, 2.897e-05, 1.000e+00], dtype=float16), array([3.648e-04, 1.535e-02, 9.844e-01], dtype=float16), array([7.075e-05, 7.147e-02, 9.287e-01], dtype=float16), array([2.742e-04, 2.732e-04, 9.995e-01], dtype=float16), array([2.342e-05, 3.209e-04, 9.995e-01], dtype=float16), array([1.109e-04, 2.438e-02, 9.756e-01], dtype=float16), array([0.003641, 0.001235, 0.995   ], dtype=float16), array([0.00651, 0.4512 , 0.542  ], dtype=float16), array([0.10425 , 0.005215, 0.8906  ], dtype=float16), array([4.327e-05, 6.886e-04, 9.995e-01], dtype=float16), array([3.952e-05, 9.634e-01, 3.677e-02], dtype=float16), array([1.150e-05, 1.979e-04, 1.000e+00], dtype=float16), array([5.178e-04, 1.385e-01, 8.608e-01], dtype=float16), array([3.320e-05, 8.675e-03, 9.912e-01], dtype=float16), array([0.006775, 0.005413, 0.988   ], dtype=float16), array([0.0471, 0.361 , 0.592 ], dtype=float16), array([6.920e-05, 1.943e-05, 1.000e+00], dtype=float16), array([9.37e-04, 3.56e-05, 9.99e-01], dtype=float16), array([0.003 , 0.721 , 0.2756], dtype=float16), array([2.50e-06, 1.97e-06, 1.00e+00], dtype=float16), array([1.448e-05, 8.736e-04, 9.990e-01], dtype=float16), array([3.04e-06, 5.96e-08, 1.00e+00], dtype=float16), array([8.88e-06, 1.29e-05, 1.00e+00], dtype=float16), array([1.055e-05, 3.421e-05, 1.000e+00], dtype=float16), array([3.655e-04, 1.968e-03, 9.976e-01], dtype=float16), array([0.02951, 0.01866, 0.9517 ], dtype=float16), array([3.695e-05, 1.500e-02, 9.849e-01], dtype=float16), array([1.317e-04, 6.977e-03, 9.927e-01], dtype=float16), array([0.0021, 0.285 , 0.713 ], dtype=float16), array([0.002254, 0.0429  , 0.955   ], dtype=float16), array([5.112e-04, 4.122e-04, 9.990e-01], dtype=float16), array([0.002   , 0.001652, 0.9966  ], dtype=float16), array([0.00603, 0.02052, 0.9736 ], dtype=float16), array([6.020e-06, 2.635e-05, 1.000e+00], dtype=float16), array([0.00355, 0.0351 , 0.9614 ], dtype=float16), array([8.e-07, 2.e-07, 1.e+00], dtype=float16), array([1.94e-05, 1.93e-04, 1.00e+00], dtype=float16), array([0.002884, 0.0214  , 0.9756  ], dtype=float16), array([7.749e-06, 7.763e-04, 9.990e-01], dtype=float16), array([1.657e-05, 1.429e-01, 8.569e-01], dtype=float16), array([6.20e-05, 7.32e-04, 9.99e-01], dtype=float16), array([4.679e-05, 7.195e-04, 9.990e-01], dtype=float16), array([1.6e-05, 6.3e-06, 1.0e+00], dtype=float16), array([0.002316, 0.01102 , 0.987   ], dtype=float16), array([8.3e-07, 6.0e-08, 1.0e+00], dtype=float16), array([7.004e-05, 3.998e-03, 9.961e-01], dtype=float16), array([5.662e-05, 1.444e-03, 9.985e-01], dtype=float16), array([1.180e-04, 1.636e-03, 9.980e-01], dtype=float16), array([1.281e-05, 3.468e-03, 9.966e-01], dtype=float16), array([0.005325, 0.001263, 0.9937  ], dtype=float16), array([1.186e-05, 2.390e-04, 9.995e-01], dtype=float16), array([1.45e-05, 1.87e-05, 1.00e+00], dtype=float16), array([2.238e-04, 2.454e-01, 7.544e-01], dtype=float16), array([8.434e-05, 2.103e-02, 9.790e-01], dtype=float16), array([1.504e-04, 2.609e-02, 9.736e-01], dtype=float16), array([1.13e-04, 8.36e-05, 1.00e+00], dtype=float16), array([3.821e-05, 2.770e-04, 9.995e-01], dtype=float16), array([3.612e-05, 5.150e-03, 9.946e-01], dtype=float16), array([0.0013275, 0.8643   , 0.1346   ], dtype=float16), array([0.2332 , 0.04706, 0.7197 ], dtype=float16), array([1.456e-04, 8.173e-04, 9.990e-01], dtype=float16), array([0.3423 , 0.02925, 0.6284 ], dtype=float16), array([5.943e-05, 4.451e-04, 9.995e-01], dtype=float16), array([5.305e-06, 1.022e-03, 9.990e-01], dtype=float16), array([1.49e-06, 1.03e-05, 1.00e+00], dtype=float16), array([2.973e-04, 1.137e-02, 9.883e-01], dtype=float16), array([1.956e-04, 6.157e-03, 9.937e-01], dtype=float16), array([1.770e-05, 3.054e-04, 9.995e-01], dtype=float16), array([0.1417, 0.598 , 0.2603], dtype=float16), array([4.977e-05, 1.883e-03, 9.980e-01], dtype=float16), array([5.859e-05, 1.266e-01, 8.735e-01], dtype=float16), array([3.046e-05, 4.268e-04, 9.995e-01], dtype=float16), array([3.76e-06, 3.34e-06, 1.00e+00], dtype=float16), array([5.546e-04, 4.098e-04, 9.990e-01], dtype=float16), array([4.250e-05, 7.137e-03, 9.927e-01], dtype=float16), array([2.41e-05, 1.65e-05, 1.00e+00], dtype=float16), array([3.403e-05, 3.305e-03, 9.966e-01], dtype=float16), array([4.631e-05, 2.987e-03, 9.971e-01], dtype=float16), array([4.804e-05, 1.355e-04, 1.000e+00], dtype=float16), array([2.921e-06, 1.003e-04, 1.000e+00], dtype=float16), array([1.922e-04, 3.910e-05, 1.000e+00], dtype=float16), array([3.868e-05, 2.842e-01, 7.158e-01], dtype=float16), array([3.64e-06, 2.35e-05, 1.00e+00], dtype=float16), array([1.336e-04, 2.891e-05, 1.000e+00], dtype=float16), array([1.633e-05, 1.611e-03, 9.985e-01], dtype=float16), array([4.077e-05, 1.597e-03, 9.985e-01], dtype=float16), array([6.807e-05, 1.081e-03, 9.990e-01], dtype=float16), array([3.76e-06, 9.86e-05, 1.00e+00], dtype=float16), array([1.490e-05, 2.444e-04, 9.995e-01], dtype=float16), array([3.934e-06, 1.316e-04, 1.000e+00], dtype=float16), array([3.207e-05, 4.784e-03, 9.951e-01], dtype=float16), array([4.294e-04, 4.818e-03, 9.946e-01], dtype=float16), array([2.736e-05, 2.672e-03, 9.971e-01], dtype=float16), array([1.264e-05, 3.317e-03, 9.966e-01], dtype=float16), array([8.71e-05, 8.54e-04, 9.99e-01], dtype=float16), array([1.508e-05, 7.644e-04, 9.990e-01], dtype=float16), array([5.417e-04, 1.074e-02, 9.888e-01], dtype=float16), array([2.57e-05, 2.87e-05, 1.00e+00], dtype=float16), array([0.007084, 0.4033  , 0.59    ], dtype=float16), array([6.735e-06, 3.332e-03, 9.966e-01], dtype=float16), array([3.79e-05, 1.04e-03, 9.99e-01], dtype=float16), array([1.089e-04, 7.835e-03, 9.922e-01], dtype=float16), array([1.013e-06, 5.186e-05, 1.000e+00], dtype=float16), array([1.115e-05, 1.372e-02, 9.863e-01], dtype=float16), array([1.429e-04, 3.619e-02, 9.639e-01], dtype=float16), array([1.132e-04, 3.274e-01, 6.724e-01], dtype=float16), array([0.0086 , 0.08105, 0.91   ], dtype=float16), array([8.821e-06, 5.779e-04, 9.995e-01], dtype=float16), array([1.31e-06, 4.95e-06, 1.00e+00], dtype=float16), array([7.004e-05, 5.344e-02, 9.463e-01], dtype=float16), array([3.81e-06, 1.33e-05, 1.00e+00], dtype=float16), array([3.743e-05, 1.761e-02, 9.824e-01], dtype=float16), array([5.42e-06, 4.77e-06, 1.00e+00], dtype=float16), array([1.854e-05, 5.341e-05, 1.000e+00], dtype=float16), array([0.001829, 0.02048 , 0.9775  ], dtype=float16), array([5.805e-05, 8.267e-05, 1.000e+00], dtype=float16), array([8.516e-04, 1.265e-04, 9.990e-01], dtype=float16), array([2.217e-04, 2.484e-02, 9.751e-01], dtype=float16), array([1.143e-04, 9.409e-01, 5.893e-02], dtype=float16), array([1.439e-04, 5.364e-07, 1.000e+00], dtype=float16), array([7.153e-06, 3.393e-04, 9.995e-01], dtype=float16), array([6.258e-06, 1.172e-03, 9.990e-01], dtype=float16), array([8.70e-06, 9.24e-06, 1.00e+00], dtype=float16), array([6.551e-05, 6.886e-04, 9.990e-01], dtype=float16), array([1.525e-04, 1.531e-03, 9.985e-01], dtype=float16), array([1.751e-03, 8.875e-05, 9.980e-01], dtype=float16), array([1.487e-04, 9.262e-03, 9.907e-01], dtype=float16), array([0.002386, 0.03116 , 0.9663  ], dtype=float16), array([9.644e-05, 2.562e-03, 9.976e-01], dtype=float16), array([2.17e-05, 8.14e-05, 1.00e+00], dtype=float16), array([8.e-07, 6.e-07, 1.e+00], dtype=float16), array([0.7407 , 0.00286, 0.2566 ], dtype=float16), array([5.507e-05, 1.853e-03, 9.980e-01], dtype=float16), array([1.26e-05, 1.54e-05, 1.00e+00], dtype=float16), array([0.1444 , 0.01012, 0.8457 ], dtype=float16), array([0.01013, 0.06042, 0.9297 ], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([4.65e-06, 1.30e-05, 1.00e+00], dtype=float16), array([9.167e-05, 2.186e-04, 9.995e-01], dtype=float16), array([5.345e-04, 7.868e-05, 9.995e-01], dtype=float16), array([0.00821, 0.1431 , 0.8486 ], dtype=float16), array([1.215e-04, 2.438e-05, 1.000e+00], dtype=float16), array([0.00523 , 0.001127, 0.9937  ], dtype=float16), array([0.0291  , 0.004726, 0.9663  ], dtype=float16), array([1.615e-05, 5.989e-03, 9.941e-01], dtype=float16), array([1.318e-04, 1.804e-01, 8.193e-01], dtype=float16), array([7.286e-04, 4.947e-06, 9.990e-01], dtype=float16), array([8.574e-04, 7.507e-02, 9.243e-01], dtype=float16), array([3.934e-06, 1.261e-04, 1.000e+00], dtype=float16), array([1.4210e-04, 1.1334e-01, 8.8672e-01], dtype=float16), array([2.4e-07, 6.0e-08, 1.0e+00], dtype=float16), array([1.854e-05, 9.298e-06, 1.000e+00], dtype=float16), array([2.450e-05, 2.876e-03, 9.971e-01], dtype=float16), array([2.464e-03, 8.373e-04, 9.966e-01], dtype=float16), array([0.04053, 0.388  , 0.5713 ], dtype=float16), array([1.997e-05, 2.124e-02, 9.785e-01], dtype=float16), array([7.124e-04, 1.379e-02, 9.854e-01], dtype=float16), array([1.4186e-05, 1.0345e-02, 9.8975e-01], dtype=float16), array([2.353e-04, 4.692e-01, 5.308e-01], dtype=float16), array([0.469  , 0.02698, 0.504  ], dtype=float16), array([5.126e-06, 4.473e-04, 9.995e-01], dtype=float16), array([4.666e-04, 1.920e-02, 9.805e-01], dtype=float16), array([1.1742e-05, 1.0556e-04, 1.0000e+00], dtype=float16), array([0.03513, 0.05957, 0.9053 ], dtype=float16), array([3.989e-04, 1.248e-04, 9.995e-01], dtype=float16), array([3.439e-05, 3.374e-04, 9.995e-01], dtype=float16), array([5.941e-04, 8.279e-05, 9.995e-01], dtype=float16), array([0.001834, 0.00829 , 0.9897  ], dtype=float16), array([1.392e-04, 1.283e-04, 9.995e-01], dtype=float16), array([1.647e-04, 3.047e-02, 9.692e-01], dtype=float16), array([0.1714  , 0.002909, 0.8257  ], dtype=float16), array([0.3276, 0.2207, 0.4517], dtype=float16), array([9.656e-06, 1.086e-03, 9.990e-01], dtype=float16), array([1.180e-05, 5.245e-03, 9.946e-01], dtype=float16), array([4.292e-05, 1.285e-04, 1.000e+00], dtype=float16), array([4.643e-05, 4.532e-02, 9.546e-01], dtype=float16), array([5.72e-06, 6.14e-06, 1.00e+00], dtype=float16), array([3.50e-05, 3.78e-05, 1.00e+00], dtype=float16), array([6.437e-06, 2.388e-04, 9.995e-01], dtype=float16), array([2.325e-05, 1.388e-02, 9.863e-01], dtype=float16), array([6.735e-05, 3.843e-04, 9.995e-01], dtype=float16), array([2.496e-04, 1.451e-01, 8.545e-01], dtype=float16), array([2.265e-05, 1.317e-05, 1.000e+00], dtype=float16), array([1.570e-03, 2.421e-04, 9.980e-01], dtype=float16), array([7.749e-06, 9.878e-01, 1.201e-02], dtype=float16), array([4.714e-04, 1.153e-03, 9.985e-01], dtype=float16), array([2.384e-05, 6.920e-05, 1.000e+00], dtype=float16), array([2.999e-04, 1.888e-04, 9.995e-01], dtype=float16), array([0.001369, 0.02336 , 0.975   ], dtype=float16), array([1.186e-05, 2.682e-06, 1.000e+00], dtype=float16), array([0.02054, 0.00918, 0.97   ], dtype=float16), array([0.1395, 0.1927, 0.6675], dtype=float16), array([1.25e-05, 3.89e-05, 1.00e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.2307 , 0.00846, 0.7607 ], dtype=float16), array([0.003029, 0.8955  , 0.1015  ], dtype=float16), array([1.097e-05, 2.987e-04, 9.995e-01], dtype=float16), array([4.71e-06, 3.61e-05, 1.00e+00], dtype=float16), array([9.799e-05, 1.958e-02, 9.805e-01], dtype=float16), array([7.296e-05, 2.789e-05, 1.000e+00], dtype=float16), array([9.590e-05, 1.360e-02, 9.863e-01], dtype=float16), array([8.464e-06, 9.475e-04, 9.990e-01], dtype=float16), array([2.939e-05, 2.357e-03, 9.976e-01], dtype=float16), array([1.323e-05, 1.021e-03, 9.990e-01], dtype=float16), array([1.907e-05, 5.131e-04, 9.995e-01], dtype=float16), array([2.24e-05, 2.59e-05, 1.00e+00], dtype=float16), array([1.305e-05, 1.073e-06, 1.000e+00], dtype=float16), array([1.118e-04, 4.993e-03, 9.951e-01], dtype=float16), array([1.895e-05, 1.326e-02, 9.868e-01], dtype=float16), array([3.099e-06, 5.597e-05, 1.000e+00], dtype=float16), array([2.14e-05, 9.68e-03, 9.90e-01], dtype=float16), array([1.615e-05, 1.945e-03, 9.980e-01], dtype=float16), array([4.940e-04, 1.081e-03, 9.985e-01], dtype=float16), array([1.5e-06, 1.2e-07, 1.0e+00], dtype=float16), array([4.625e-05, 2.937e-04, 9.995e-01], dtype=float16), array([0.001079, 0.05038 , 0.9487  ], dtype=float16), array([4.318e-04, 1.276e-02, 9.868e-01], dtype=float16), array([7.451e-06, 2.927e-05, 1.000e+00], dtype=float16), array([8.631e-05, 2.062e-03, 9.980e-01], dtype=float16), array([4.17e-05, 4.47e-06, 1.00e+00], dtype=float16), array([1.907e-05, 2.375e-03, 9.976e-01], dtype=float16), array([0.001106, 0.478   , 0.521   ], dtype=float16), array([0.0227 , 0.00784, 0.969  ], dtype=float16), array([4.063e-04, 1.502e-03, 9.980e-01], dtype=float16), array([7.313e-05, 8.442e-01, 1.558e-01], dtype=float16), array([2.518e-04, 2.861e-06, 9.995e-01], dtype=float16), array([4.964e-04, 5.918e-04, 9.990e-01], dtype=float16), array([1.8847e-04, 1.1414e-01, 8.8574e-01], dtype=float16), array([2.393e-04, 1.965e-03, 9.976e-01], dtype=float16), array([1.520e-05, 1.926e-04, 1.000e+00], dtype=float16), array([7.14e-04, 2.39e-04, 9.99e-01], dtype=float16), array([8.166e-06, 1.350e-03, 9.985e-01], dtype=float16), array([2.000e-04, 2.072e-02, 9.790e-01], dtype=float16), array([1.897e-04, 3.244e-02, 9.673e-01], dtype=float16), array([3.380e-05, 5.573e-03, 9.946e-01], dtype=float16), array([0.011185, 0.07556 , 0.913   ], dtype=float16), array([0.0366  , 0.003778, 0.9595  ], dtype=float16), array([4.189e-04, 1.541e-04, 9.995e-01], dtype=float16), array([5.960e-05, 9.076e-02, 9.092e-01], dtype=float16), array([7.215e-04, 5.453e-02, 9.448e-01], dtype=float16), array([1.470e-03, 6.342e-05, 9.985e-01], dtype=float16), array([4.227e-03, 4.320e-04, 9.951e-01], dtype=float16), array([4.e-07, 2.e-07, 1.e+00], dtype=float16), array([1.948e-04, 4.687e-04, 9.995e-01], dtype=float16), array([1.373e-04, 1.431e-06, 1.000e+00], dtype=float16), array([4.4e-05, 3.8e-06, 1.0e+00], dtype=float16), array([0.004025, 0.002216, 0.9937  ], dtype=float16), array([0.1353  , 0.001313, 0.8633  ], dtype=float16), array([1.1396e-04, 6.0059e-02, 9.3994e-01], dtype=float16), array([2.998e-05, 1.777e-03, 9.980e-01], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  1
---------------------------------------------------
Extracting Features for Prompt  2  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([8.936e-04, 9.644e-01, 3.455e-02], dtype=float16), array([1.7e-06, 1.3e-06, 1.0e+00], dtype=float16), array([4.95e-06, 1.67e-06, 1.00e+00], dtype=float16), array([1.00e+00, 4.77e-06, 6.66e-05], dtype=float16), array([3.142e-04, 4.718e-04, 9.990e-01], dtype=float16), array([0.55    , 0.4487  , 0.001593], dtype=float16), array([1.000e+00, 1.675e-05, 5.162e-05], dtype=float16), array([1.298e-04, 2.077e-03, 9.976e-01], dtype=float16), array([1.000e+00, 0.000e+00, 1.048e-04], dtype=float16), array([5.78e-06, 3.46e-06, 1.00e+00], dtype=float16), array([1.085e-05, 3.755e-06, 1.000e+00], dtype=float16), array([4.88e-05, 2.09e-06, 1.00e+00], dtype=float16), array([8.17e-05, 9.23e-05, 1.00e+00], dtype=float16), array([4.73e-05, 9.99e-01, 8.54e-04], dtype=float16), array([1.132e-06, 5.144e-05, 1.000e+00], dtype=float16), array([0.9805  , 0.004135, 0.01533 ], dtype=float16), array([9.976e-01, 2.425e-04, 1.982e-03], dtype=float16), array([4.8e-07, 1.4e-06, 1.0e+00], dtype=float16), array([5.e-07, 6.e-08, 1.e+00], dtype=float16), array([9.995e-01, 6.390e-05, 2.909e-04], dtype=float16), array([2.56e-06, 3.77e-05, 1.00e+00], dtype=float16), array([9.990e-01, 4.652e-04, 5.579e-04], dtype=float16), array([2.867e-05, 2.801e-06, 1.000e+00], dtype=float16), array([0.00591, 0.00435, 0.9897 ], dtype=float16), array([8.37e-05, 3.78e-05, 1.00e+00], dtype=float16), array([1.817e-04, 8.154e-05, 9.995e-01], dtype=float16), array([0.006203, 0.04117 , 0.9526  ], dtype=float16), array([5.66e-06, 0.00e+00, 1.00e+00], dtype=float16), array([1.125e-04, 8.535e-04, 9.990e-01], dtype=float16), array([1.955e-04, 8.330e-01, 1.667e-01], dtype=float16), array([1.000e+00, 7.153e-06, 1.599e-04], dtype=float16), array([3.05e-05, 1.59e-05, 1.00e+00], dtype=float16), array([2.1e-06, 3.0e-07, 1.0e+00], dtype=float16), array([1.00e+00, 5.96e-08, 2.56e-05], dtype=float16), array([0.04428, 0.00357, 0.952  ], dtype=float16), array([9.912e-01, 8.553e-03, 2.599e-04], dtype=float16), array([1.2274e-01, 8.7646e-01, 8.3256e-04], dtype=float16), array([0.003786, 0.994   , 0.002161], dtype=float16), array([0.02467, 0.04404, 0.931  ], dtype=float16), array([1.245e-04, 3.593e-03, 9.961e-01], dtype=float16), array([7.808e-06, 2.131e-04, 1.000e+00], dtype=float16), array([9.995e-01, 3.409e-04, 5.734e-05], dtype=float16), array([1.000e+00, 4.888e-05, 1.047e-04], dtype=float16), array([5.537e-05, 1.619e-04, 1.000e+00], dtype=float16), array([1.000e+00, 3.099e-06, 1.885e-04], dtype=float16), array([9.995e-01, 2.170e-04, 1.084e-04], dtype=float16), array([1.835e-04, 4.590e-05, 1.000e+00], dtype=float16), array([0.004223, 0.4463  , 0.55    ], dtype=float16), array([1.000e+00, 3.099e-06, 1.454e-04], dtype=float16), array([0.1967, 0.2517, 0.552 ], dtype=float16), array([3.6e-07, 3.6e-07, 1.0e+00], dtype=float16), array([0.00206, 0.964  , 0.0339 ], dtype=float16), array([3.535e-05, 1.000e+00, 7.153e-07], dtype=float16), array([0.006516, 0.891   , 0.1025  ], dtype=float16), array([1.427e-03, 9.084e-05, 9.985e-01], dtype=float16), array([1.7369e-04, 9.9805e-01, 1.6775e-03], dtype=float16), array([9.844e-01, 1.477e-02, 7.086e-04], dtype=float16), array([0.00706, 0.6396 , 0.3535 ], dtype=float16), array([7.549e-01, 5.484e-06, 2.451e-01], dtype=float16), array([1.0616e-04, 5.7518e-05, 1.0000e+00], dtype=float16), array([4.95e-06, 9.54e-07, 1.00e+00], dtype=float16), array([6.e-07, 6.e-08, 1.e+00], dtype=float16), array([9.924e-05, 1.039e-03, 9.990e-01], dtype=float16), array([0.0015745, 0.06128  , 0.937    ], dtype=float16), array([1.000e+00, 4.828e-06, 3.153e-05], dtype=float16), array([1.31e-06, 4.41e-05, 1.00e+00], dtype=float16), array([7.927e-06, 1.259e-04, 1.000e+00], dtype=float16), array([7.024e-04, 9.951e-01, 4.303e-03], dtype=float16), array([0.9893  , 0.001936, 0.008644], dtype=float16), array([0.03223, 0.2004 , 0.7676 ], dtype=float16), array([3.952e-05, 1.642e-03, 9.985e-01], dtype=float16), array([9.423e-05, 7.933e-05, 1.000e+00], dtype=float16), array([1.085e-05, 5.364e-07, 1.000e+00], dtype=float16), array([1.937e-04, 4.951e-01, 5.049e-01], dtype=float16), array([1.088e-04, 9.912e-01, 8.888e-03], dtype=float16), array([1.341e-04, 2.409e-03, 9.976e-01], dtype=float16), array([9.9951e-01, 3.0184e-04, 1.0836e-04], dtype=float16), array([1.11e-05, 3.40e-06, 1.00e+00], dtype=float16), array([2.64e-05, 4.77e-06, 1.00e+00], dtype=float16), array([9.990e-01, 5.770e-05, 7.663e-04], dtype=float16), array([3.304e-04, 9.893e-01, 1.047e-02], dtype=float16), array([1.425e-05, 2.384e-07, 1.000e+00], dtype=float16), array([1.585e-05, 1.000e+00, 6.217e-05], dtype=float16), array([1.85e-06, 1.19e-07, 1.00e+00], dtype=float16), array([1.e-06, 3.e-07, 1.e+00], dtype=float16), array([0.9673  , 0.03183 , 0.001017], dtype=float16), array([1.234e-05, 3.219e-06, 1.000e+00], dtype=float16), array([3.576e-07, 6.914e-05, 1.000e+00], dtype=float16), array([3.780e-03, 1.234e-05, 9.961e-01], dtype=float16), array([1.695e-02, 5.007e-06, 9.829e-01], dtype=float16), array([0.04532, 0.8965 , 0.05832], dtype=float16), array([2.44e-06, 1.55e-06, 1.00e+00], dtype=float16), array([1.9e-06, 6.6e-07, 1.0e+00], dtype=float16), array([4.64e-05, 1.93e-04, 1.00e+00], dtype=float16), array([9.799e-05, 2.056e-05, 1.000e+00], dtype=float16), array([2.165e-04, 1.937e-04, 9.995e-01], dtype=float16), array([1.e-07, 6.e-08, 1.e+00], dtype=float16), array([1.550e-06, 7.358e-04, 9.990e-01], dtype=float16), array([1.371e-06, 5.454e-05, 1.000e+00], dtype=float16), array([4.192e-03, 1.854e-05, 9.956e-01], dtype=float16), array([1.431e-05, 1.425e-05, 1.000e+00], dtype=float16), array([2.43e-05, 1.31e-06, 1.00e+00], dtype=float16), array([1.377e-04, 2.402e-05, 1.000e+00], dtype=float16), array([0.00991, 0.07513, 0.915  ], dtype=float16), array([1.980e-04, 3.714e-03, 9.961e-01], dtype=float16), array([0.3887 , 0.00199, 0.6094 ], dtype=float16), array([0.006668, 0.5513  , 0.4421  ], dtype=float16), array([0.00746, 0.962  , 0.03065], dtype=float16), array([0.002184, 0.8506  , 0.1473  ], dtype=float16), array([1.371e-06, 1.985e-05, 1.000e+00], dtype=float16), array([1.918e-04, 3.415e-05, 1.000e+00], dtype=float16), array([2.98e-05, 1.79e-07, 1.00e+00], dtype=float16), array([0.3271  , 0.6714  , 0.001648], dtype=float16), array([0.3762 , 0.515  , 0.10876], dtype=float16), array([6.671e-04, 1.326e-02, 9.858e-01], dtype=float16), array([4.613e-05, 8.959e-05, 1.000e+00], dtype=float16), array([3.16e-06, 1.31e-06, 1.00e+00], dtype=float16), array([2.4e-07, 1.2e-07, 1.0e+00], dtype=float16), array([1.000e+00, 2.682e-06, 5.656e-05], dtype=float16), array([0.7314, 0.2632, 0.0054], dtype=float16), array([6.e-08, 2.e-07, 1.e+00], dtype=float16), array([0.002947, 0.02473 , 0.972   ], dtype=float16), array([0.000996, 0.01259 , 0.9863  ], dtype=float16), array([0.0767  , 0.004513, 0.919   ], dtype=float16), array([0.002398, 0.001919, 0.9956  ], dtype=float16), array([1.000e+00, 5.841e-06, 8.315e-05], dtype=float16), array([7.977e-04, 3.424e-04, 9.990e-01], dtype=float16), array([9.475e-04, 6.394e-04, 9.985e-01], dtype=float16), array([3.624e-05, 1.264e-04, 1.000e+00], dtype=float16), array([1.00e-05, 2.01e-03, 9.98e-01], dtype=float16), array([2.337e-04, 1.329e-05, 9.995e-01], dtype=float16), array([0.1178 , 0.02347, 0.859  ], dtype=float16), array([9.868e-01, 6.258e-06, 1.308e-02], dtype=float16), array([3.37e-05, 9.30e-06, 1.00e+00], dtype=float16), array([3.95e-05, 8.58e-06, 1.00e+00], dtype=float16), array([1.000e+00, 8.106e-06, 3.535e-05], dtype=float16), array([0.01811, 0.3843 , 0.5977 ], dtype=float16), array([0.1768, 0.4756, 0.3477], dtype=float16), array([4.022e-04, 1.492e-03, 9.980e-01], dtype=float16), array([0.002766, 0.9946  , 0.002691], dtype=float16), array([3.555e-04, 4.119e-05, 9.995e-01], dtype=float16), array([1.844e-03, 1.332e-04, 9.980e-01], dtype=float16), array([1.931e-05, 2.378e-04, 9.995e-01], dtype=float16), array([0.004753, 0.008446, 0.987   ], dtype=float16), array([5.989e-04, 4.756e-05, 9.995e-01], dtype=float16), array([4.935e-05, 8.398e-01, 1.600e-01], dtype=float16), array([7.119e-04, 6.638e-04, 9.985e-01], dtype=float16), array([5.96e-06, 9.42e-06, 1.00e+00], dtype=float16), array([9.9854e-01, 3.9995e-05, 1.3895e-03], dtype=float16), array([3.211e-04, 9.990e-01, 6.423e-04], dtype=float16), array([1.3794e-02, 3.4928e-04, 9.8584e-01], dtype=float16), array([9.858e-01, 2.801e-06, 1.402e-02], dtype=float16), array([9.985e-01, 1.013e-06, 1.507e-03], dtype=float16), array([0.01097, 0.8994 , 0.0898 ], dtype=float16), array([9.995e-01, 8.661e-05, 2.627e-04], dtype=float16), array([0.8945  , 0.1031  , 0.002195], dtype=float16), array([6.952e-04, 3.204e-03, 9.961e-01], dtype=float16), array([9.780e-01, 3.433e-04, 2.141e-02], dtype=float16), array([1.000e+00, 1.192e-07, 2.295e-05], dtype=float16), array([0.04175, 0.02322, 0.935  ], dtype=float16), array([0.03235, 0.0303 , 0.9375 ], dtype=float16), array([1.000e+00, 2.307e-05, 1.806e-05], dtype=float16), array([0.1718 , 0.08405, 0.744  ], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.833e-04, 2.153e-03, 9.976e-01], dtype=float16), array([0.006798, 0.001884, 0.991   ], dtype=float16), array([0.01584 , 0.001432, 0.983   ], dtype=float16), array([4.3273e-04, 9.9951e-01, 1.0026e-04], dtype=float16), array([0.2185, 0.037 , 0.7446], dtype=float16), array([1.497e-02, 9.849e-01, 3.271e-04], dtype=float16), array([1.042e-03, 3.061e-04, 9.985e-01], dtype=float16), array([9.185e-05, 2.159e-02, 9.785e-01], dtype=float16), array([0.0958 , 0.11444, 0.7896 ], dtype=float16), array([1.000e+00, 7.677e-05, 1.603e-04], dtype=float16), array([0.3115, 0.2211, 0.4675], dtype=float16), array([0.001627, 0.007347, 0.991   ], dtype=float16), array([2.507e-01, 4.675e-04, 7.490e-01], dtype=float16), array([0.001034, 0.001944, 0.997   ], dtype=float16), array([2.556e-04, 9.995e-01, 3.514e-04], dtype=float16), array([9.985e-01, 8.112e-05, 1.484e-03], dtype=float16), array([1.00e+00, 6.54e-05, 4.39e-05], dtype=float16), array([1.00e+00, 1.06e-05, 7.93e-06], dtype=float16), array([0.0366 , 0.03543, 0.9277 ], dtype=float16), array([3.622e-04, 6.914e-01, 3.081e-01], dtype=float16), array([6.e-08, 1.e-07, 1.e+00], dtype=float16), array([1.272e-04, 9.990e-01, 1.071e-03], dtype=float16), array([1.222e-01, 1.213e-04, 8.774e-01], dtype=float16), array([4.e-07, 3.e-07, 1.e+00], dtype=float16), array([6.e-07, 5.e-07, 1.e+00], dtype=float16), array([8.994e-05, 7.320e-03, 9.927e-01], dtype=float16), array([1.00e+00, 5.72e-06, 4.72e-05], dtype=float16), array([9.712e-01, 2.841e-02, 6.180e-04], dtype=float16), array([0.002987, 0.006474, 0.9907  ], dtype=float16), array([7.21e-06, 1.04e-05, 1.00e+00], dtype=float16), array([9.888e-01, 1.090e-02, 4.137e-04], dtype=float16), array([3.4e-06, 1.2e-07, 1.0e+00], dtype=float16), array([9.946e-01, 6.952e-04, 4.772e-03], dtype=float16), array([1.000e+00, 3.195e-05, 5.114e-05], dtype=float16), array([1.00e+00, 5.36e-07, 3.27e-05], dtype=float16), array([6.85e-06, 3.58e-07, 1.00e+00], dtype=float16), array([1.903e-04, 2.279e-03, 9.976e-01], dtype=float16), array([3.6e-07, 1.2e-07, 1.0e+00], dtype=float16), array([9.149e-05, 2.754e-05, 1.000e+00], dtype=float16), array([1.26e-05, 5.96e-08, 1.00e+00], dtype=float16), array([1.1677e-04, 5.3883e-04, 9.9951e-01], dtype=float16), array([2.608e-04, 2.122e-05, 9.995e-01], dtype=float16), array([0.00205, 0.9727 , 0.02545], dtype=float16), array([4.53e-06, 3.58e-06, 1.00e+00], dtype=float16), array([0.02986 , 0.002295, 0.968   ], dtype=float16), array([8.571e-05, 8.507e-04, 9.990e-01], dtype=float16), array([4.868e-02, 8.435e-04, 9.507e-01], dtype=float16), array([1.9e-06, 2.4e-07, 1.0e+00], dtype=float16), array([0.01945, 0.01929, 0.9614 ], dtype=float16), array([2.666e-04, 3.136e-03, 9.966e-01], dtype=float16), array([0.007214, 0.2374  , 0.7554  ], dtype=float16), array([1.000e+00, 5.960e-08, 2.253e-05], dtype=float16), array([0.012695, 0.02202 , 0.9653  ], dtype=float16), array([3.231e-03, 1.096e-04, 9.966e-01], dtype=float16), array([9.976e-01, 2.052e-03, 2.221e-04], dtype=float16), array([0.003294, 0.955   , 0.04147 ], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.00e+00, 2.49e-05, 3.97e-05], dtype=float16), array([1.00e+00, 2.38e-07, 2.26e-05], dtype=float16), array([8.565e-05, 9.941e-01, 5.550e-03], dtype=float16), array([0.686  , 0.01111, 0.303  ], dtype=float16), array([7.749e-07, 2.724e-05, 1.000e+00], dtype=float16), array([1.000e+00, 1.192e-07, 1.556e-05], dtype=float16), array([4.241e-04, 6.006e-02, 9.395e-01], dtype=float16), array([1.13e-06, 5.96e-07, 1.00e+00], dtype=float16), array([1.209e-03, 2.563e-05, 9.985e-01], dtype=float16), array([1.241e-04, 9.995e-01, 5.083e-04], dtype=float16), array([0.1117, 0.0981, 0.79  ], dtype=float16), array([3.159e-03, 6.366e-04, 9.961e-01], dtype=float16), array([3.910e-05, 1.149e-03, 9.990e-01], dtype=float16), array([0.4167 , 0.01443, 0.569  ], dtype=float16), array([2.56e-06, 6.62e-06, 1.00e+00], dtype=float16), array([0.646 , 0.0666, 0.2876], dtype=float16), array([2.573e-04, 1.701e-02, 9.829e-01], dtype=float16), array([6.08e-06, 9.48e-05, 1.00e+00], dtype=float16), array([7.572e-04, 9.594e-04, 9.980e-01], dtype=float16), array([9.12e-06, 1.97e-06, 1.00e+00], dtype=float16), array([1.191e-03, 9.804e-04, 9.980e-01], dtype=float16), array([3.359e-04, 1.955e-02, 9.800e-01], dtype=float16), array([5.645e-05, 9.990e-01, 7.482e-04], dtype=float16), array([5.293e-05, 9.146e-01, 8.527e-02], dtype=float16), array([1.00e+00, 5.96e-08, 3.91e-05], dtype=float16), array([0.0319  , 0.002392, 0.966   ], dtype=float16), array([0.00828, 0.92   , 0.0717 ], dtype=float16), array([0.1699 , 0.8257 , 0.00445], dtype=float16), array([1.00e+00, 4.77e-06, 5.01e-05], dtype=float16), array([0.00704 , 0.008736, 0.9844  ], dtype=float16), array([0.001453, 0.2203  , 0.7783  ], dtype=float16), array([1.00e+00, 5.96e-07, 2.44e-05], dtype=float16), array([1.000e+00, 1.192e-07, 1.924e-04], dtype=float16), array([1.675e-05, 1.287e-05, 1.000e+00], dtype=float16), array([5.364e-07, 1.943e-05, 1.000e+00], dtype=float16), array([9.316e-05, 1.729e-06, 1.000e+00], dtype=float16), array([0.02066 , 0.004986, 0.974   ], dtype=float16), array([1.9e-06, 5.0e-06, 1.0e+00], dtype=float16), array([5.90e-06, 1.43e-06, 1.00e+00], dtype=float16), array([0.604 , 0.3257, 0.0704], dtype=float16), array([9.537e-03, 9.902e-01, 3.655e-04], dtype=float16), array([1.00e+00, 2.45e-05, 3.57e-05], dtype=float16), array([5.188e-03, 7.153e-07, 9.946e-01], dtype=float16), array([9.995e-01, 3.022e-05, 3.629e-04], dtype=float16), array([6.065e-04, 2.645e-03, 9.966e-01], dtype=float16), array([0.004055, 0.0269  , 0.969   ], dtype=float16), array([6.466e-04, 5.192e-05, 9.995e-01], dtype=float16), array([0.994   , 0.001281, 0.004677], dtype=float16), array([5.755e-04, 3.242e-04, 9.990e-01], dtype=float16), array([1.614e-04, 5.741e-04, 9.990e-01], dtype=float16), array([0.5786 , 0.01092, 0.4104 ], dtype=float16), array([2.220e-04, 2.365e-03, 9.976e-01], dtype=float16), array([9.093e-04, 4.411e-06, 9.990e-01], dtype=float16), array([1.0e+00, 2.4e-07, 6.2e-06], dtype=float16), array([0.01443, 0.812  , 0.1735 ], dtype=float16), array([0.02133, 0.853  , 0.1256 ], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  2
---------------------------------------------------
Extracting Features for Prompt  3  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([8.851e-05, 2.128e-04, 9.995e-01], dtype=float16), array([5.078e-05, 6.485e-04, 9.995e-01], dtype=float16), array([5.960e-07, 1.884e-05, 1.000e+00], dtype=float16), array([2.21e-06, 1.21e-05, 1.00e+00], dtype=float16), array([6.855e-06, 1.131e-03, 9.990e-01], dtype=float16), array([0.01335 , 0.003664, 0.983   ], dtype=float16), array([0.00143, 0.11224, 0.886  ], dtype=float16), array([2.26e-06, 7.15e-07, 1.00e+00], dtype=float16), array([2.1400e-03, 1.1843e-04, 9.9756e-01], dtype=float16), array([4.747e-04, 9.514e-03, 9.902e-01], dtype=float16), array([1.1635e-04, 7.8506e-03, 9.9219e-01], dtype=float16), array([1.955e-05, 2.364e-04, 9.995e-01], dtype=float16), array([5.836e-04, 5.957e-01, 4.038e-01], dtype=float16), array([5.4e-07, 1.5e-06, 1.0e+00], dtype=float16), array([1.46e-04, 7.48e-03, 9.92e-01], dtype=float16), array([0.0637  , 0.004025, 0.932   ], dtype=float16), array([0.003323, 0.0314  , 0.9653  ], dtype=float16), array([1.788e-06, 1.102e-04, 1.000e+00], dtype=float16), array([4.625e-04, 6.035e-01, 3.960e-01], dtype=float16), array([8.118e-05, 4.869e-04, 9.995e-01], dtype=float16), array([7.331e-06, 1.003e-04, 1.000e+00], dtype=float16), array([0.001344, 0.2219  , 0.777   ], dtype=float16), array([6.753e-05, 5.474e-03, 9.946e-01], dtype=float16), array([2.615e-04, 6.466e-03, 9.932e-01], dtype=float16), array([2.209e-04, 6.573e-02, 9.341e-01], dtype=float16), array([8.857e-05, 3.298e-01, 6.699e-01], dtype=float16), array([7.463e-05, 9.814e-01, 1.859e-02], dtype=float16), array([2.768e-04, 2.123e-04, 9.995e-01], dtype=float16), array([0.003592, 0.4763  , 0.52    ], dtype=float16), array([9.865e-05, 1.036e-03, 9.990e-01], dtype=float16), array([0.01521, 0.02673, 0.958  ], dtype=float16), array([1.115e-05, 2.642e-04, 9.995e-01], dtype=float16), array([2.999e-04, 3.040e-04, 9.995e-01], dtype=float16), array([0.08093 , 0.001477, 0.9175  ], dtype=float16), array([0.001202, 0.01    , 0.989   ], dtype=float16), array([0.31  , 0.3606, 0.3293], dtype=float16), array([3.189e-05, 3.254e-05, 1.000e+00], dtype=float16), array([7.415e-04, 2.419e-02, 9.751e-01], dtype=float16), array([2.997e-04, 3.330e-03, 9.966e-01], dtype=float16), array([0.001017, 0.7197  , 0.279   ], dtype=float16), array([0.002 , 0.3262, 0.672 ], dtype=float16), array([0.02058, 0.549  , 0.4304 ], dtype=float16), array([0.01627, 0.1026 , 0.8813 ], dtype=float16), array([3.451e-05, 3.934e-05, 1.000e+00], dtype=float16), array([0.138  , 0.01356, 0.8486 ], dtype=float16), array([1.562e-04, 2.415e-01, 7.583e-01], dtype=float16), array([3.66e-05, 1.93e-05, 1.00e+00], dtype=float16), array([3.331e-04, 3.065e-03, 9.966e-01], dtype=float16), array([3.190e-04, 3.029e-03, 9.966e-01], dtype=float16), array([1.771e-04, 7.272e-06, 1.000e+00], dtype=float16), array([0.00121, 0.1716 , 0.827  ], dtype=float16), array([0.002617, 0.5835  , 0.4138  ], dtype=float16), array([2.730e-04, 5.585e-02, 9.438e-01], dtype=float16), array([7.596e-04, 1.584e-02, 9.834e-01], dtype=float16), array([2.071e-04, 1.923e-02, 9.805e-01], dtype=float16), array([5.56e-05, 1.78e-01, 8.22e-01], dtype=float16), array([7.868e-06, 1.382e-03, 9.985e-01], dtype=float16), array([3.022e-05, 1.126e-04, 1.000e+00], dtype=float16), array([0.001613, 0.00813 , 0.99    ], dtype=float16), array([1.0124e-02, 2.9516e-04, 9.8975e-01], dtype=float16), array([5.44e-05, 3.33e-02, 9.67e-01], dtype=float16), array([1.812e-05, 8.667e-05, 1.000e+00], dtype=float16), array([2.391e-04, 2.441e-03, 9.976e-01], dtype=float16), array([0.001248, 0.04855 , 0.95    ], dtype=float16), array([0.002562, 0.1975  , 0.8     ], dtype=float16), array([2.398e-04, 1.468e-03, 9.985e-01], dtype=float16), array([3.579e-04, 2.621e-02, 9.736e-01], dtype=float16), array([1.454e-05, 3.793e-04, 9.995e-01], dtype=float16), array([7.358e-04, 8.774e-04, 9.985e-01], dtype=float16), array([0.0085, 0.1915, 0.8   ], dtype=float16), array([1.824e-04, 1.700e-03, 9.980e-01], dtype=float16), array([7.457e-05, 1.119e-04, 1.000e+00], dtype=float16), array([1.508e-05, 1.266e-03, 9.985e-01], dtype=float16), array([1.575e-04, 2.319e-01, 7.681e-01], dtype=float16), array([6.658e-05, 7.826e-05, 1.000e+00], dtype=float16), array([2.087e-04, 1.443e-01, 8.555e-01], dtype=float16), array([0.281 , 0.5176, 0.2015], dtype=float16), array([7.849e-04, 2.002e-02, 9.790e-01], dtype=float16), array([1.25e-06, 3.87e-06, 1.00e+00], dtype=float16), array([0.005413, 0.926   , 0.0687  ], dtype=float16), array([8.804e-05, 1.467e-03, 9.985e-01], dtype=float16), array([7.29e-05, 2.96e-01, 7.04e-01], dtype=float16), array([1.577e-04, 5.670e-04, 9.995e-01], dtype=float16), array([1.342e-04, 1.378e-02, 9.863e-01], dtype=float16), array([4.926e-04, 2.341e-02, 9.761e-01], dtype=float16), array([0.0285 , 0.01483, 0.9565 ], dtype=float16), array([0.091 , 0.736 , 0.1731], dtype=float16), array([1.26e-05, 1.85e-05, 1.00e+00], dtype=float16), array([0.1975 , 0.01057, 0.792  ], dtype=float16), array([0.1976 , 0.03827, 0.764  ], dtype=float16), array([1.63e-05, 3.66e-05, 1.00e+00], dtype=float16), array([0.0008535, 0.5107   , 0.4883   ], dtype=float16), array([4.77e-05, 6.20e-06, 1.00e+00], dtype=float16), array([8.34e-07, 5.84e-06, 1.00e+00], dtype=float16), array([0.001465 , 0.0011015, 0.9976   ], dtype=float16), array([1.022e-04, 5.913e-04, 9.995e-01], dtype=float16), array([2.503e-06, 2.925e-04, 9.995e-01], dtype=float16), array([2.135e-04, 2.256e-03, 9.976e-01], dtype=float16), array([8.273e-05, 7.749e-01, 2.251e-01], dtype=float16), array([2.80e-04, 8.07e-04, 9.99e-01], dtype=float16), array([1.848e-04, 7.085e-01, 2.910e-01], dtype=float16), array([1.007e-05, 1.512e-04, 1.000e+00], dtype=float16), array([2.341e-04, 2.025e-04, 9.995e-01], dtype=float16), array([7.361e-05, 1.045e-01, 8.955e-01], dtype=float16), array([1.1706e-04, 1.7319e-03, 9.9805e-01], dtype=float16), array([3.544e-03, 1.720e-04, 9.961e-01], dtype=float16), array([4.053e-05, 2.213e-04, 9.995e-01], dtype=float16), array([1.0043e-04, 2.9697e-03, 9.9707e-01], dtype=float16), array([1.127e-05, 1.324e-03, 9.985e-01], dtype=float16), array([1.69e-05, 6.68e-06, 1.00e+00], dtype=float16), array([5.112e-04, 3.430e-02, 9.653e-01], dtype=float16), array([3.499e-05, 2.621e-03, 9.976e-01], dtype=float16), array([1.797e-02, 7.610e-04, 9.814e-01], dtype=float16), array([4.596e-05, 8.732e-05, 1.000e+00], dtype=float16), array([0.002153, 0.673   , 0.3247  ], dtype=float16), array([3.368e-05, 1.658e-03, 9.985e-01], dtype=float16), array([4.691e-05, 2.163e-03, 9.976e-01], dtype=float16), array([2.718e-05, 7.825e-04, 9.990e-01], dtype=float16), array([0.4915 , 0.09906, 0.4094 ], dtype=float16), array([2.785e-04, 1.899e-04, 9.995e-01], dtype=float16), array([1.991e-05, 1.461e-03, 9.985e-01], dtype=float16), array([7.284e-05, 3.707e-05, 1.000e+00], dtype=float16), array([8.029e-05, 1.465e-04, 1.000e+00], dtype=float16), array([5.474e-04, 1.097e-05, 9.995e-01], dtype=float16), array([0.0045 , 0.02144, 0.974  ], dtype=float16), array([5.396e-02, 6.890e-05, 9.458e-01], dtype=float16), array([4.65e-06, 7.99e-04, 9.99e-01], dtype=float16), array([4.679e-05, 1.547e-03, 9.985e-01], dtype=float16), array([1.296e-04, 2.930e-04, 9.995e-01], dtype=float16), array([1.25e-06, 1.39e-05, 1.00e+00], dtype=float16), array([2.025e-04, 8.179e-03, 9.917e-01], dtype=float16), array([5.09e-05, 1.18e-01, 8.82e-01], dtype=float16), array([0.001253, 0.02391 , 0.9746  ], dtype=float16), array([1.550e-04, 9.985e-04, 9.990e-01], dtype=float16), array([1.807e-04, 9.125e-03, 9.907e-01], dtype=float16), array([0.001713, 0.5874  , 0.4106  ], dtype=float16), array([0.01706, 0.2944 , 0.6885 ], dtype=float16), array([3.30e-05, 6.67e-03, 9.93e-01], dtype=float16), array([8.40e-06, 2.86e-05, 1.00e+00], dtype=float16), array([3.397e-06, 5.007e-05, 1.000e+00], dtype=float16), array([4.816e-05, 7.061e-03, 9.927e-01], dtype=float16), array([0.001374, 0.00238 , 0.996   ], dtype=float16), array([1.547e-04, 4.051e-03, 9.956e-01], dtype=float16), array([5.016e-04, 2.576e-01, 7.417e-01], dtype=float16), array([8.020e-04, 5.879e-04, 9.985e-01], dtype=float16), array([0.001827, 0.479   , 0.519   ], dtype=float16), array([3.548e-04, 1.670e-04, 9.995e-01], dtype=float16), array([1.609e-04, 2.285e-03, 9.976e-01], dtype=float16), array([0.00212 , 0.005955, 0.9917  ], dtype=float16), array([7.114e-04, 3.955e-02, 9.600e-01], dtype=float16), array([7.111e-05, 5.021e-04, 9.995e-01], dtype=float16), array([4.54e-01, 3.12e-05, 5.46e-01], dtype=float16), array([0.07355, 0.5405 , 0.386  ], dtype=float16), array([8.863e-05, 3.073e-03, 9.971e-01], dtype=float16), array([0.003605, 0.832   , 0.1643  ], dtype=float16), array([0.003899, 0.1265  , 0.8696  ], dtype=float16), array([4.768e-06, 3.030e-04, 9.995e-01], dtype=float16), array([1.017e-04, 4.452e-05, 1.000e+00], dtype=float16), array([0.963   , 0.001725, 0.0355  ], dtype=float16), array([2.60e-05, 6.16e-05, 1.00e+00], dtype=float16), array([1.150e-05, 3.591e-04, 9.995e-01], dtype=float16), array([0.2474, 0.2769, 0.4758], dtype=float16), array([0.001329, 0.1763  , 0.8223  ], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([9.209e-05, 5.872e-02, 9.414e-01], dtype=float16), array([7.188e-05, 1.143e-03, 9.990e-01], dtype=float16), array([0.004494, 0.001494, 0.994   ], dtype=float16), array([3.705e-04, 2.325e-02, 9.766e-01], dtype=float16), array([0.004807, 0.002949, 0.992   ], dtype=float16), array([4.6e-06, 2.5e-06, 1.0e+00], dtype=float16), array([0.0364  , 0.002304, 0.9614  ], dtype=float16), array([1.1635e-04, 1.5332e-01, 8.4668e-01], dtype=float16), array([0.002748, 0.05832 , 0.939   ], dtype=float16), array([0.00798, 0.1439 , 0.848  ], dtype=float16), array([2.16e-05, 3.22e-06, 1.00e+00], dtype=float16), array([5.87e-04, 7.37e-02, 9.26e-01], dtype=float16), array([0.01011 , 0.010475, 0.9795  ], dtype=float16), array([4.590e-06, 8.947e-05, 1.000e+00], dtype=float16), array([8.470e-05, 9.784e-02, 9.023e-01], dtype=float16), array([3.68e-05, 3.64e-06, 1.00e+00], dtype=float16), array([9.477e-05, 4.272e-04, 9.995e-01], dtype=float16), array([0.1311 , 0.00344, 0.865  ], dtype=float16), array([3.633e-04, 7.366e-03, 9.922e-01], dtype=float16), array([0.001456, 0.02228 , 0.976   ], dtype=float16), array([3.452e-04, 3.464e-02, 9.648e-01], dtype=float16), array([7.546e-05, 7.431e-03, 9.927e-01], dtype=float16), array([0.947   , 0.002674, 0.0507  ], dtype=float16), array([3.165e-05, 6.475e-04, 9.995e-01], dtype=float16), array([2.097e-04, 2.172e-01, 7.827e-01], dtype=float16), array([0.0006986, 0.656    , 0.3435   ], dtype=float16), array([5.999e-04, 9.800e-01, 1.952e-02], dtype=float16), array([4.458e-01, 3.955e-04, 5.537e-01], dtype=float16), array([5.895e-05, 1.674e-04, 1.000e+00], dtype=float16), array([5.6863e-05, 1.3405e-02, 9.8633e-01], dtype=float16), array([1.200e-04, 5.029e-02, 9.497e-01], dtype=float16), array([4.668e-04, 8.896e-03, 9.907e-01], dtype=float16), array([0.01563 , 0.002663, 0.982   ], dtype=float16), array([0.002144, 0.01958 , 0.9785  ], dtype=float16), array([0.07477 , 0.001662, 0.9233  ], dtype=float16), array([5.131e-04, 1.193e-02, 9.878e-01], dtype=float16), array([7.56e-05, 9.59e-05, 1.00e+00], dtype=float16), array([6.361e-04, 9.575e-03, 9.897e-01], dtype=float16), array([5.522e-04, 1.087e-02, 9.888e-01], dtype=float16), array([2.92e-04, 1.68e-02, 9.83e-01], dtype=float16), array([4.592e-04, 2.479e-01, 7.515e-01], dtype=float16), array([5.345e-04, 8.499e-03, 9.912e-01], dtype=float16), array([3.541e-05, 6.474e-03, 9.937e-01], dtype=float16), array([1.299e-05, 1.891e-03, 9.980e-01], dtype=float16), array([4.792e-05, 1.164e-03, 9.990e-01], dtype=float16), array([1.479e-04, 6.244e-02, 9.375e-01], dtype=float16), array([0.001381, 0.01307 , 0.9854  ], dtype=float16), array([4.878e-04, 1.929e-02, 9.805e-01], dtype=float16), array([3.850e-05, 1.034e-03, 9.990e-01], dtype=float16), array([6.235e-05, 1.068e-02, 9.893e-01], dtype=float16), array([3.844e-05, 2.081e-03, 9.980e-01], dtype=float16), array([0.01633, 0.10205, 0.882  ], dtype=float16), array([1.237e-04, 2.834e-02, 9.717e-01], dtype=float16), array([0.0007744, 0.6055   , 0.3936   ], dtype=float16), array([1.996e-02, 6.447e-04, 9.795e-01], dtype=float16), array([8.595e-05, 6.439e-02, 9.355e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.577  , 0.01578, 0.4072 ], dtype=float16), array([0.005253, 0.4946  , 0.5     ], dtype=float16), array([4.330e-04, 9.842e-03, 9.897e-01], dtype=float16), array([1.843e-04, 1.309e-03, 9.985e-01], dtype=float16), array([9.620e-05, 6.665e-01, 3.333e-01], dtype=float16), array([0.00198 , 0.002598, 0.9956  ], dtype=float16), array([3.576e-04, 6.050e-03, 9.937e-01], dtype=float16), array([3.380e-05, 1.748e-03, 9.980e-01], dtype=float16), array([2.654e-04, 5.263e-05, 9.995e-01], dtype=float16), array([1.237e-04, 1.106e-03, 9.985e-01], dtype=float16), array([5.341e-05, 5.438e-02, 9.453e-01], dtype=float16), array([4.268e-05, 1.828e-02, 9.814e-01], dtype=float16), array([7.224e-05, 2.378e-04, 9.995e-01], dtype=float16), array([8.798e-04, 1.337e-02, 9.858e-01], dtype=float16), array([3.002e-04, 2.080e-02, 9.790e-01], dtype=float16), array([1.496e-05, 2.581e-05, 1.000e+00], dtype=float16), array([5.584e-04, 1.308e-03, 9.980e-01], dtype=float16), array([1.1414e-04, 6.0303e-01, 3.9697e-01], dtype=float16), array([3.749e-05, 3.424e-02, 9.658e-01], dtype=float16), array([1.317e-05, 1.459e-04, 1.000e+00], dtype=float16), array([1.283e-03, 4.978e-04, 9.980e-01], dtype=float16), array([4.039e-04, 3.386e-03, 9.961e-01], dtype=float16), array([3.433e-05, 2.850e-03, 9.971e-01], dtype=float16), array([3.111e-04, 1.890e-01, 8.105e-01], dtype=float16), array([7.453e-04, 1.349e-02, 9.858e-01], dtype=float16), array([0.01718, 0.03177, 0.951  ], dtype=float16), array([2.034e-04, 7.397e-05, 9.995e-01], dtype=float16), array([2.782e-04, 7.446e-02, 9.253e-01], dtype=float16), array([0.005493, 0.1534  , 0.841   ], dtype=float16), array([2.236e-04, 2.884e-03, 9.971e-01], dtype=float16), array([2.859e-04, 5.429e-02, 9.453e-01], dtype=float16), array([0.47   , 0.02017, 0.51   ], dtype=float16), array([6.708e-02, 1.780e-04, 9.326e-01], dtype=float16), array([2.589e-04, 9.890e-04, 9.985e-01], dtype=float16), array([8.410e-05, 6.289e-04, 9.995e-01], dtype=float16), array([0.02417, 0.00113, 0.9746 ], dtype=float16), array([0.0161, 0.2834, 0.7   ], dtype=float16), array([3.582e-05, 3.231e-03, 9.966e-01], dtype=float16), array([8.166e-05, 6.137e-04, 9.995e-01], dtype=float16), array([8.714e-05, 1.713e-04, 9.995e-01], dtype=float16), array([2.009e-04, 4.282e-02, 9.570e-01], dtype=float16), array([0.001377, 0.05365 , 0.945   ], dtype=float16), array([4.539e-04, 1.553e-03, 9.980e-01], dtype=float16), array([0.0347  , 0.004253, 0.961   ], dtype=float16), array([2.961e-04, 5.122e-01, 4.875e-01], dtype=float16), array([0.002129, 0.834   , 0.1641  ], dtype=float16), array([0.004604, 0.1125  , 0.883   ], dtype=float16), array([0.2051  , 0.001125, 0.794   ], dtype=float16), array([4.339e-05, 2.359e-04, 9.995e-01], dtype=float16), array([2.136e-04, 1.909e-03, 9.980e-01], dtype=float16), array([1.075e-04, 9.379e-04, 9.990e-01], dtype=float16), array([2.909e-05, 1.619e-04, 1.000e+00], dtype=float16), array([0.000985, 0.01382 , 0.9854  ], dtype=float16), array([0.4001, 0.0653, 0.5347], dtype=float16), array([4.297e-05, 1.235e-02, 9.878e-01], dtype=float16), array([3.052e-04, 1.436e-01, 8.560e-01], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  3
---------------------------------------------------
Extracting Features for Prompt  4  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([3.4e-06, 3.8e-06, 1.0e+00], dtype=float16), array([3.8981e-05, 1.1504e-04, 1.0000e+00], dtype=float16), array([1.258e-05, 2.942e-04, 9.995e-01], dtype=float16), array([3.319e-04, 6.022e-04, 9.990e-01], dtype=float16), array([3.934e-05, 1.655e-01, 8.345e-01], dtype=float16), array([3.276e-04, 1.785e-03, 9.980e-01], dtype=float16), array([3.737e-05, 1.186e-05, 1.000e+00], dtype=float16), array([3.9e-06, 5.4e-07, 1.0e+00], dtype=float16), array([1.047e-04, 5.329e-05, 1.000e+00], dtype=float16), array([7.468e-05, 1.058e-01, 8.940e-01], dtype=float16), array([7.93e-06, 1.58e-05, 1.00e+00], dtype=float16), array([0.00794, 0.2854 , 0.7065 ], dtype=float16), array([1.019e-05, 1.985e-05, 1.000e+00], dtype=float16), array([6.318e-06, 2.265e-05, 1.000e+00], dtype=float16), array([1.425e-04, 4.959e-03, 9.951e-01], dtype=float16), array([2.086e-05, 1.309e-04, 1.000e+00], dtype=float16), array([1.553e-04, 3.220e-01, 6.777e-01], dtype=float16), array([9.060e-06, 5.817e-04, 9.995e-01], dtype=float16), array([6.288e-05, 2.243e-03, 9.976e-01], dtype=float16), array([0.00309, 0.04868, 0.948  ], dtype=float16), array([1.409e-04, 3.784e-04, 9.995e-01], dtype=float16), array([2.8872e-04, 1.4534e-02, 9.8535e-01], dtype=float16), array([7.570e-06, 4.504e-04, 9.995e-01], dtype=float16), array([6.676e-06, 4.606e-04, 9.995e-01], dtype=float16), array([4.834e-05, 3.189e-02, 9.683e-01], dtype=float16), array([1.338e-04, 1.768e-03, 9.980e-01], dtype=float16), array([2.438e-04, 7.920e-01, 2.080e-01], dtype=float16), array([8.100e-05, 2.726e-03, 9.971e-01], dtype=float16), array([3.097e-04, 1.160e-03, 9.985e-01], dtype=float16), array([2.50e-06, 5.25e-06, 1.00e+00], dtype=float16), array([2.438e-05, 5.560e-04, 9.995e-01], dtype=float16), array([2.700e-05, 3.338e-03, 9.966e-01], dtype=float16), array([1.961e-05, 2.835e-04, 9.995e-01], dtype=float16), array([0.001946, 0.487   , 0.5107  ], dtype=float16), array([4.995e-05, 1.549e-03, 9.985e-01], dtype=float16), array([1.775e-04, 1.407e-01, 8.589e-01], dtype=float16), array([1.129e-04, 2.621e-02, 9.736e-01], dtype=float16), array([1.636e-04, 3.618e-03, 9.961e-01], dtype=float16), array([1.734e-05, 3.828e-03, 9.961e-01], dtype=float16), array([1.0556e-04, 3.5181e-01, 6.4795e-01], dtype=float16), array([9.835e-06, 1.293e-04, 1.000e+00], dtype=float16), array([2.303e-04, 6.423e-04, 9.990e-01], dtype=float16), array([8.25e-05, 6.30e-05, 1.00e+00], dtype=float16), array([2.366e-05, 3.239e-03, 9.966e-01], dtype=float16), array([3.242e-04, 4.709e-04, 9.990e-01], dtype=float16), array([5.907e-05, 2.174e-02, 9.780e-01], dtype=float16), array([3.165e-05, 1.633e-03, 9.985e-01], dtype=float16), array([6.247e-05, 1.851e-02, 9.814e-01], dtype=float16), array([4.166e-05, 3.982e-05, 1.000e+00], dtype=float16), array([1.605e-04, 8.173e-02, 9.180e-01], dtype=float16), array([6.139e-05, 6.805e-03, 9.932e-01], dtype=float16), array([3.278e-06, 3.867e-04, 9.995e-01], dtype=float16), array([0.001038, 0.1283  , 0.8706  ], dtype=float16), array([5.847e-05, 9.351e-01, 6.464e-02], dtype=float16), array([4.95e-06, 6.05e-05, 1.00e+00], dtype=float16), array([5.722e-06, 2.923e-04, 9.995e-01], dtype=float16), array([0.0007863, 0.2307   , 0.7686   ], dtype=float16), array([4.804e-05, 5.898e-03, 9.941e-01], dtype=float16), array([5.037e-05, 1.655e-03, 9.985e-01], dtype=float16), array([3.862e-04, 1.571e-01, 8.428e-01], dtype=float16), array([2.527e-05, 4.269e-03, 9.956e-01], dtype=float16), array([8.23e-06, 1.01e-05, 1.00e+00], dtype=float16), array([0.2421 , 0.02477, 0.7334 ], dtype=float16), array([2.44e-06, 4.47e-06, 1.00e+00], dtype=float16), array([0.01752, 0.8984 , 0.08386], dtype=float16), array([6.12e-05, 9.55e-03, 9.90e-01], dtype=float16), array([1.688e-04, 1.423e-01, 8.574e-01], dtype=float16), array([3.076e-05, 7.145e-03, 9.927e-01], dtype=float16), array([3.344e-05, 1.625e-02, 9.839e-01], dtype=float16), array([8.25e-05, 1.61e-02, 9.84e-01], dtype=float16), array([5.305e-05, 4.741e-01, 5.259e-01], dtype=float16), array([8.112e-05, 1.158e-03, 9.985e-01], dtype=float16), array([1.484e-05, 6.747e-04, 9.995e-01], dtype=float16), array([3.e-07, 8.e-07, 1.e+00], dtype=float16), array([4.172e-07, 1.792e-04, 1.000e+00], dtype=float16), array([7.558e-04, 1.321e-01, 8.672e-01], dtype=float16), array([3.695e-06, 8.525e-01, 1.476e-01], dtype=float16), array([3.147e-04, 4.514e-02, 9.546e-01], dtype=float16), array([2.444e-06, 1.189e-04, 1.000e+00], dtype=float16), array([8.9e-07, 3.2e-06, 1.0e+00], dtype=float16), array([8.14e-05, 8.70e-06, 1.00e+00], dtype=float16), array([2.027e-06, 1.251e-03, 9.985e-01], dtype=float16), array([2.223e-05, 8.529e-05, 1.000e+00], dtype=float16), array([2.694e-05, 5.856e-03, 9.941e-01], dtype=float16), array([7.987e-06, 8.194e-03, 9.917e-01], dtype=float16), array([2.608e-04, 2.470e-02, 9.751e-01], dtype=float16), array([1.324e-04, 7.725e-03, 9.922e-01], dtype=float16), array([3.082e-05, 2.583e-03, 9.976e-01], dtype=float16), array([7.248e-05, 1.274e-03, 9.985e-01], dtype=float16), array([6.44e-06, 5.30e-06, 1.00e+00], dtype=float16), array([3.028e-04, 1.002e-02, 9.897e-01], dtype=float16), array([9.739e-05, 2.033e-03, 9.980e-01], dtype=float16), array([5.60e-06, 1.67e-06, 1.00e+00], dtype=float16), array([5.782e-06, 5.260e-03, 9.946e-01], dtype=float16), array([2.747e-04, 1.572e-02, 9.839e-01], dtype=float16), array([3.052e-04, 5.166e-01, 4.832e-01], dtype=float16), array([1.0556e-04, 3.8795e-03, 9.9609e-01], dtype=float16), array([4.303e-05, 2.910e-01, 7.090e-01], dtype=float16), array([2.474e-05, 7.927e-03, 9.922e-01], dtype=float16), array([5.026e-04, 2.061e-02, 9.790e-01], dtype=float16), array([2.450e-05, 4.169e-03, 9.956e-01], dtype=float16), array([5.329e-05, 1.609e-04, 1.000e+00], dtype=float16), array([3.010e-05, 8.872e-01, 1.128e-01], dtype=float16), array([1.001e-05, 5.074e-04, 9.995e-01], dtype=float16), array([4.786e-05, 1.249e-03, 9.985e-01], dtype=float16), array([1.85e-05, 2.81e-05, 1.00e+00], dtype=float16), array([5.817e-05, 2.275e-03, 9.976e-01], dtype=float16), array([2.390e-05, 5.470e-03, 9.946e-01], dtype=float16), array([1.35e-05, 1.64e-05, 1.00e+00], dtype=float16), array([6.127e-05, 4.821e-04, 9.995e-01], dtype=float16), array([6.372e-05, 3.198e-02, 9.678e-01], dtype=float16), array([1.377e-05, 4.768e-06, 1.000e+00], dtype=float16), array([1.6e-06, 1.0e-06, 1.0e+00], dtype=float16), array([3.165e-05, 4.868e-03, 9.951e-01], dtype=float16), array([8.392e-05, 2.693e-03, 9.971e-01], dtype=float16), array([2.199e-05, 2.278e-01, 7.720e-01], dtype=float16), array([2.171e-04, 5.207e-03, 9.946e-01], dtype=float16), array([5.543e-06, 3.872e-04, 9.995e-01], dtype=float16), array([1.72e-05, 7.95e-05, 1.00e+00], dtype=float16), array([8.124e-05, 3.839e-02, 9.614e-01], dtype=float16), array([6.139e-06, 1.074e-04, 1.000e+00], dtype=float16), array([9.239e-06, 4.726e-03, 9.951e-01], dtype=float16), array([2.339e-04, 1.016e-03, 9.985e-01], dtype=float16), array([0.002296, 0.443   , 0.5547  ], dtype=float16), array([5.180e-05, 4.349e-03, 9.956e-01], dtype=float16), array([0.006676, 0.00829 , 0.985   ], dtype=float16), array([8.970e-05, 1.486e-01, 8.516e-01], dtype=float16), array([3.469e-05, 3.785e-05, 1.000e+00], dtype=float16), array([5.245e-06, 1.155e-03, 9.990e-01], dtype=float16), array([2.942e-04, 4.947e-05, 9.995e-01], dtype=float16), array([3.058e-05, 1.128e-03, 9.990e-01], dtype=float16), array([4.888e-06, 1.205e-03, 9.990e-01], dtype=float16), array([2.50e-06, 1.61e-05, 1.00e+00], dtype=float16), array([9.012e-05, 1.335e-05, 1.000e+00], dtype=float16), array([9.477e-06, 1.149e-03, 9.990e-01], dtype=float16), array([2.15e-06, 8.94e-07, 1.00e+00], dtype=float16), array([4.768e-05, 1.726e-02, 9.829e-01], dtype=float16), array([4.65e-06, 2.47e-05, 1.00e+00], dtype=float16), array([6.258e-06, 1.475e-02, 9.854e-01], dtype=float16), array([6.741e-05, 5.807e-02, 9.419e-01], dtype=float16), array([9.239e-06, 1.373e-04, 1.000e+00], dtype=float16), array([3.927e-04, 7.385e-02, 9.258e-01], dtype=float16), array([1.329e-05, 1.544e-02, 9.844e-01], dtype=float16), array([9.18e-06, 1.78e-05, 1.00e+00], dtype=float16), array([0.00425, 0.6973 , 0.2983 ], dtype=float16), array([1.349e-04, 1.029e-02, 9.897e-01], dtype=float16), array([1.371e-05, 1.704e-04, 1.000e+00], dtype=float16), array([3.91e-05, 8.86e-03, 9.91e-01], dtype=float16), array([1.264e-04, 1.643e-01, 8.354e-01], dtype=float16), array([2.999e-04, 4.213e-04, 9.995e-01], dtype=float16), array([4.178e-05, 5.779e-04, 9.995e-01], dtype=float16), array([0.0508  , 0.006027, 0.9434  ], dtype=float16), array([2.354e-05, 1.384e-03, 9.985e-01], dtype=float16), array([7.868e-06, 1.285e-04, 1.000e+00], dtype=float16), array([5.374e-04, 1.537e-01, 8.457e-01], dtype=float16), array([5.407e-04, 8.069e-02, 9.189e-01], dtype=float16), array([7.424e-04, 1.803e-01, 8.188e-01], dtype=float16), array([1.837e-04, 7.014e-04, 9.990e-01], dtype=float16), array([0.119  , 0.00905, 0.872  ], dtype=float16), array([2.670e-05, 1.311e-02, 9.868e-01], dtype=float16), array([1.454e-05, 3.887e-03, 9.961e-01], dtype=float16), array([8.577e-05, 1.189e-04, 1.000e+00], dtype=float16), array([1.726e-04, 7.843e-03, 9.922e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.283e-05, 3.298e-03, 9.966e-01], dtype=float16), array([2.062e-05, 2.369e-04, 9.995e-01], dtype=float16), array([5.364e-05, 1.186e-03, 9.985e-01], dtype=float16), array([2.348e-05, 3.437e-03, 9.966e-01], dtype=float16), array([5.829e-05, 2.518e-04, 9.995e-01], dtype=float16), array([2.712e-05, 4.623e-02, 9.536e-01], dtype=float16), array([0.00103, 0.02925, 0.9697 ], dtype=float16), array([3.755e-06, 1.494e-04, 1.000e+00], dtype=float16), array([1.633e-05, 2.472e-04, 9.995e-01], dtype=float16), array([2.921e-06, 2.639e-04, 9.995e-01], dtype=float16), array([8.500e-05, 7.784e-05, 1.000e+00], dtype=float16), array([2.056e-05, 1.866e-05, 1.000e+00], dtype=float16), array([1.472e-05, 1.331e-02, 9.868e-01], dtype=float16), array([6.276e-05, 5.293e-04, 9.995e-01], dtype=float16), array([1.240e-05, 4.656e-04, 9.995e-01], dtype=float16), array([0.00289, 0.00756, 0.9897 ], dtype=float16), array([8.702e-06, 5.336e-04, 9.995e-01], dtype=float16), array([5.732e-04, 2.956e-02, 9.697e-01], dtype=float16), array([2.229e-05, 5.436e-03, 9.946e-01], dtype=float16), array([5.704e-05, 2.571e-02, 9.741e-01], dtype=float16), array([2.7061e-05, 1.3725e-02, 9.8633e-01], dtype=float16), array([5.01e-06, 9.97e-04, 9.99e-01], dtype=float16), array([1.121e-05, 2.468e-03, 9.976e-01], dtype=float16), array([3.803e-05, 9.270e-03, 9.907e-01], dtype=float16), array([2.8133e-05, 1.0284e-02, 9.8975e-01], dtype=float16), array([1.192e-05, 7.782e-02, 9.224e-01], dtype=float16), array([4.756e-04, 1.902e-01, 8.096e-01], dtype=float16), array([8.148e-05, 5.058e-03, 9.946e-01], dtype=float16), array([5.364e-06, 5.140e-04, 9.995e-01], dtype=float16), array([1.6463e-04, 1.4626e-02, 9.8535e-01], dtype=float16), array([1.229e-04, 1.941e-02, 9.805e-01], dtype=float16), array([8.106e-06, 1.726e-04, 1.000e+00], dtype=float16), array([5.764e-05, 8.821e-06, 1.000e+00], dtype=float16), array([2.28e-05, 9.49e-04, 9.99e-01], dtype=float16), array([0.003515, 0.02553 , 0.971   ], dtype=float16), array([3.928e-05, 3.643e-03, 9.961e-01], dtype=float16), array([2.046e-04, 1.759e-01, 8.237e-01], dtype=float16), array([1.669e-06, 2.579e-03, 9.976e-01], dtype=float16), array([1.543e-04, 7.137e-03, 9.927e-01], dtype=float16), array([1.138e-05, 1.264e-05, 1.000e+00], dtype=float16), array([7.385e-05, 5.707e-03, 9.941e-01], dtype=float16), array([6.974e-05, 2.152e-01, 7.847e-01], dtype=float16), array([7.087e-05, 2.784e-05, 1.000e+00], dtype=float16), array([2.390e-05, 1.654e-03, 9.985e-01], dtype=float16), array([3.493e-04, 3.169e-04, 9.995e-01], dtype=float16), array([2.325e-05, 8.110e-03, 9.917e-01], dtype=float16), array([8.696e-05, 1.006e-02, 9.897e-01], dtype=float16), array([3.176e-04, 2.517e-01, 7.480e-01], dtype=float16), array([6.843e-05, 2.132e-02, 9.785e-01], dtype=float16), array([5.741e-04, 8.679e-02, 9.126e-01], dtype=float16), array([3.22e-05, 9.21e-04, 9.99e-01], dtype=float16), array([3.451e-05, 8.057e-03, 9.917e-01], dtype=float16), array([5.382e-05, 2.462e-01, 7.539e-01], dtype=float16), array([1.1975e-04, 2.5311e-03, 9.9756e-01], dtype=float16), array([0.001107, 0.1729  , 0.826   ], dtype=float16), array([1.1224e-04, 3.5309e-02, 9.6436e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.772e-05, 3.982e-04, 9.995e-01], dtype=float16), array([1.830e-05, 1.269e-03, 9.985e-01], dtype=float16), array([1.776e-05, 1.042e-03, 9.990e-01], dtype=float16), array([2.43e-05, 2.85e-03, 9.97e-01], dtype=float16), array([9.00e-06, 2.66e-05, 1.00e+00], dtype=float16), array([1.466e-05, 1.918e-02, 9.810e-01], dtype=float16), array([9.412e-05, 1.836e-01, 8.164e-01], dtype=float16), array([4.27e-04, 5.68e-01, 4.32e-01], dtype=float16), array([1.067e-05, 1.058e-03, 9.990e-01], dtype=float16), array([3.34e-06, 1.73e-05, 1.00e+00], dtype=float16), array([3.135e-05, 4.667e-05, 1.000e+00], dtype=float16), array([3.457e-06, 2.518e-04, 9.995e-01], dtype=float16), array([5.960e-06, 2.073e-03, 9.980e-01], dtype=float16), array([0.01892 , 0.005787, 0.975   ], dtype=float16), array([4.071e-05, 3.485e-03, 9.966e-01], dtype=float16), array([1.544e-04, 1.428e-02, 9.854e-01], dtype=float16), array([2.599e-05, 2.684e-03, 9.971e-01], dtype=float16), array([5.656e-05, 2.393e-02, 9.761e-01], dtype=float16), array([2.090e-04, 1.958e-02, 9.800e-01], dtype=float16), array([7.325e-05, 2.867e-02, 9.712e-01], dtype=float16), array([0.02414, 0.4707 , 0.505  ], dtype=float16), array([5.990e-05, 2.995e-04, 9.995e-01], dtype=float16), array([1.436e-05, 3.029e-03, 9.971e-01], dtype=float16), array([2.766e-05, 4.846e-02, 9.517e-01], dtype=float16), array([2.842e-04, 1.139e-02, 9.883e-01], dtype=float16), array([2.725e-04, 1.588e-02, 9.839e-01], dtype=float16), array([1.967e-05, 6.037e-04, 9.995e-01], dtype=float16), array([0.01344, 0.3013 , 0.6855 ], dtype=float16), array([0.002716, 0.2311  , 0.766   ], dtype=float16), array([1.311e-05, 7.431e-03, 9.927e-01], dtype=float16), array([4.107e-05, 2.140e-03, 9.980e-01], dtype=float16), array([2.325e-04, 2.271e-02, 9.771e-01], dtype=float16), array([5.407e-04, 1.262e-02, 9.868e-01], dtype=float16), array([1.235e-04, 6.851e-01, 3.149e-01], dtype=float16), array([1.240e-05, 4.756e-04, 9.995e-01], dtype=float16), array([1.612e-04, 9.964e-03, 9.897e-01], dtype=float16), array([8.42e-05, 7.51e-06, 1.00e+00], dtype=float16), array([9.418e-06, 5.114e-05, 1.000e+00], dtype=float16), array([8.714e-05, 9.857e-03, 9.902e-01], dtype=float16), array([1.736e-04, 2.956e-02, 9.702e-01], dtype=float16), array([0.02199, 0.01906, 0.959  ], dtype=float16), array([3.531e-04, 4.951e-03, 9.946e-01], dtype=float16), array([0.001486, 0.0428  , 0.9556  ], dtype=float16), array([1.88e-04, 7.18e-04, 9.99e-01], dtype=float16), array([1.27e-05, 3.36e-05, 1.00e+00], dtype=float16), array([7.510e-06, 6.309e-04, 9.995e-01], dtype=float16), array([5.536e-04, 7.988e-03, 9.912e-01], dtype=float16), array([0.01177, 0.079  , 0.909  ], dtype=float16), array([1.058e-04, 5.383e-03, 9.946e-01], dtype=float16), array([2.310e-04, 1.808e-03, 9.980e-01], dtype=float16), array([1.156e-05, 2.398e-03, 9.976e-01], dtype=float16), array([7.039e-05, 2.340e-03, 9.976e-01], dtype=float16), array([3.147e-04, 1.796e-01, 8.198e-01], dtype=float16), array([8.428e-05, 3.323e-02, 9.668e-01], dtype=float16), array([1.788e-06, 1.881e-04, 1.000e+00], dtype=float16), array([3.296e-05, 7.212e-05, 1.000e+00], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  4
---------------------------------------------------
Extracting Features for Prompt  5  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([4.470e-06, 4.656e-04, 9.995e-01], dtype=float16), array([4.8e-07, 4.6e-06, 1.0e+00], dtype=float16), array([0.003649, 0.8325  , 0.1641  ], dtype=float16), array([4.048e-04, 3.008e-02, 9.697e-01], dtype=float16), array([4.59e-06, 1.18e-05, 1.00e+00], dtype=float16), array([0.010635, 0.25    , 0.7393  ], dtype=float16), array([0.001062, 0.004696, 0.994   ], dtype=float16), array([3.982e-05, 6.439e-03, 9.937e-01], dtype=float16), array([2.561e-04, 1.529e-04, 9.995e-01], dtype=float16), array([1.069e-04, 8.350e-02, 9.165e-01], dtype=float16), array([2.401e-04, 3.451e-05, 9.995e-01], dtype=float16), array([6.920e-05, 2.930e-04, 9.995e-01], dtype=float16), array([4.941e-05, 1.674e-02, 9.834e-01], dtype=float16), array([2.533e-05, 4.837e-03, 9.951e-01], dtype=float16), array([6.020e-06, 3.308e-02, 9.668e-01], dtype=float16), array([0.000784, 0.6157  , 0.3838  ], dtype=float16), array([2.078e-04, 3.687e-02, 9.629e-01], dtype=float16), array([3.161e-04, 1.407e-02, 9.858e-01], dtype=float16), array([0.010864, 0.0377  , 0.9517  ], dtype=float16), array([2.367e-04, 2.642e-03, 9.971e-01], dtype=float16), array([2.117e-04, 2.872e-03, 9.971e-01], dtype=float16), array([6.08e-06, 3.04e-05, 1.00e+00], dtype=float16), array([7.758e-04, 1.441e-02, 9.849e-01], dtype=float16), array([2.09e-06, 2.69e-05, 1.00e+00], dtype=float16), array([7.278e-05, 6.022e-04, 9.995e-01], dtype=float16), array([0.0014 , 0.94   , 0.05847], dtype=float16), array([8.249e-05, 5.441e-04, 9.995e-01], dtype=float16), array([2.950e-05, 1.479e-03, 9.985e-01], dtype=float16), array([1.2577e-05, 1.2865e-03, 9.9854e-01], dtype=float16), array([3.910e-05, 3.110e-02, 9.688e-01], dtype=float16), array([7.792e-04, 5.833e-03, 9.932e-01], dtype=float16), array([8.351e-05, 2.010e-04, 9.995e-01], dtype=float16), array([6.771e-05, 6.256e-02, 9.375e-01], dtype=float16), array([0.01469, 0.2744 , 0.711  ], dtype=float16), array([0.0082, 0.824 , 0.1674], dtype=float16), array([0.0007734, 0.2301   , 0.769    ], dtype=float16), array([3.808e-04, 2.026e-01, 7.969e-01], dtype=float16), array([2.549e-04, 2.811e-03, 9.971e-01], dtype=float16), array([2.861e-06, 5.178e-04, 9.995e-01], dtype=float16), array([9.239e-06, 2.453e-04, 9.995e-01], dtype=float16), array([1.627e-05, 6.992e-03, 9.932e-01], dtype=float16), array([7.397e-05, 3.259e-02, 9.673e-01], dtype=float16), array([2.92e-06, 2.33e-05, 1.00e+00], dtype=float16), array([1.477e-04, 3.599e-03, 9.961e-01], dtype=float16), array([9.385e-01, 2.065e-04, 6.107e-02], dtype=float16), array([5.424e-05, 6.976e-02, 9.302e-01], dtype=float16), array([8.82e-06, 1.57e-05, 1.00e+00], dtype=float16), array([2.515e-05, 1.638e-04, 1.000e+00], dtype=float16), array([3.33e-05, 5.78e-06, 1.00e+00], dtype=float16), array([4.834e-05, 8.303e-05, 1.000e+00], dtype=float16), array([4.77e-07, 3.93e-05, 1.00e+00], dtype=float16), array([1.115e-05, 5.093e-04, 9.995e-01], dtype=float16), array([1.38e-05, 2.89e-03, 9.97e-01], dtype=float16), array([2.503e-06, 7.772e-04, 9.990e-01], dtype=float16), array([3.58e-07, 1.42e-05, 1.00e+00], dtype=float16), array([2.325e-06, 3.314e-04, 9.995e-01], dtype=float16), array([6.557e-07, 1.255e-04, 1.000e+00], dtype=float16), array([1.272e-04, 3.157e-03, 9.966e-01], dtype=float16), array([5.326e-04, 1.827e-03, 9.976e-01], dtype=float16), array([2.497e-05, 5.941e-04, 9.995e-01], dtype=float16), array([1.734e-05, 6.903e-02, 9.312e-01], dtype=float16), array([9.108e-05, 8.644e-03, 9.912e-01], dtype=float16), array([1.490e-05, 3.762e-04, 9.995e-01], dtype=float16), array([9.894e-06, 1.293e-05, 1.000e+00], dtype=float16), array([0.001052, 0.5024  , 0.4966  ], dtype=float16), array([1.338e-04, 3.568e-02, 9.644e-01], dtype=float16), array([7.21e-06, 3.74e-05, 1.00e+00], dtype=float16), array([2.38e-05, 2.67e-03, 9.97e-01], dtype=float16), array([0.1478, 0.0678, 0.784 ], dtype=float16), array([4.759e-04, 8.687e-01, 1.307e-01], dtype=float16), array([3.445e-04, 2.176e-02, 9.780e-01], dtype=float16), array([3.815e-06, 2.818e-04, 9.995e-01], dtype=float16), array([1.899e-04, 2.478e-03, 9.976e-01], dtype=float16), array([4.504e-04, 7.812e-03, 9.917e-01], dtype=float16), array([4.232e-06, 3.114e-04, 9.995e-01], dtype=float16), array([1.61e-05, 1.94e-03, 9.98e-01], dtype=float16), array([1.25e-06, 7.39e-06, 1.00e+00], dtype=float16), array([1.e-07, 0.e+00, 1.e+00], dtype=float16), array([4.902e-04, 8.716e-02, 9.126e-01], dtype=float16), array([9.358e-06, 1.430e-03, 9.985e-01], dtype=float16), array([6.0e-07, 9.5e-07, 1.0e+00], dtype=float16), array([1.55e-06, 5.78e-06, 1.00e+00], dtype=float16), array([4.77e-07, 3.69e-05, 1.00e+00], dtype=float16), array([1.13e-06, 6.79e-05, 1.00e+00], dtype=float16), array([1.669e-06, 3.355e-04, 9.995e-01], dtype=float16), array([0.001068, 0.00872 , 0.99    ], dtype=float16), array([4.172e-07, 3.207e-05, 1.000e+00], dtype=float16), array([2.295e-05, 1.620e-03, 9.985e-01], dtype=float16), array([4.54e-05, 9.48e-05, 1.00e+00], dtype=float16), array([0.00596, 0.9214 , 0.0726 ], dtype=float16), array([1.380e-04, 3.448e-03, 9.966e-01], dtype=float16), array([1.1e-06, 9.2e-06, 1.0e+00], dtype=float16), array([1.609e-06, 6.634e-05, 1.000e+00], dtype=float16), array([2.444e-06, 6.566e-04, 9.995e-01], dtype=float16), array([1.311e-06, 4.965e-05, 1.000e+00], dtype=float16), array([1.07e-06, 1.97e-06, 1.00e+00], dtype=float16), array([1.425e-05, 1.099e-03, 9.990e-01], dtype=float16), array([1.472e-05, 6.323e-02, 9.365e-01], dtype=float16), array([7.415e-04, 1.246e-02, 9.868e-01], dtype=float16), array([1.0e-05, 7.3e-06, 1.0e+00], dtype=float16), array([5.4e-07, 3.7e-06, 1.0e+00], dtype=float16), array([1.019e-05, 3.493e-04, 9.995e-01], dtype=float16), array([5.36e-07, 1.05e-05, 1.00e+00], dtype=float16), array([3.225e-05, 1.880e-02, 9.810e-01], dtype=float16), array([2.700e-05, 3.361e-03, 9.966e-01], dtype=float16), array([7.75e-07, 6.81e-05, 1.00e+00], dtype=float16), array([0.01205, 0.01984, 0.9683 ], dtype=float16), array([1.550e-06, 4.468e-04, 9.995e-01], dtype=float16), array([2.027e-06, 8.154e-05, 1.000e+00], dtype=float16), array([1.271e-04, 1.459e-03, 9.985e-01], dtype=float16), array([2.849e-05, 1.438e-03, 9.985e-01], dtype=float16), array([1.0e-06, 5.2e-06, 1.0e+00], dtype=float16), array([4.375e-05, 3.389e-03, 9.966e-01], dtype=float16), array([1.490e-06, 1.305e-04, 1.000e+00], dtype=float16), array([2.569e-05, 2.829e-03, 9.971e-01], dtype=float16), array([5.007e-06, 3.994e-05, 1.000e+00], dtype=float16), array([1.562e-05, 5.836e-04, 9.995e-01], dtype=float16), array([6.6e-07, 8.4e-06, 1.0e+00], dtype=float16), array([1.460e-05, 4.239e-04, 9.995e-01], dtype=float16), array([2.080e-05, 4.208e-03, 9.956e-01], dtype=float16), array([1.127e-05, 2.999e-04, 9.995e-01], dtype=float16), array([1.e-07, 3.e-07, 1.e+00], dtype=float16), array([2.277e-04, 5.999e-04, 9.990e-01], dtype=float16), array([0.09375, 0.0733 , 0.833  ], dtype=float16), array([1.800e-05, 3.016e-05, 1.000e+00], dtype=float16), array([0.000947, 0.0774  , 0.922   ], dtype=float16), array([1.788e-07, 1.484e-05, 1.000e+00], dtype=float16), array([2.606e-04, 1.092e-01, 8.906e-01], dtype=float16), array([7.749e-07, 1.302e-02, 9.868e-01], dtype=float16), array([2.217e-05, 1.001e-03, 9.990e-01], dtype=float16), array([1.773e-04, 1.247e-02, 9.873e-01], dtype=float16), array([8.661e-05, 2.472e-03, 9.976e-01], dtype=float16), array([2.563e-06, 2.737e-04, 9.995e-01], dtype=float16), array([3.81e-06, 1.34e-04, 1.00e+00], dtype=float16), array([3.0e-07, 6.8e-06, 1.0e+00], dtype=float16), array([2.086e-06, 1.434e-04, 1.000e+00], dtype=float16), array([3.4e-06, 1.4e-06, 1.0e+00], dtype=float16), array([5.4e-07, 5.9e-06, 1.0e+00], dtype=float16), array([1.371e-06, 2.515e-05, 1.000e+00], dtype=float16), array([1.55e-06, 2.87e-05, 1.00e+00], dtype=float16), array([1.997e-05, 8.500e-05, 1.000e+00], dtype=float16), array([7.10e-05, 5.53e-02, 9.45e-01], dtype=float16), array([1.705e-05, 2.111e-04, 1.000e+00], dtype=float16), array([3.285e-04, 2.741e-03, 9.971e-01], dtype=float16), array([0.002579, 0.9927  , 0.00474 ], dtype=float16), array([1.663e-05, 2.607e-03, 9.976e-01], dtype=float16), array([3.809e-05, 4.859e-04, 9.995e-01], dtype=float16), array([2.894e-04, 2.726e-03, 9.971e-01], dtype=float16), array([2.514e-03, 8.826e-04, 9.966e-01], dtype=float16), array([1.1945e-04, 5.0964e-03, 9.9463e-01], dtype=float16), array([7.944e-04, 2.563e-03, 9.966e-01], dtype=float16), array([5.207e-04, 1.091e-02, 9.888e-01], dtype=float16), array([5.064e-04, 1.107e-01, 8.887e-01], dtype=float16), array([3.880e-05, 3.829e-04, 9.995e-01], dtype=float16), array([1.347e-02, 1.503e-04, 9.863e-01], dtype=float16), array([1.882e-04, 2.252e-02, 9.771e-01], dtype=float16), array([6.2037e-04, 1.3695e-02, 9.8584e-01], dtype=float16), array([0.007713, 0.06805 , 0.9243  ], dtype=float16), array([0.2133 , 0.08234, 0.7046 ], dtype=float16), array([3.874e-06, 1.037e-03, 9.990e-01], dtype=float16), array([3.459e-04, 1.756e-02, 9.819e-01], dtype=float16), array([5.679e-04, 9.854e-01, 1.422e-02], dtype=float16), array([0.002104, 0.04953 , 0.948   ], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([7.272e-06, 1.273e-04, 1.000e+00], dtype=float16), array([3.10e-06, 6.44e-06, 1.00e+00], dtype=float16), array([2.854e-04, 4.780e-03, 9.951e-01], dtype=float16), array([4.311e-04, 1.235e-02, 9.873e-01], dtype=float16), array([0.00435, 0.6494 , 0.3464 ], dtype=float16), array([0.002634, 0.4678  , 0.53    ], dtype=float16), array([0.001937, 0.2764  , 0.7217  ], dtype=float16), array([2.468e-05, 6.954e-03, 9.932e-01], dtype=float16), array([0.0017605, 0.004353 , 0.9937   ], dtype=float16), array([5.231e-04, 4.444e-03, 9.951e-01], dtype=float16), array([3.809e-03, 1.657e-04, 9.961e-01], dtype=float16), array([2.5e-06, 1.6e-05, 1.0e+00], dtype=float16), array([0.003838, 0.1364  , 0.86    ], dtype=float16), array([3.260e-05, 1.111e-02, 9.888e-01], dtype=float16), array([8.90e-05, 6.52e-03, 9.93e-01], dtype=float16), array([2.246e-04, 4.520e-04, 9.995e-01], dtype=float16), array([3.073e-04, 7.118e-03, 9.927e-01], dtype=float16), array([0.00591, 0.01096, 0.983  ], dtype=float16), array([8.05e-06, 9.36e-06, 1.00e+00], dtype=float16), array([2.037e-04, 6.351e-03, 9.937e-01], dtype=float16), array([1.448e-05, 1.466e-05, 1.000e+00], dtype=float16), array([5.9223e-04, 1.3214e-02, 9.8633e-01], dtype=float16), array([0.05893, 0.1065 , 0.8345 ], dtype=float16), array([2.718e-05, 1.097e-05, 1.000e+00], dtype=float16), array([3.64e-06, 3.57e-05, 1.00e+00], dtype=float16), array([1.577e-04, 6.191e-03, 9.937e-01], dtype=float16), array([3.142e-04, 2.499e-03, 9.971e-01], dtype=float16), array([0.002014, 0.3096  , 0.6885  ], dtype=float16), array([7.2479e-05, 1.1425e-03, 9.9902e-01], dtype=float16), array([8.6e-06, 5.4e-07, 1.0e+00], dtype=float16), array([0.001138, 0.10406 , 0.895   ], dtype=float16), array([1.252e-05, 1.292e-03, 9.985e-01], dtype=float16), array([9.508e-04, 4.473e-04, 9.985e-01], dtype=float16), array([0.123  , 0.784  , 0.09283], dtype=float16), array([0.01144, 0.6167 , 0.3718 ], dtype=float16), array([4.418e-04, 3.992e-02, 9.595e-01], dtype=float16), array([3.0e-07, 2.4e-07, 1.0e+00], dtype=float16), array([2.825e-05, 9.465e-04, 9.990e-01], dtype=float16), array([2.7990e-04, 1.2886e-02, 9.8682e-01], dtype=float16), array([4.780e-05, 7.141e-03, 9.927e-01], dtype=float16), array([2.325e-06, 5.264e-04, 9.995e-01], dtype=float16), array([4.232e-06, 4.471e-03, 9.956e-01], dtype=float16), array([8.726e-05, 1.095e-01, 8.906e-01], dtype=float16), array([2.378e-05, 1.275e-02, 9.873e-01], dtype=float16), array([5.808e-04, 9.460e-03, 9.897e-01], dtype=float16), array([4.65e-06, 1.18e-05, 1.00e+00], dtype=float16), array([7.157e-04, 4.132e-02, 9.580e-01], dtype=float16), array([2.2948e-05, 1.1444e-02, 9.8877e-01], dtype=float16), array([6.974e-06, 8.793e-04, 9.990e-01], dtype=float16), array([2.12e-05, 1.75e-04, 1.00e+00], dtype=float16), array([1.318e-04, 3.332e-05, 1.000e+00], dtype=float16), array([0.001055, 0.01553 , 0.9834  ], dtype=float16), array([3.901e-04, 1.799e-01, 8.198e-01], dtype=float16), array([3.304e-04, 1.675e-02, 9.829e-01], dtype=float16), array([3.60e-04, 3.98e-02, 9.60e-01], dtype=float16), array([5.026e-04, 9.201e-03, 9.902e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.09265, 0.1279 , 0.7793 ], dtype=float16), array([1.425e-05, 3.117e-03, 9.971e-01], dtype=float16), array([5.60e-06, 1.43e-06, 1.00e+00], dtype=float16), array([0.0146 , 0.9077 , 0.07764], dtype=float16), array([4.13e-04, 8.54e-03, 9.91e-01], dtype=float16), array([6.032e-05, 9.990e-01, 1.007e-03], dtype=float16), array([1.788e-04, 9.321e-01, 6.769e-02], dtype=float16), array([5.305e-06, 3.027e-03, 9.971e-01], dtype=float16), array([3.681e-04, 4.150e-03, 9.956e-01], dtype=float16), array([2.170e-05, 2.348e-04, 9.995e-01], dtype=float16), array([6.020e-06, 6.207e-03, 9.937e-01], dtype=float16), array([1.261e-04, 1.747e-03, 9.980e-01], dtype=float16), array([2.337e-05, 3.514e-04, 9.995e-01], dtype=float16), array([0.001805, 0.8857  , 0.1127  ], dtype=float16), array([5.305e-06, 5.066e-05, 1.000e+00], dtype=float16), array([1.66e-05, 4.12e-05, 1.00e+00], dtype=float16), array([3.898e-05, 2.866e-01, 7.134e-01], dtype=float16), array([2.778e-05, 2.159e-02, 9.785e-01], dtype=float16), array([0.01488, 0.02878, 0.9565 ], dtype=float16), array([8.76e-06, 1.21e-05, 1.00e+00], dtype=float16), array([3.439e-05, 9.956e-01, 4.532e-03], dtype=float16), array([1.732e-04, 8.223e-01, 1.777e-01], dtype=float16), array([1.382e-04, 2.744e-01, 7.256e-01], dtype=float16), array([6.437e-06, 1.281e-03, 9.985e-01], dtype=float16), array([9.460e-04, 1.113e-02, 9.878e-01], dtype=float16), array([1.777e-04, 2.224e-03, 9.976e-01], dtype=float16), array([0.00734 , 0.003017, 0.9897  ], dtype=float16), array([1.154e-04, 6.860e-02, 9.312e-01], dtype=float16), array([1.015e-02, 1.174e-05, 9.897e-01], dtype=float16), array([2.137e-04, 3.283e-03, 9.966e-01], dtype=float16), array([2.407e-04, 9.567e-03, 9.902e-01], dtype=float16), array([0.00476 , 0.001859, 0.993   ], dtype=float16), array([0.05487, 0.03665, 0.9087 ], dtype=float16), array([1.117e-04, 6.974e-05, 1.000e+00], dtype=float16), array([0.00749 , 0.001212, 0.991   ], dtype=float16), array([1.844e-04, 2.354e-04, 9.995e-01], dtype=float16), array([1.566e-04, 1.508e-02, 9.849e-01], dtype=float16), array([3.514e-04, 9.758e-03, 9.897e-01], dtype=float16), array([6.73e-05, 9.87e-04, 9.99e-01], dtype=float16), array([2.102e-04, 4.502e-01, 5.493e-01], dtype=float16), array([9.936e-05, 7.461e-01, 2.539e-01], dtype=float16), array([0.3213, 0.2461, 0.4326], dtype=float16), array([0.003355, 0.1576  , 0.839   ], dtype=float16), array([0.001904, 0.2235  , 0.7744  ], dtype=float16), array([1.1605e-04, 4.1199e-02, 9.5850e-01], dtype=float16), array([3.407e-04, 5.299e-03, 9.941e-01], dtype=float16), array([1.084e-04, 1.599e-02, 9.839e-01], dtype=float16), array([0.00598, 0.0229 , 0.971  ], dtype=float16), array([6.1989e-06, 1.1104e-04, 1.0000e+00], dtype=float16), array([0.006504, 0.4978  , 0.4958  ], dtype=float16), array([7.9966e-04, 1.1215e-02, 9.8779e-01], dtype=float16), array([8.5e-06, 3.2e-05, 1.0e+00], dtype=float16), array([2.344e-04, 6.235e-01, 3.760e-01], dtype=float16), array([6.161e-04, 3.156e-02, 9.678e-01], dtype=float16), array([1.024e-04, 1.455e-01, 8.545e-01], dtype=float16), array([5.093e-04, 1.207e-02, 9.873e-01], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  5
---------------------------------------------------
Extracting Features for Prompt  6  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([9.352e-05, 7.974e-01, 2.028e-01], dtype=float16), array([1.013e-05, 1.714e-02, 9.829e-01], dtype=float16), array([1.603e-05, 7.027e-03, 9.932e-01], dtype=float16), array([0.003574, 0.0207  , 0.9756  ], dtype=float16), array([3.868e-05, 7.783e-01, 2.217e-01], dtype=float16), array([4.551e-04, 1.749e-01, 8.247e-01], dtype=float16), array([0.001005, 0.912   , 0.0868  ], dtype=float16), array([7.373e-05, 2.217e-02, 9.775e-01], dtype=float16), array([5.817e-04, 8.038e-02, 9.189e-01], dtype=float16), array([3.467e-04, 1.176e-01, 8.818e-01], dtype=float16), array([3.543e-04, 6.431e-01, 3.564e-01], dtype=float16), array([3.533e-04, 4.661e-01, 5.337e-01], dtype=float16), array([1.465e-04, 4.703e-05, 1.000e+00], dtype=float16), array([8.816e-05, 1.498e-01, 8.501e-01], dtype=float16), array([3.296e-05, 6.804e-04, 9.995e-01], dtype=float16), array([0.0005517, 0.514    , 0.485    ], dtype=float16), array([3.614e-04, 8.628e-01, 1.370e-01], dtype=float16), array([3.03e-05, 3.27e-05, 1.00e+00], dtype=float16), array([3.822e-04, 9.503e-02, 9.048e-01], dtype=float16), array([0.0368, 0.1638, 0.7993], dtype=float16), array([3.177e-05, 4.377e-01, 5.620e-01], dtype=float16), array([0.000945, 0.1998  , 0.7993  ], dtype=float16), array([4.480e-04, 8.809e-01, 1.185e-01], dtype=float16), array([2.325e-04, 3.027e-03, 9.966e-01], dtype=float16), array([3.648e-05, 9.041e-03, 9.907e-01], dtype=float16), array([8.541e-05, 3.936e-04, 9.995e-01], dtype=float16), array([1.435e-04, 1.273e-03, 9.985e-01], dtype=float16), array([7.062e-04, 2.717e-01, 7.275e-01], dtype=float16), array([3.290e-05, 6.137e-02, 9.385e-01], dtype=float16), array([2.832e-04, 8.711e-01, 1.284e-01], dtype=float16), array([0.001827, 0.1528  , 0.845   ], dtype=float16), array([2.298e-04, 2.266e-01, 7.734e-01], dtype=float16), array([6.682e-05, 2.107e-01, 7.891e-01], dtype=float16), array([6.909e-04, 1.332e-02, 9.858e-01], dtype=float16), array([0.012375, 0.449   , 0.5386  ], dtype=float16), array([1.907e-06, 9.995e-01, 4.027e-04], dtype=float16), array([2.104e-05, 6.615e-03, 9.932e-01], dtype=float16), array([1.569e-04, 4.412e-01, 5.586e-01], dtype=float16), array([4.917e-05, 1.679e-03, 9.980e-01], dtype=float16), array([0.0015545, 0.05154  , 0.947    ], dtype=float16), array([1.239e-04, 7.844e-05, 1.000e+00], dtype=float16), array([0.006252, 0.6357  , 0.358   ], dtype=float16), array([0.000962, 0.661   , 0.3376  ], dtype=float16), array([7.409e-05, 2.352e-01, 7.646e-01], dtype=float16), array([0.015465, 0.871   , 0.1136  ], dtype=float16), array([6.378e-05, 6.675e-01, 3.325e-01], dtype=float16), array([1.651e-04, 5.109e-02, 9.487e-01], dtype=float16), array([2.024e-04, 2.046e-02, 9.795e-01], dtype=float16), array([6.037e-04, 5.096e-02, 9.482e-01], dtype=float16), array([0.003063, 0.3386  , 0.658   ], dtype=float16), array([0.03976, 0.898  , 0.06207], dtype=float16), array([4.518e-05, 5.005e-02, 9.497e-01], dtype=float16), array([0.001188, 0.007614, 0.991   ], dtype=float16), array([5.955e-05, 6.458e-03, 9.937e-01], dtype=float16), array([4.494e-05, 9.381e-02, 9.062e-01], dtype=float16), array([5.126e-06, 5.131e-04, 9.995e-01], dtype=float16), array([5.11e-04, 8.64e-02, 9.13e-01], dtype=float16), array([5.2357e-04, 1.5045e-02, 9.8438e-01], dtype=float16), array([9.847e-05, 8.745e-01, 1.254e-01], dtype=float16), array([0.001722, 0.3394  , 0.659   ], dtype=float16), array([1.391e-04, 1.232e-03, 9.985e-01], dtype=float16), array([4.575e-04, 1.319e-02, 9.863e-01], dtype=float16), array([8.947e-05, 1.013e-03, 9.990e-01], dtype=float16), array([7.37e-05, 1.62e-05, 1.00e+00], dtype=float16), array([0.001146, 0.1267  , 0.872   ], dtype=float16), array([1.060e-04, 4.956e-02, 9.502e-01], dtype=float16), array([2.2256e-04, 1.0175e-01, 8.9795e-01], dtype=float16), array([3.839e-05, 5.895e-05, 1.000e+00], dtype=float16), array([8.09e-05, 5.92e-02, 9.41e-01], dtype=float16), array([1.506e-04, 3.967e-03, 9.961e-01], dtype=float16), array([6.598e-05, 1.044e-01, 8.955e-01], dtype=float16), array([1.913e-05, 9.283e-02, 9.072e-01], dtype=float16), array([3.511e-05, 6.848e-02, 9.316e-01], dtype=float16), array([2.87e-05, 1.70e-04, 1.00e+00], dtype=float16), array([3.998e-04, 6.390e-02, 9.355e-01], dtype=float16), array([5.060e-05, 1.238e-02, 9.878e-01], dtype=float16), array([0.0008726, 0.554    , 0.4448   ], dtype=float16), array([5.406e-05, 7.043e-04, 9.990e-01], dtype=float16), array([3.016e-05, 5.969e-02, 9.404e-01], dtype=float16), array([3.099e-04, 1.333e-01, 8.662e-01], dtype=float16), array([3.083e-04, 2.274e-01, 7.725e-01], dtype=float16), array([5.144e-05, 1.926e-04, 1.000e+00], dtype=float16), array([4.187e-04, 8.276e-02, 9.170e-01], dtype=float16), array([9.799e-05, 1.781e-04, 9.995e-01], dtype=float16), array([1.466e-05, 3.080e-03, 9.971e-01], dtype=float16), array([6.509e-04, 7.744e-01, 2.247e-01], dtype=float16), array([7.505e-04, 6.113e-04, 9.985e-01], dtype=float16), array([1.272e-04, 2.031e-02, 9.795e-01], dtype=float16), array([3.750e-04, 1.526e-02, 9.844e-01], dtype=float16), array([8.078e-04, 7.465e-02, 9.243e-01], dtype=float16), array([1.401e-05, 7.179e-03, 9.927e-01], dtype=float16), array([8.923e-05, 1.278e-01, 8.721e-01], dtype=float16), array([1.149e-04, 6.166e-04, 9.995e-01], dtype=float16), array([1.7488e-04, 1.2494e-01, 8.7500e-01], dtype=float16), array([0.001037, 0.1473  , 0.8516  ], dtype=float16), array([0.0010605, 0.906    , 0.09296  ], dtype=float16), array([0.000654, 0.4878  , 0.5117  ], dtype=float16), array([3.517e-05, 4.646e-03, 9.951e-01], dtype=float16), array([3.273e-04, 1.615e-01, 8.384e-01], dtype=float16), array([3.533e-04, 6.104e-04, 9.990e-01], dtype=float16), array([1.45e-05, 9.05e-05, 1.00e+00], dtype=float16), array([7.111e-05, 1.892e-04, 9.995e-01], dtype=float16), array([2.104e-05, 3.054e-04, 9.995e-01], dtype=float16), array([5.45e-05, 7.32e-04, 9.99e-01], dtype=float16), array([1.031e-05, 2.949e-03, 9.971e-01], dtype=float16), array([0.001191, 0.04095 , 0.958   ], dtype=float16), array([6.574e-05, 5.474e-04, 9.995e-01], dtype=float16), array([3.397e-05, 7.198e-03, 9.927e-01], dtype=float16), array([3.982e-05, 6.588e-03, 9.932e-01], dtype=float16), array([7.63e-05, 9.35e-05, 1.00e+00], dtype=float16), array([1.323e-04, 4.520e-03, 9.951e-01], dtype=float16), array([4.756e-05, 3.868e-03, 9.961e-01], dtype=float16), array([0.001144, 0.968   , 0.03088 ], dtype=float16), array([1.369e-04, 1.774e-01, 8.223e-01], dtype=float16), array([0.002556, 0.05313 , 0.9443  ], dtype=float16), array([4.876e-05, 3.155e-03, 9.966e-01], dtype=float16), array([9.537e-06, 3.855e-03, 9.961e-01], dtype=float16), array([5.794e-05, 6.458e-03, 9.937e-01], dtype=float16), array([0.001788, 0.4163  , 0.582   ], dtype=float16), array([2.012e-04, 2.998e-02, 9.697e-01], dtype=float16), array([1.056e-04, 7.930e-01, 2.070e-01], dtype=float16), array([1.659e-04, 2.987e-04, 9.995e-01], dtype=float16), array([3.976e-05, 9.912e-05, 1.000e+00], dtype=float16), array([9.758e-03, 9.897e-01, 4.222e-04], dtype=float16), array([2.211e-05, 2.976e-01, 7.021e-01], dtype=float16), array([4.373e-04, 1.741e-01, 8.257e-01], dtype=float16), array([1.2165e-04, 1.1298e-01, 8.8672e-01], dtype=float16), array([2.697e-04, 5.823e-02, 9.414e-01], dtype=float16), array([8.601e-05, 9.674e-03, 9.902e-01], dtype=float16), array([2.058e-04, 1.962e-02, 9.800e-01], dtype=float16), array([4.542e-05, 2.344e-01, 7.656e-01], dtype=float16), array([1.458e-04, 1.456e-02, 9.854e-01], dtype=float16), array([6.18e-04, 3.27e-02, 9.67e-01], dtype=float16), array([5.651e-05, 9.966e-01, 3.256e-03], dtype=float16), array([8.821e-05, 3.893e-03, 9.961e-01], dtype=float16), array([0.00521, 0.307  , 0.688  ], dtype=float16), array([9.656e-05, 2.632e-02, 9.736e-01], dtype=float16), array([4.190e-05, 2.907e-02, 9.707e-01], dtype=float16), array([3.181e-04, 2.556e-01, 7.441e-01], dtype=float16), array([2.768e-04, 8.492e-03, 9.912e-01], dtype=float16), array([0.001333, 0.2993  , 0.699   ], dtype=float16), array([2.930e-04, 6.107e-03, 9.937e-01], dtype=float16), array([3.499e-05, 1.956e-01, 8.042e-01], dtype=float16), array([9.847e-05, 3.330e-03, 9.966e-01], dtype=float16), array([2.744e-04, 8.135e-01, 1.864e-01], dtype=float16), array([7.617e-05, 6.946e-02, 9.307e-01], dtype=float16), array([1.1915e-04, 7.3671e-05, 1.0000e+00], dtype=float16), array([2.28e-05, 3.71e-05, 1.00e+00], dtype=float16), array([3.531e-04, 1.165e-01, 8.833e-01], dtype=float16), array([5.126e-05, 3.469e-03, 9.966e-01], dtype=float16), array([1.584e-04, 1.868e-01, 8.130e-01], dtype=float16), array([0.01967, 0.613  , 0.3677 ], dtype=float16), array([6.795e-04, 3.323e-02, 9.663e-01], dtype=float16), array([2.995e-04, 2.422e-01, 7.573e-01], dtype=float16), array([4.427e-04, 2.652e-02, 9.731e-01], dtype=float16), array([0.02283, 0.7495 , 0.2277 ], dtype=float16), array([2.732e-04, 4.593e-02, 9.536e-01], dtype=float16), array([2.259e-04, 3.903e-02, 9.609e-01], dtype=float16), array([0.0009203, 0.1376   , 0.8613   ], dtype=float16), array([5.704e-05, 1.721e-02, 9.829e-01], dtype=float16), array([1.86e-05, 5.82e-02, 9.42e-01], dtype=float16), array([9.928e-04, 3.464e-04, 9.985e-01], dtype=float16), array([4.435e-04, 3.620e-03, 9.961e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.262e-04, 7.988e-03, 9.917e-01], dtype=float16), array([1.565e-04, 1.245e-01, 8.755e-01], dtype=float16), array([1.687e-04, 1.771e-01, 8.228e-01], dtype=float16), array([1.389e-04, 2.174e-02, 9.780e-01], dtype=float16), array([5.054e-04, 1.721e-02, 9.824e-01], dtype=float16), array([2.799e-04, 9.399e-01, 5.954e-02], dtype=float16), array([0.002436, 0.3591  , 0.638   ], dtype=float16), array([1.012e-04, 1.699e-01, 8.301e-01], dtype=float16), array([1.669e-04, 8.186e-03, 9.917e-01], dtype=float16), array([9.313e-04, 1.569e-03, 9.976e-01], dtype=float16), array([0.002544, 0.011955, 0.9854  ], dtype=float16), array([2.497e-05, 2.378e-04, 9.995e-01], dtype=float16), array([7.081e-04, 6.995e-02, 9.292e-01], dtype=float16), array([2.390e-04, 1.463e-02, 9.854e-01], dtype=float16), array([0.0008345, 0.3752   , 0.624    ], dtype=float16), array([2.377e-02, 5.012e-04, 9.756e-01], dtype=float16), array([3.595e-04, 7.758e-02, 9.219e-01], dtype=float16), array([4.294e-04, 9.546e-02, 9.043e-01], dtype=float16), array([5.132e-05, 1.399e-02, 9.858e-01], dtype=float16), array([3.231e-04, 2.842e-01, 7.153e-01], dtype=float16), array([2.801e-04, 1.218e-02, 9.873e-01], dtype=float16), array([9.2e-06, 6.8e-06, 1.0e+00], dtype=float16), array([0.001109, 0.00834 , 0.9907  ], dtype=float16), array([2.0087e-05, 1.0175e-01, 8.9844e-01], dtype=float16), array([1.357e-04, 2.785e-03, 9.971e-01], dtype=float16), array([7.457e-05, 3.809e-02, 9.619e-01], dtype=float16), array([1.907e-04, 3.826e-01, 6.172e-01], dtype=float16), array([0.001031, 0.1304  , 0.8687  ], dtype=float16), array([2.69e-05, 9.86e-04, 9.99e-01], dtype=float16), array([1.794e-04, 7.239e-02, 9.272e-01], dtype=float16), array([0.001548, 0.05756 , 0.941   ], dtype=float16), array([2.176e-05, 1.796e-03, 9.980e-01], dtype=float16), array([8.183e-04, 3.941e-03, 9.951e-01], dtype=float16), array([0.002354, 0.6     , 0.3975  ], dtype=float16), array([7.162e-04, 9.546e-01, 4.456e-02], dtype=float16), array([4.101e-05, 6.723e-05, 1.000e+00], dtype=float16), array([7.206e-05, 2.356e-03, 9.976e-01], dtype=float16), array([4.888e-05, 8.326e-04, 9.990e-01], dtype=float16), array([3.433e-05, 2.875e-02, 9.712e-01], dtype=float16), array([1.0705e-04, 5.4596e-02, 9.4531e-01], dtype=float16), array([2.688e-05, 2.522e-04, 9.995e-01], dtype=float16), array([2.837e-05, 1.634e-02, 9.834e-01], dtype=float16), array([7.731e-05, 3.333e-02, 9.668e-01], dtype=float16), array([3.150e-04, 1.506e-04, 9.995e-01], dtype=float16), array([3.1281e-04, 1.3485e-03, 9.9854e-01], dtype=float16), array([2.85e-05, 4.60e-02, 9.54e-01], dtype=float16), array([3.16e-05, 2.56e-05, 1.00e+00], dtype=float16), array([4.584e-05, 6.016e-01, 3.984e-01], dtype=float16), array([1.675e-05, 3.803e-05, 1.000e+00], dtype=float16), array([4.237e-04, 2.401e-04, 9.995e-01], dtype=float16), array([2.147e-04, 1.584e-01, 8.413e-01], dtype=float16), array([0.001714, 0.03662 , 0.9614  ], dtype=float16), array([7.243e-04, 6.741e-03, 9.927e-01], dtype=float16), array([0.002047, 0.003757, 0.994   ], dtype=float16), array([5.755e-04, 1.032e-02, 9.893e-01], dtype=float16), array([1.748e-04, 7.465e-02, 9.253e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.002012, 0.951   , 0.04672 ], dtype=float16), array([3.915e-04, 4.367e-02, 9.561e-01], dtype=float16), array([1.634e-04, 2.434e-04, 9.995e-01], dtype=float16), array([2.880e-04, 1.050e-01, 8.945e-01], dtype=float16), array([1.317e-04, 1.245e-02, 9.873e-01], dtype=float16), array([1.053e-04, 6.049e-02, 9.395e-01], dtype=float16), array([0.01662, 0.0953 , 0.888  ], dtype=float16), array([0.001114, 0.004105, 0.9946  ], dtype=float16), array([9.501e-05, 1.903e-02, 9.810e-01], dtype=float16), array([1.949e-04, 1.709e-02, 9.829e-01], dtype=float16), array([8.661e-05, 2.197e-02, 9.780e-01], dtype=float16), array([2.511e-04, 6.733e-04, 9.990e-01], dtype=float16), array([8.124e-05, 8.141e-03, 9.917e-01], dtype=float16), array([7.147e-05, 1.680e-02, 9.829e-01], dtype=float16), array([2.557e-05, 6.757e-04, 9.995e-01], dtype=float16), array([9.074e-04, 1.582e-02, 9.834e-01], dtype=float16), array([1.464e-04, 1.122e-03, 9.985e-01], dtype=float16), array([2.873e-05, 1.234e-04, 1.000e+00], dtype=float16), array([8.407e-04, 8.574e-01, 1.416e-01], dtype=float16), array([1.754e-04, 1.950e-02, 9.805e-01], dtype=float16), array([0.02174, 0.859  , 0.11926], dtype=float16), array([3.362e-04, 9.824e-01, 1.709e-02], dtype=float16), array([4.053e-06, 5.672e-03, 9.941e-01], dtype=float16), array([4.685e-04, 2.474e-03, 9.971e-01], dtype=float16), array([0.01634, 0.289  , 0.6943 ], dtype=float16), array([7.13e-04, 7.15e-03, 9.92e-01], dtype=float16), array([4.776e-04, 8.716e-01, 1.278e-01], dtype=float16), array([0.003223, 0.4656  , 0.5312  ], dtype=float16), array([0.002563, 0.00275 , 0.9946  ], dtype=float16), array([5.227e-05, 3.986e-03, 9.961e-01], dtype=float16), array([3.99e-05, 7.99e-04, 9.99e-01], dtype=float16), array([0.01121, 0.324  , 0.665  ], dtype=float16), array([0.001169, 0.9814  , 0.01727 ], dtype=float16), array([6.895e-04, 1.771e-01, 8.223e-01], dtype=float16), array([3.402e-04, 2.835e-02, 9.712e-01], dtype=float16), array([2.913e-04, 5.871e-05, 9.995e-01], dtype=float16), array([0.1149 , 0.04697, 0.838  ], dtype=float16), array([2.135e-04, 3.848e-02, 9.614e-01], dtype=float16), array([1.2165e-04, 2.7634e-02, 9.7217e-01], dtype=float16), array([9.427e-04, 1.259e-02, 9.863e-01], dtype=float16), array([5.670e-04, 3.931e-01, 6.064e-01], dtype=float16), array([0.0242, 0.909 , 0.0667], dtype=float16), array([0.0029  , 0.011986, 0.9854  ], dtype=float16), array([1.541e-03, 8.364e-04, 9.976e-01], dtype=float16), array([3.090e-04, 7.294e-02, 9.268e-01], dtype=float16), array([4.786e-05, 1.219e-03, 9.985e-01], dtype=float16), array([0.02277, 0.378  , 0.5996 ], dtype=float16), array([3.58e-05, 1.55e-06, 1.00e+00], dtype=float16), array([1.192e-05, 6.723e-05, 1.000e+00], dtype=float16), array([0.006145, 0.05997 , 0.934   ], dtype=float16), array([0.001046, 0.2075  , 0.7915  ], dtype=float16), array([2.551e-05, 5.293e-01, 4.707e-01], dtype=float16), array([8.116e-04, 8.540e-01, 1.450e-01], dtype=float16), array([0.001023, 0.3572  , 0.6416  ], dtype=float16), array([5.865e-05, 6.555e-02, 9.346e-01], dtype=float16), array([1.514e-04, 1.548e-01, 8.452e-01], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  6
---------------------------------------------------
Extracting Features for Prompt  7  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([7.451e-06, 1.093e-04, 1.000e+00], dtype=float16), array([1.550e-06, 3.517e-05, 1.000e+00], dtype=float16), array([8.9e-07, 9.5e-07, 1.0e+00], dtype=float16), array([1.246e-05, 6.258e-06, 1.000e+00], dtype=float16), array([1.97e-06, 4.77e-05, 1.00e+00], dtype=float16), array([1.633e-05, 7.057e-05, 1.000e+00], dtype=float16), array([9.364e-05, 6.439e-03, 9.937e-01], dtype=float16), array([3.934e-06, 7.004e-05, 1.000e+00], dtype=float16), array([1.067e-05, 1.818e-05, 1.000e+00], dtype=float16), array([3.0e-06, 5.7e-06, 1.0e+00], dtype=float16), array([9.54e-07, 3.38e-05, 1.00e+00], dtype=float16), array([3.1e-06, 3.1e-06, 1.0e+00], dtype=float16), array([3.040e-06, 1.615e-05, 1.000e+00], dtype=float16), array([5.42e-06, 7.37e-05, 1.00e+00], dtype=float16), array([2.116e-05, 2.476e-03, 9.976e-01], dtype=float16), array([1.39e-05, 4.41e-06, 1.00e+00], dtype=float16), array([0.01901 , 0.002577, 0.9785  ], dtype=float16), array([2.4e-07, 2.0e-06, 1.0e+00], dtype=float16), array([4.250e-05, 2.234e-04, 9.995e-01], dtype=float16), array([1.168e-05, 3.904e-05, 1.000e+00], dtype=float16), array([1.198e-05, 1.649e-03, 9.985e-01], dtype=float16), array([9.36e-06, 6.79e-06, 1.00e+00], dtype=float16), array([1.0e-06, 2.9e-05, 1.0e+00], dtype=float16), array([1.12e-05, 4.08e-05, 1.00e+00], dtype=float16), array([3.04e-06, 4.47e-06, 1.00e+00], dtype=float16), array([1.622e-04, 2.873e-05, 1.000e+00], dtype=float16), array([1.229e-04, 3.912e-04, 9.995e-01], dtype=float16), array([3.719e-05, 1.415e-04, 1.000e+00], dtype=float16), array([7.75e-07, 3.16e-06, 1.00e+00], dtype=float16), array([3.618e-05, 1.856e-04, 1.000e+00], dtype=float16), array([1.360e-04, 3.564e-05, 1.000e+00], dtype=float16), array([8.3e-07, 4.2e-06, 1.0e+00], dtype=float16), array([3.278e-06, 8.874e-04, 9.990e-01], dtype=float16), array([4.512e-05, 1.204e-05, 1.000e+00], dtype=float16), array([5.132e-05, 4.802e-04, 9.995e-01], dtype=float16), array([4.470e-05, 1.523e-04, 1.000e+00], dtype=float16), array([6.123e-04, 3.223e-03, 9.961e-01], dtype=float16), array([6.217e-05, 7.468e-05, 1.000e+00], dtype=float16), array([1.602e-04, 3.231e-04, 9.995e-01], dtype=float16), array([9.716e-06, 2.204e-04, 1.000e+00], dtype=float16), array([3.319e-04, 8.450e-04, 9.990e-01], dtype=float16), array([1.901e-05, 2.015e-05, 1.000e+00], dtype=float16), array([1.13e-06, 2.92e-06, 1.00e+00], dtype=float16), array([1.115e-05, 6.616e-05, 1.000e+00], dtype=float16), array([9.900e-05, 2.097e-02, 9.790e-01], dtype=float16), array([2.93e-05, 8.96e-05, 1.00e+00], dtype=float16), array([5.4e-07, 9.5e-06, 1.0e+00], dtype=float16), array([6.56e-07, 4.53e-06, 1.00e+00], dtype=float16), array([1.01e-06, 5.36e-06, 1.00e+00], dtype=float16), array([4.17e-07, 1.85e-06, 1.00e+00], dtype=float16), array([1.37e-06, 4.77e-06, 1.00e+00], dtype=float16), array([1.514e-05, 1.248e-04, 1.000e+00], dtype=float16), array([6.0e-07, 4.1e-06, 1.0e+00], dtype=float16), array([1.0e-06, 3.9e-06, 1.0e+00], dtype=float16), array([6.318e-06, 6.133e-05, 1.000e+00], dtype=float16), array([1.78e-05, 7.21e-06, 1.00e+00], dtype=float16), array([2.136e-04, 1.345e-04, 9.995e-01], dtype=float16), array([6.974e-05, 2.491e-04, 9.995e-01], dtype=float16), array([7.504e-05, 5.375e-03, 9.946e-01], dtype=float16), array([3.262e-04, 4.511e-04, 9.990e-01], dtype=float16), array([2.575e-05, 2.653e-03, 9.976e-01], dtype=float16), array([4.77e-07, 2.56e-06, 1.00e+00], dtype=float16), array([3.076e-04, 2.041e-01, 7.954e-01], dtype=float16), array([5.484e-06, 2.795e-05, 1.000e+00], dtype=float16), array([0.01591, 0.00335, 0.981  ], dtype=float16), array([4.035e-05, 8.039e-04, 9.990e-01], dtype=float16), array([2.819e-05, 2.384e-05, 1.000e+00], dtype=float16), array([2.03e-06, 6.05e-05, 1.00e+00], dtype=float16), array([2.56e-06, 5.48e-05, 1.00e+00], dtype=float16), array([6.217e-05, 5.466e-05, 1.000e+00], dtype=float16), array([8.3e-07, 1.8e-06, 1.0e+00], dtype=float16), array([2.146e-06, 1.355e-04, 1.000e+00], dtype=float16), array([7.75e-07, 1.25e-06, 1.00e+00], dtype=float16), array([2.98e-07, 2.86e-06, 1.00e+00], dtype=float16), array([8.9e-07, 1.7e-06, 1.0e+00], dtype=float16), array([2.9e-06, 7.5e-06, 1.0e+00], dtype=float16), array([1.560e-03, 2.813e-04, 9.980e-01], dtype=float16), array([1.25e-06, 3.99e-06, 1.00e+00], dtype=float16), array([3.0e-07, 1.9e-06, 1.0e+00], dtype=float16), array([1.4e-06, 4.5e-06, 1.0e+00], dtype=float16), array([1.855e-04, 1.807e-02, 9.819e-01], dtype=float16), array([6.56e-07, 1.55e-06, 1.00e+00], dtype=float16), array([1.085e-05, 1.419e-05, 1.000e+00], dtype=float16), array([4.470e-06, 8.017e-05, 1.000e+00], dtype=float16), array([2.9e-06, 3.3e-06, 1.0e+00], dtype=float16), array([6.437e-06, 1.293e-05, 1.000e+00], dtype=float16), array([1.37e-06, 2.26e-06, 1.00e+00], dtype=float16), array([5.186e-05, 3.455e-02, 9.653e-01], dtype=float16), array([3.748e-04, 3.867e-02, 9.609e-01], dtype=float16), array([1.22e-05, 5.13e-06, 1.00e+00], dtype=float16), array([1.544e-05, 1.011e-02, 9.897e-01], dtype=float16), array([1.293e-05, 6.497e-06, 1.000e+00], dtype=float16), array([6.14e-06, 1.10e-05, 1.00e+00], dtype=float16), array([4.8e-07, 2.5e-06, 1.0e+00], dtype=float16), array([8.3e-07, 2.4e-06, 1.0e+00], dtype=float16), array([1.967e-06, 2.339e-04, 1.000e+00], dtype=float16), array([5.e-07, 9.e-07, 1.e+00], dtype=float16), array([8.941e-07, 1.734e-05, 1.000e+00], dtype=float16), array([1.252e-05, 3.273e-04, 9.995e-01], dtype=float16), array([1.310e-04, 4.406e-04, 9.995e-01], dtype=float16), array([4.888e-06, 2.964e-04, 9.995e-01], dtype=float16), array([7.784e-05, 7.362e-03, 9.927e-01], dtype=float16), array([5.4e-07, 5.9e-06, 1.0e+00], dtype=float16), array([6.6e-07, 1.2e-06, 1.0e+00], dtype=float16), array([9.48e-06, 2.97e-05, 1.00e+00], dtype=float16), array([0.00201 , 0.002768, 0.995   ], dtype=float16), array([8.970e-05, 5.569e-04, 9.995e-01], dtype=float16), array([4.8e-07, 2.8e-06, 1.0e+00], dtype=float16), array([1.264e-05, 5.836e-04, 9.995e-01], dtype=float16), array([1.454e-05, 4.053e-06, 1.000e+00], dtype=float16), array([2.921e-06, 3.076e-05, 1.000e+00], dtype=float16), array([4.77e-07, 1.97e-06, 1.00e+00], dtype=float16), array([8.833e-05, 5.970e-03, 9.941e-01], dtype=float16), array([1.9014e-05, 1.6775e-03, 9.9854e-01], dtype=float16), array([1.049e-05, 1.334e-04, 1.000e+00], dtype=float16), array([1.81e-05, 6.04e-03, 9.94e-01], dtype=float16), array([3.46e-06, 4.23e-06, 1.00e+00], dtype=float16), array([7.927e-06, 5.215e-05, 1.000e+00], dtype=float16), array([1.055e-05, 1.162e-05, 1.000e+00], dtype=float16), array([9.418e-06, 5.589e-04, 9.995e-01], dtype=float16), array([1.812e-05, 2.319e-04, 9.995e-01], dtype=float16), array([3.999e-05, 1.342e-02, 9.863e-01], dtype=float16), array([1.043e-05, 1.222e-05, 1.000e+00], dtype=float16), array([2.32e-06, 5.07e-06, 1.00e+00], dtype=float16), array([5.543e-06, 2.242e-04, 1.000e+00], dtype=float16), array([1.957e-04, 1.099e-02, 9.888e-01], dtype=float16), array([3.099e-06, 3.800e-04, 9.995e-01], dtype=float16), array([5.662e-06, 4.742e-04, 9.995e-01], dtype=float16), array([7.004e-05, 2.200e-01, 7.798e-01], dtype=float16), array([3.636e-06, 1.644e-04, 1.000e+00], dtype=float16), array([1.044e-04, 3.726e-02, 9.624e-01], dtype=float16), array([3.58e-07, 1.43e-06, 1.00e+00], dtype=float16), array([6.294e-05, 1.019e-03, 9.990e-01], dtype=float16), array([4.659e-04, 1.011e-01, 8.984e-01], dtype=float16), array([5.96e-07, 8.46e-06, 1.00e+00], dtype=float16), array([4.512e-05, 4.004e-02, 9.600e-01], dtype=float16), array([1.746e-05, 5.989e-04, 9.995e-01], dtype=float16), array([3.576e-06, 2.866e-04, 9.995e-01], dtype=float16), array([1.827e-04, 2.098e-02, 9.790e-01], dtype=float16), array([1.848e-06, 2.366e-05, 1.000e+00], dtype=float16), array([2.575e-05, 1.700e-03, 9.980e-01], dtype=float16), array([2.015e-04, 6.121e-05, 9.995e-01], dtype=float16), array([2.8e-06, 2.2e-06, 1.0e+00], dtype=float16), array([4.2e-07, 5.3e-06, 1.0e+00], dtype=float16), array([0.001092, 0.1758  , 0.823   ], dtype=float16), array([3.133e-04, 5.186e-01, 4.812e-01], dtype=float16), array([8.40e-06, 4.17e-05, 1.00e+00], dtype=float16), array([3.5e-06, 4.5e-06, 1.0e+00], dtype=float16), array([8.73e-05, 5.73e-05, 1.00e+00], dtype=float16), array([2.86e-06, 2.12e-05, 1.00e+00], dtype=float16), array([1.4e-06, 1.2e-06, 1.0e+00], dtype=float16), array([3.2520e-04, 1.4854e-02, 9.8486e-01], dtype=float16), array([6.74e-06, 4.65e-06, 1.00e+00], dtype=float16), array([2.682e-06, 2.664e-05, 1.000e+00], dtype=float16), array([5.513e-05, 1.431e-03, 9.985e-01], dtype=float16), array([0.001808, 0.07263 , 0.926   ], dtype=float16), array([2.956e-04, 2.243e-02, 9.771e-01], dtype=float16), array([1.15e-05, 1.71e-05, 1.00e+00], dtype=float16), array([7.473e-03, 2.513e-04, 9.922e-01], dtype=float16), array([3.5143e-04, 1.0884e-04, 9.9951e-01], dtype=float16), array([8.88e-06, 2.44e-06, 1.00e+00], dtype=float16), array([6.658e-05, 1.234e-05, 1.000e+00], dtype=float16), array([9.120e-06, 2.223e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([5.07e-06, 4.53e-06, 1.00e+00], dtype=float16), array([2.699e-04, 4.060e-04, 9.995e-01], dtype=float16), array([2.449e-04, 1.365e-04, 9.995e-01], dtype=float16), array([2.377e-04, 2.520e-04, 9.995e-01], dtype=float16), array([2.6722e-03, 1.0943e-04, 9.9707e-01], dtype=float16), array([1.7369e-04, 1.2124e-04, 9.9951e-01], dtype=float16), array([1.10e-05, 2.92e-05, 1.00e+00], dtype=float16), array([2.7e-06, 5.0e-06, 1.0e+00], dtype=float16), array([1.544e-05, 4.101e-04, 9.995e-01], dtype=float16), array([2.680e-04, 1.240e-05, 9.995e-01], dtype=float16), array([7.927e-06, 1.144e-05, 1.000e+00], dtype=float16), array([1.359e-05, 2.415e-03, 9.976e-01], dtype=float16), array([4.737e-04, 3.988e-05, 9.995e-01], dtype=float16), array([4.935e-05, 9.112e-04, 9.990e-01], dtype=float16), array([4.17e-07, 6.81e-05, 1.00e+00], dtype=float16), array([3.517e-06, 2.533e-05, 1.000e+00], dtype=float16), array([5.059e-04, 8.282e-02, 9.165e-01], dtype=float16), array([1.638e-04, 4.935e-05, 1.000e+00], dtype=float16), array([6.0e-08, 1.7e-06, 1.0e+00], dtype=float16), array([4.387e-05, 2.395e-01, 7.603e-01], dtype=float16), array([1.061e-05, 3.307e-03, 9.966e-01], dtype=float16), array([2.980e-06, 1.645e-05, 1.000e+00], dtype=float16), array([0.002659, 0.003115, 0.994   ], dtype=float16), array([2.503e-06, 1.305e-04, 1.000e+00], dtype=float16), array([3.034e-05, 1.523e-04, 1.000e+00], dtype=float16), array([7.749e-07, 1.895e-05, 1.000e+00], dtype=float16), array([9.865e-05, 8.759e-03, 9.912e-01], dtype=float16), array([8.917e-05, 7.839e-04, 9.990e-01], dtype=float16), array([2.509e-05, 6.294e-03, 9.937e-01], dtype=float16), array([2.950e-05, 4.181e-03, 9.956e-01], dtype=float16), array([2.432e-05, 1.174e-05, 1.000e+00], dtype=float16), array([2.1e-06, 1.7e-06, 1.0e+00], dtype=float16), array([3.254e-05, 9.280e-05, 1.000e+00], dtype=float16), array([2.601e-04, 1.210e-05, 9.995e-01], dtype=float16), array([9.835e-06, 1.306e-03, 9.985e-01], dtype=float16), array([2.933e-05, 2.115e-04, 1.000e+00], dtype=float16), array([6.914e-06, 4.897e-04, 9.995e-01], dtype=float16), array([8.345e-07, 2.897e-05, 1.000e+00], dtype=float16), array([5.99e-05, 8.33e-05, 1.00e+00], dtype=float16), array([5.4e-07, 8.9e-07, 1.0e+00], dtype=float16), array([2.4e-07, 2.4e-07, 1.0e+00], dtype=float16), array([1.25e-06, 5.96e-07, 1.00e+00], dtype=float16), array([7.749e-06, 1.234e-04, 1.000e+00], dtype=float16), array([2.50e-06, 4.14e-05, 1.00e+00], dtype=float16), array([1.384e-04, 4.148e-05, 1.000e+00], dtype=float16), array([4.4e-06, 8.3e-05, 1.0e+00], dtype=float16), array([1.290e-04, 6.134e-03, 9.937e-01], dtype=float16), array([2.55e-05, 6.30e-05, 1.00e+00], dtype=float16), array([1.e-07, 5.e-07, 1.e+00], dtype=float16), array([2.038e-05, 1.733e-04, 1.000e+00], dtype=float16), array([8.881e-06, 1.967e-05, 1.000e+00], dtype=float16), array([8.404e-06, 2.164e-05, 1.000e+00], dtype=float16), array([1.758e-05, 4.756e-04, 9.995e-01], dtype=float16), array([5.370e-05, 7.284e-05, 1.000e+00], dtype=float16), array([2.733e-02, 2.354e-05, 9.727e-01], dtype=float16), array([3.976e-05, 7.689e-05, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.006e-04, 2.390e-05, 1.000e+00], dtype=float16), array([1.46e-05, 7.20e-05, 1.00e+00], dtype=float16), array([1.82e-05, 6.00e-05, 1.00e+00], dtype=float16), array([1.676e-04, 1.007e-04, 9.995e-01], dtype=float16), array([3.10e-06, 6.14e-06, 1.00e+00], dtype=float16), array([5.4e-07, 5.2e-06, 1.0e+00], dtype=float16), array([7.2479e-05, 1.2215e-02, 9.8779e-01], dtype=float16), array([6.62e-06, 4.27e-05, 1.00e+00], dtype=float16), array([3.93e-05, 8.81e-04, 9.99e-01], dtype=float16), array([1.544e-05, 8.936e-04, 9.990e-01], dtype=float16), array([7.153e-07, 3.034e-05, 1.000e+00], dtype=float16), array([4.47e-06, 6.29e-05, 1.00e+00], dtype=float16), array([1.425e-04, 1.242e-02, 9.873e-01], dtype=float16), array([7.95e-05, 7.06e-05, 1.00e+00], dtype=float16), array([6.497e-06, 4.125e-05, 1.000e+00], dtype=float16), array([1.8e-07, 2.9e-06, 1.0e+00], dtype=float16), array([5.4e-07, 7.7e-07, 1.0e+00], dtype=float16), array([1.192e-07, 2.927e-05, 1.000e+00], dtype=float16), array([3.64e-06, 2.44e-06, 1.00e+00], dtype=float16), array([1.389e-05, 5.203e-05, 1.000e+00], dtype=float16), array([5.484e-06, 1.633e-05, 1.000e+00], dtype=float16), array([8.17e-06, 1.50e-05, 1.00e+00], dtype=float16), array([8.500e-05, 9.185e-05, 1.000e+00], dtype=float16), array([3.87e-06, 6.76e-05, 1.00e+00], dtype=float16), array([4.125e-04, 1.752e-05, 9.995e-01], dtype=float16), array([1.063e-04, 5.024e-03, 9.946e-01], dtype=float16), array([6.8e-06, 8.0e-06, 1.0e+00], dtype=float16), array([1.944e-04, 2.307e-05, 1.000e+00], dtype=float16), array([7.92e-05, 1.55e-05, 1.00e+00], dtype=float16), array([4.89e-06, 1.57e-05, 1.00e+00], dtype=float16), array([1.562e-05, 1.188e-03, 9.990e-01], dtype=float16), array([1.303e-04, 2.688e-05, 1.000e+00], dtype=float16), array([1.90e-05, 4.35e-06, 1.00e+00], dtype=float16), array([1.82e-05, 1.35e-05, 1.00e+00], dtype=float16), array([1.1e-06, 4.8e-07, 1.0e+00], dtype=float16), array([5.716e-05, 3.129e-05, 1.000e+00], dtype=float16), array([1.284e-04, 1.174e-05, 1.000e+00], dtype=float16), array([2.551e-05, 1.536e-03, 9.985e-01], dtype=float16), array([1.550e-06, 1.285e-03, 9.985e-01], dtype=float16), array([3.16e-06, 2.24e-05, 1.00e+00], dtype=float16), array([7.27e-06, 2.56e-06, 1.00e+00], dtype=float16), array([2.229e-05, 7.835e-03, 9.922e-01], dtype=float16), array([3.72e-05, 2.54e-05, 1.00e+00], dtype=float16), array([0.002253, 0.01287 , 0.985   ], dtype=float16), array([2.044e-05, 2.348e-05, 1.000e+00], dtype=float16), array([1.907e-04, 2.226e-03, 9.976e-01], dtype=float16), array([1.43e-06, 7.87e-06, 1.00e+00], dtype=float16), array([1.15e-05, 9.52e-05, 1.00e+00], dtype=float16), array([1.43e-06, 3.16e-06, 1.00e+00], dtype=float16), array([7.7e-06, 2.1e-06, 1.0e+00], dtype=float16), array([1.186e-05, 3.082e-05, 1.000e+00], dtype=float16), array([1.723e-05, 1.166e-02, 9.883e-01], dtype=float16), array([2.72e-05, 4.41e-05, 1.00e+00], dtype=float16), array([8.3447e-06, 1.1367e-04, 1.0000e+00], dtype=float16), array([6.258e-06, 2.724e-05, 1.000e+00], dtype=float16), array([4.15e-05, 9.64e-03, 9.90e-01], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  7
---------------------------------------------------
Extracting Features for Prompt  8  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.438e-05, 3.166e-04, 9.995e-01], dtype=float16), array([1.781e-04, 4.590e-02, 9.541e-01], dtype=float16), array([3.356e-05, 6.094e-04, 9.995e-01], dtype=float16), array([1.0467e-04, 1.2360e-02, 9.8730e-01], dtype=float16), array([0.006763, 0.2668  , 0.7266  ], dtype=float16), array([1.981e-04, 1.149e-03, 9.985e-01], dtype=float16), array([0.002857, 0.1683  , 0.8286  ], dtype=float16), array([1.664e-04, 7.538e-03, 9.922e-01], dtype=float16), array([5.078e-05, 6.967e-04, 9.990e-01], dtype=float16), array([1.720e-04, 9.233e-01, 7.666e-02], dtype=float16), array([1.931e-05, 4.451e-04, 9.995e-01], dtype=float16), array([0.1108  , 0.008835, 0.8804  ], dtype=float16), array([9.853e-05, 4.280e-04, 9.995e-01], dtype=float16), array([0.001113, 0.02962 , 0.969   ], dtype=float16), array([3.822e-04, 3.647e-02, 9.634e-01], dtype=float16), array([0.01631, 0.932  , 0.05154], dtype=float16), array([4.631e-05, 7.786e-03, 9.922e-01], dtype=float16), array([3.254e-05, 6.711e-05, 1.000e+00], dtype=float16), array([6.251e-04, 6.016e-03, 9.932e-01], dtype=float16), array([0.0012045, 0.3716   , 0.627    ], dtype=float16), array([1.314e-04, 2.142e-03, 9.976e-01], dtype=float16), array([3.9983e-04, 1.1215e-02, 9.8828e-01], dtype=float16), array([1.730e-04, 1.125e-01, 8.872e-01], dtype=float16), array([2.311e-04, 8.864e-04, 9.990e-01], dtype=float16), array([1.351e-04, 5.737e-01, 4.263e-01], dtype=float16), array([7.24e-04, 9.28e-01, 7.11e-02], dtype=float16), array([8.30e-04, 3.19e-03, 9.96e-01], dtype=float16), array([1.538e-04, 1.739e-04, 9.995e-01], dtype=float16), array([3.00e-05, 2.29e-05, 1.00e+00], dtype=float16), array([3.558e-05, 1.149e-01, 8.848e-01], dtype=float16), array([0.005306, 0.011024, 0.984   ], dtype=float16), array([3.390e-04, 5.493e-01, 4.502e-01], dtype=float16), array([2.061e-04, 3.433e-01, 6.567e-01], dtype=float16), array([1.647e-03, 9.151e-04, 9.976e-01], dtype=float16), array([0.01125, 0.3748 , 0.6143 ], dtype=float16), array([0.001171, 0.2236  , 0.7754  ], dtype=float16), array([0.001641, 0.3862  , 0.6123  ], dtype=float16), array([1.855e-04, 2.325e-03, 9.976e-01], dtype=float16), array([1.045e-04, 2.806e-04, 9.995e-01], dtype=float16), array([1.43e-05, 3.16e-06, 1.00e+00], dtype=float16), array([5.7507e-04, 1.1505e-02, 9.8779e-01], dtype=float16), array([0.00695 , 0.005116, 0.988   ], dtype=float16), array([0.001432, 0.8984  , 0.1003  ], dtype=float16), array([2.161e-04, 8.202e-05, 9.995e-01], dtype=float16), array([0.002537, 0.6777  , 0.3196  ], dtype=float16), array([5.159e-04, 2.562e-03, 9.971e-01], dtype=float16), array([2.537e-04, 5.573e-03, 9.941e-01], dtype=float16), array([5.765e-04, 4.486e-02, 9.546e-01], dtype=float16), array([0.002214, 0.4165  , 0.581   ], dtype=float16), array([1.386e-04, 9.022e-04, 9.990e-01], dtype=float16), array([0.001512, 0.06506 , 0.9336  ], dtype=float16), array([2.513e-04, 3.044e-02, 9.692e-01], dtype=float16), array([5.9223e-04, 1.3306e-02, 9.8633e-01], dtype=float16), array([4.929e-05, 1.285e-04, 1.000e+00], dtype=float16), array([3.147e-05, 5.205e-01, 4.795e-01], dtype=float16), array([4.1628e-04, 1.4206e-02, 9.8535e-01], dtype=float16), array([0.001104, 0.006878, 0.992   ], dtype=float16), array([4.834e-05, 7.540e-05, 1.000e+00], dtype=float16), array([0.001603, 0.3813  , 0.617   ], dtype=float16), array([7.968e-04, 1.282e-02, 9.863e-01], dtype=float16), array([7.1e-06, 3.5e-05, 1.0e+00], dtype=float16), array([4.144e-04, 1.246e-03, 9.985e-01], dtype=float16), array([8.736e-04, 1.246e-02, 9.868e-01], dtype=float16), array([3.403e-05, 8.018e-03, 9.922e-01], dtype=float16), array([1.426e-04, 9.995e-01, 2.034e-04], dtype=float16), array([1.937e-04, 4.150e-03, 9.956e-01], dtype=float16), array([1.427e-04, 8.713e-03, 9.912e-01], dtype=float16), array([6.515e-05, 1.793e-02, 9.819e-01], dtype=float16), array([4.888e-06, 2.115e-04, 1.000e+00], dtype=float16), array([5.287e-05, 3.808e-04, 9.995e-01], dtype=float16), array([1.0455e-04, 5.2881e-01, 4.7095e-01], dtype=float16), array([1.211e-04, 6.828e-03, 9.932e-01], dtype=float16), array([0.002193, 0.5854  , 0.4124  ], dtype=float16), array([4.309e-05, 5.447e-03, 9.946e-01], dtype=float16), array([5.593e-04, 1.869e-04, 9.990e-01], dtype=float16), array([0.002108, 0.01707 , 0.981   ], dtype=float16), array([0.00485, 0.00961, 0.9854 ], dtype=float16), array([1.42e-04, 6.00e-04, 9.99e-01], dtype=float16), array([4.017e-05, 1.680e-02, 9.834e-01], dtype=float16), array([0.004475, 0.2676  , 0.728   ], dtype=float16), array([4.828e-06, 2.990e-04, 9.995e-01], dtype=float16), array([1.341e-05, 3.448e-04, 9.995e-01], dtype=float16), array([5.722e-05, 2.964e-03, 9.971e-01], dtype=float16), array([5.668e-05, 1.033e-04, 1.000e+00], dtype=float16), array([5.066e-06, 4.611e-04, 9.995e-01], dtype=float16), array([0.05  , 0.1128, 0.8374], dtype=float16), array([3.123e-05, 2.809e-04, 9.995e-01], dtype=float16), array([4.35e-06, 7.14e-05, 1.00e+00], dtype=float16), array([1.580e-03, 1.190e-04, 9.985e-01], dtype=float16), array([0.05414, 0.00828, 0.9375 ], dtype=float16), array([8.613e-05, 1.211e-02, 9.878e-01], dtype=float16), array([0.001244, 0.028   , 0.9707  ], dtype=float16), array([2.193e-04, 2.920e-03, 9.971e-01], dtype=float16), array([2.217e-05, 6.437e-04, 9.995e-01], dtype=float16), array([5.984e-05, 5.173e-03, 9.946e-01], dtype=float16), array([5.424e-06, 1.836e-05, 1.000e+00], dtype=float16), array([8.583e-06, 4.520e-04, 9.995e-01], dtype=float16), array([1.776e-05, 1.292e-02, 9.873e-01], dtype=float16), array([1.293e-05, 1.015e-04, 1.000e+00], dtype=float16), array([1.712e-04, 5.015e-01, 4.985e-01], dtype=float16), array([1.305e-04, 7.416e-02, 9.258e-01], dtype=float16), array([1.794e-05, 1.447e-03, 9.985e-01], dtype=float16), array([4.715e-05, 9.146e-01, 8.533e-02], dtype=float16), array([5.350e-04, 2.951e-03, 9.966e-01], dtype=float16), array([3.648e-05, 4.326e-03, 9.956e-01], dtype=float16), array([1.631e-04, 1.613e-02, 9.839e-01], dtype=float16), array([7.620e-04, 2.263e-01, 7.729e-01], dtype=float16), array([1.033e-04, 3.325e-01, 6.675e-01], dtype=float16), array([1.506e-04, 3.668e-02, 9.634e-01], dtype=float16), array([4.619e-05, 1.317e-02, 9.868e-01], dtype=float16), array([2.592e-04, 3.344e-05, 9.995e-01], dtype=float16), array([1.67e-06, 8.64e-06, 1.00e+00], dtype=float16), array([7.877e-04, 1.814e-02, 9.810e-01], dtype=float16), array([6.115e-05, 1.849e-02, 9.814e-01], dtype=float16), array([2.146e-05, 4.468e-04, 9.995e-01], dtype=float16), array([4.65e-06, 4.25e-05, 1.00e+00], dtype=float16), array([0.00088, 0.12067, 0.8784 ], dtype=float16), array([9.280e-05, 3.729e-03, 9.961e-01], dtype=float16), array([1.514e-05, 3.636e-06, 1.000e+00], dtype=float16), array([2.545e-05, 9.756e-04, 9.990e-01], dtype=float16), array([2.4e-06, 8.3e-06, 1.0e+00], dtype=float16), array([5.311e-05, 2.472e-02, 9.751e-01], dtype=float16), array([1.938e-04, 1.510e-03, 9.985e-01], dtype=float16), array([3.81e-06, 2.37e-05, 1.00e+00], dtype=float16), array([4.274e-05, 4.485e-01, 5.518e-01], dtype=float16), array([0.001929, 0.1334  , 0.8647  ], dtype=float16), array([8.3e-07, 2.0e-06, 1.0e+00], dtype=float16), array([1.392e-04, 4.471e-03, 9.956e-01], dtype=float16), array([1.477e-04, 5.527e-02, 9.443e-01], dtype=float16), array([2.44e-06, 6.66e-05, 1.00e+00], dtype=float16), array([2.825e-04, 7.080e-01, 2.917e-01], dtype=float16), array([1.43e-06, 7.21e-06, 1.00e+00], dtype=float16), array([3.769e-04, 3.296e-02, 9.668e-01], dtype=float16), array([0.00316, 0.266  , 0.731  ], dtype=float16), array([4.95e-06, 1.43e-05, 1.00e+00], dtype=float16), array([9.954e-06, 5.360e-04, 9.995e-01], dtype=float16), array([1.359e-04, 1.685e-02, 9.829e-01], dtype=float16), array([3.600e-05, 1.562e-03, 9.985e-01], dtype=float16), array([2.395e-04, 3.156e-02, 9.683e-01], dtype=float16), array([1.644e-04, 2.121e-02, 9.785e-01], dtype=float16), array([3.946e-04, 1.307e-02, 9.863e-01], dtype=float16), array([0.00218, 0.00321, 0.9946 ], dtype=float16), array([6.139e-05, 7.065e-03, 9.927e-01], dtype=float16), array([6.694e-05, 7.935e-03, 9.922e-01], dtype=float16), array([4.578e-05, 1.839e-03, 9.980e-01], dtype=float16), array([9.030e-05, 6.821e-01, 3.179e-01], dtype=float16), array([4.982e-03, 7.790e-05, 9.951e-01], dtype=float16), array([7.194e-05, 3.702e-02, 9.629e-01], dtype=float16), array([2.658e-05, 3.114e-04, 9.995e-01], dtype=float16), array([1.541e-04, 6.464e-02, 9.351e-01], dtype=float16), array([8.845e-04, 5.734e-05, 9.990e-01], dtype=float16), array([0.52    , 0.001589, 0.4783  ], dtype=float16), array([0.001292 , 0.0014305, 0.997    ], dtype=float16), array([2.534e-04, 2.532e-01, 7.466e-01], dtype=float16), array([0.01044, 0.1016 , 0.8877 ], dtype=float16), array([6.014e-05, 5.305e-05, 1.000e+00], dtype=float16), array([0.001031, 0.01714 , 0.982   ], dtype=float16), array([0.00099, 0.06494, 0.934  ], dtype=float16), array([0.004826, 0.1271  , 0.868   ], dtype=float16), array([0.00127, 0.4097 , 0.589  ], dtype=float16), array([0.002722, 0.003546, 0.9937  ], dtype=float16), array([0.002546, 0.10504 , 0.8926  ], dtype=float16), array([2.923e-04, 5.368e-02, 9.458e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([2.618e-04, 9.384e-03, 9.902e-01], dtype=float16), array([5.6505e-04, 1.3405e-02, 9.8584e-01], dtype=float16), array([0.002352, 0.003086, 0.9946  ], dtype=float16), array([1.938e-04, 1.575e-03, 9.980e-01], dtype=float16), array([0.00623, 0.10815, 0.8857 ], dtype=float16), array([4.21e-04, 8.15e-02, 9.18e-01], dtype=float16), array([0.005657, 0.09686 , 0.8975  ], dtype=float16), array([4.797e-04, 2.608e-02, 9.736e-01], dtype=float16), array([5.895e-05, 7.977e-04, 9.990e-01], dtype=float16), array([4.871e-04, 7.473e-03, 9.922e-01], dtype=float16), array([3.877e-04, 1.638e-03, 9.980e-01], dtype=float16), array([2.354e-05, 2.699e-02, 9.731e-01], dtype=float16), array([3.901e-04, 1.359e-03, 9.980e-01], dtype=float16), array([0.003765, 0.004395, 0.9917  ], dtype=float16), array([1.838e-04, 1.732e-02, 9.824e-01], dtype=float16), array([1.316e-03, 7.296e-04, 9.980e-01], dtype=float16), array([0.02573, 0.527  , 0.4473 ], dtype=float16), array([1.215e-04, 5.412e-05, 1.000e+00], dtype=float16), array([5.865e-04, 1.269e-02, 9.868e-01], dtype=float16), array([4.65e-06, 5.48e-06, 1.00e+00], dtype=float16), array([4.792e-05, 5.093e-04, 9.995e-01], dtype=float16), array([7.672e-04, 1.179e-02, 9.873e-01], dtype=float16), array([5.54e-04, 7.48e-02, 9.25e-01], dtype=float16), array([2.253e-05, 8.267e-05, 1.000e+00], dtype=float16), array([2.362e-04, 3.772e-02, 9.619e-01], dtype=float16), array([2.265e-06, 1.642e-02, 9.834e-01], dtype=float16), array([0.003193, 0.003263, 0.9937  ], dtype=float16), array([0.001872, 0.01892 , 0.979   ], dtype=float16), array([3.958e-05, 5.951e-02, 9.404e-01], dtype=float16), array([8.821e-06, 2.429e-04, 9.995e-01], dtype=float16), array([0.001599, 0.671   , 0.3274  ], dtype=float16), array([2.037e-04, 1.993e-02, 9.800e-01], dtype=float16), array([0.000985, 0.01735 , 0.9814  ], dtype=float16), array([0.002142, 0.979   , 0.01877 ], dtype=float16), array([0.003246, 0.9307  , 0.06616 ], dtype=float16), array([0.00373, 0.01892, 0.9775 ], dtype=float16), array([0.002216, 0.006023, 0.9917  ], dtype=float16), array([4.66e-04, 7.59e-02, 9.24e-01], dtype=float16), array([1.242e-04, 7.288e-02, 9.268e-01], dtype=float16), array([2.486e-05, 1.294e-03, 9.985e-01], dtype=float16), array([8.363e-05, 2.191e-02, 9.780e-01], dtype=float16), array([9.567e-05, 7.963e-04, 9.990e-01], dtype=float16), array([4.725e-04, 2.469e-01, 7.524e-01], dtype=float16), array([0.002106, 0.0937  , 0.9043  ], dtype=float16), array([8.044e-04, 1.106e-02, 9.883e-01], dtype=float16), array([4.964e-04, 8.759e-02, 9.121e-01], dtype=float16), array([5.884e-04, 7.622e-03, 9.917e-01], dtype=float16), array([4.926e-04, 2.318e-02, 9.766e-01], dtype=float16), array([0.000523, 0.4973  , 0.502   ], dtype=float16), array([1.483e-04, 1.821e-03, 9.980e-01], dtype=float16), array([1.225e-04, 2.949e-03, 9.971e-01], dtype=float16), array([0.001283, 0.02222 , 0.9766  ], dtype=float16), array([3.0899e-04, 1.0315e-01, 8.9648e-01], dtype=float16), array([1.979e-04, 7.336e-02, 9.263e-01], dtype=float16), array([7.715e-04, 1.566e-02, 9.834e-01], dtype=float16), array([2.174e-04, 1.122e-01, 8.877e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.003824, 0.01178 , 0.9844  ], dtype=float16), array([2.887e-04, 9.814e-01, 1.831e-02], dtype=float16), array([3.102e-04, 2.009e-01, 7.988e-01], dtype=float16), array([0.005, 0.647, 0.348], dtype=float16), array([5.615e-05, 2.308e-03, 9.976e-01], dtype=float16), array([8.810e-05, 1.088e-03, 9.990e-01], dtype=float16), array([3.026e-04, 4.541e-02, 9.541e-01], dtype=float16), array([0.003986, 0.3298  , 0.666   ], dtype=float16), array([9.304e-05, 3.580e-02, 9.639e-01], dtype=float16), array([0.0054, 0.086 , 0.9087], dtype=float16), array([1.84e-04, 8.47e-02, 9.15e-01], dtype=float16), array([0.001039, 0.03049 , 0.9683  ], dtype=float16), array([2.218e-04, 3.397e-02, 9.658e-01], dtype=float16), array([1.126e-03, 3.159e-04, 9.985e-01], dtype=float16), array([6.735e-05, 3.169e-04, 9.995e-01], dtype=float16), array([6.142e-04, 1.189e-02, 9.873e-01], dtype=float16), array([9.394e-04, 1.056e-02, 9.883e-01], dtype=float16), array([1.874e-04, 8.034e-03, 9.917e-01], dtype=float16), array([1.61e-06, 1.51e-04, 1.00e+00], dtype=float16), array([1.442e-05, 7.243e-04, 9.990e-01], dtype=float16), array([2.682e-04, 9.058e-02, 9.092e-01], dtype=float16), array([0.03069, 0.03604, 0.933  ], dtype=float16), array([0.001615, 0.0852  , 0.913   ], dtype=float16), array([0.001072, 0.03348 , 0.9653  ], dtype=float16), array([8.082e-04, 1.648e-02, 9.829e-01], dtype=float16), array([6.061e-04, 3.056e-02, 9.688e-01], dtype=float16), array([1.228e-04, 8.364e-01, 1.637e-01], dtype=float16), array([4.487e-04, 1.334e-02, 9.863e-01], dtype=float16), array([0.06775, 0.2155 , 0.717  ], dtype=float16), array([3.409e-05, 8.736e-04, 9.990e-01], dtype=float16), array([3.219e-04, 2.213e-03, 9.976e-01], dtype=float16), array([2.029e-04, 5.923e-02, 9.404e-01], dtype=float16), array([0.002338, 0.001073, 0.9966  ], dtype=float16), array([6.4182e-04, 1.2215e-02, 9.8730e-01], dtype=float16), array([1.1104e-04, 6.0425e-03, 9.9365e-01], dtype=float16), array([5.474e-04, 9.796e-03, 9.897e-01], dtype=float16), array([0.005123, 0.1617  , 0.833   ], dtype=float16), array([8.047e-05, 2.153e-04, 9.995e-01], dtype=float16), array([2.50e-06, 3.88e-03, 9.96e-01], dtype=float16), array([3.772e-04, 1.552e-03, 9.980e-01], dtype=float16), array([7.415e-04, 1.298e-02, 9.863e-01], dtype=float16), array([0.0013485, 0.856    , 0.1428   ], dtype=float16), array([4.780e-05, 3.130e-03, 9.966e-01], dtype=float16), array([0.001709, 0.1536  , 0.8447  ], dtype=float16), array([7.801e-04, 1.296e-03, 9.980e-01], dtype=float16), array([1.833e-04, 2.264e-03, 9.976e-01], dtype=float16), array([9.761e-04, 1.271e-02, 9.863e-01], dtype=float16), array([3.284e-05, 2.829e-03, 9.971e-01], dtype=float16), array([2.539e-04, 1.215e-02, 9.878e-01], dtype=float16), array([5.2404e-04, 1.4084e-02, 9.8535e-01], dtype=float16), array([7.7e-06, 1.8e-07, 1.0e+00], dtype=float16), array([3.046e-05, 6.623e-04, 9.995e-01], dtype=float16), array([2.372e-04, 2.390e-03, 9.976e-01], dtype=float16), array([5.393e-04, 9.375e-02, 9.058e-01], dtype=float16), array([3.487e-05, 4.965e-05, 1.000e+00], dtype=float16), array([3.219e-04, 1.477e-01, 8.521e-01], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  8
---------------------------------------------------
Extracting Features for Prompt  9  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.727e-04, 3.201e-01, 6.797e-01], dtype=float16), array([4.95e-06, 8.90e-04, 9.99e-01], dtype=float16), array([9.477e-06, 1.802e-02, 9.819e-01], dtype=float16), array([1.431e-04, 3.134e-02, 9.688e-01], dtype=float16), array([0.000827, 0.21    , 0.789   ], dtype=float16), array([3.726e-04, 2.333e-01, 7.666e-01], dtype=float16), array([1.419e-04, 2.963e-02, 9.702e-01], dtype=float16), array([1.0103e-04, 1.0475e-02, 9.8926e-01], dtype=float16), array([2.977e-03, 2.016e-04, 9.966e-01], dtype=float16), array([5.57e-05, 2.14e-01, 7.86e-01], dtype=float16), array([1.878e-05, 3.145e-04, 9.995e-01], dtype=float16), array([0.0104, 0.0851, 0.9043], dtype=float16), array([1.550e-05, 3.013e-01, 6.987e-01], dtype=float16), array([6.557e-05, 3.086e-01, 6.914e-01], dtype=float16), array([4.649e-05, 1.093e-02, 9.893e-01], dtype=float16), array([8.22e-05, 3.49e-02, 9.65e-01], dtype=float16), array([0.001511, 0.11176 , 0.8867  ], dtype=float16), array([1.46e-05, 3.94e-03, 9.96e-01], dtype=float16), array([3.409e-05, 2.116e-02, 9.790e-01], dtype=float16), array([3.924e-04, 3.870e-01, 6.128e-01], dtype=float16), array([2.515e-05, 4.935e-04, 9.995e-01], dtype=float16), array([4.489e-04, 3.537e-02, 9.644e-01], dtype=float16), array([5.960e-05, 2.396e-02, 9.761e-01], dtype=float16), array([2.474e-05, 8.683e-04, 9.990e-01], dtype=float16), array([8.523e-06, 1.257e-03, 9.985e-01], dtype=float16), array([2.747e-04, 8.240e-02, 9.175e-01], dtype=float16), array([1.502e-05, 2.785e-04, 9.995e-01], dtype=float16), array([8.053e-05, 4.684e-03, 9.951e-01], dtype=float16), array([3.797e-05, 3.604e-01, 6.396e-01], dtype=float16), array([2.3246e-05, 1.0195e-03, 9.9902e-01], dtype=float16), array([0.003035, 0.02855 , 0.9683  ], dtype=float16), array([0.0009265, 0.378    , 0.621    ], dtype=float16), array([1.007e-05, 7.378e-03, 9.927e-01], dtype=float16), array([0.012596, 0.7627  , 0.2246  ], dtype=float16), array([0.001128, 0.0778  , 0.921   ], dtype=float16), array([0.003298, 0.59    , 0.407   ], dtype=float16), array([4.35e-06, 5.96e-06, 1.00e+00], dtype=float16), array([2.302e-04, 5.123e-03, 9.946e-01], dtype=float16), array([3.070e-05, 1.270e-03, 9.985e-01], dtype=float16), array([1.510e-04, 3.568e-02, 9.644e-01], dtype=float16), array([9.358e-06, 7.263e-03, 9.927e-01], dtype=float16), array([3.588e-05, 1.975e-04, 1.000e+00], dtype=float16), array([3.994e-04, 7.290e-01, 2.703e-01], dtype=float16), array([3.946e-05, 3.018e-04, 9.995e-01], dtype=float16), array([0.001813, 0.02734 , 0.9707  ], dtype=float16), array([1.459e-04, 8.978e-02, 9.102e-01], dtype=float16), array([6.974e-06, 7.015e-05, 1.000e+00], dtype=float16), array([8.690e-05, 5.081e-03, 9.946e-01], dtype=float16), array([2.141e-04, 1.030e-01, 8.970e-01], dtype=float16), array([1.824e-05, 2.983e-01, 7.017e-01], dtype=float16), array([0.24  , 0.484 , 0.2761], dtype=float16), array([3.833e-05, 3.622e-02, 9.639e-01], dtype=float16), array([6.711e-05, 9.766e-01, 2.321e-02], dtype=float16), array([5.37e-04, 1.36e-03, 9.98e-01], dtype=float16), array([5.722e-06, 5.253e-03, 9.946e-01], dtype=float16), array([2.795e-05, 1.002e-03, 9.990e-01], dtype=float16), array([4.699e-04, 2.322e-04, 9.995e-01], dtype=float16), array([2.974e-05, 1.737e-04, 1.000e+00], dtype=float16), array([2.06e-03, 8.23e-04, 9.97e-01], dtype=float16), array([5.846e-04, 1.180e-01, 8.813e-01], dtype=float16), array([5.454e-05, 8.186e-03, 9.917e-01], dtype=float16), array([3.761e-05, 3.149e-02, 9.683e-01], dtype=float16), array([0.005333, 0.755   , 0.2396  ], dtype=float16), array([2.217e-04, 5.439e-01, 4.561e-01], dtype=float16), array([4.456e-04, 1.367e-01, 8.628e-01], dtype=float16), array([3.386e-05, 2.083e-04, 1.000e+00], dtype=float16), array([7.474e-05, 1.910e-01, 8.091e-01], dtype=float16), array([1.305e-05, 6.495e-04, 9.995e-01], dtype=float16), array([5.251e-05, 3.732e-02, 9.624e-01], dtype=float16), array([1.031e-04, 7.843e-03, 9.922e-01], dtype=float16), array([6.332e-04, 2.018e-01, 7.974e-01], dtype=float16), array([3.660e-05, 6.614e-04, 9.995e-01], dtype=float16), array([1.127e-05, 1.196e-03, 9.990e-01], dtype=float16), array([1.210e-04, 7.656e-03, 9.922e-01], dtype=float16), array([4.131e-05, 2.161e-04, 9.995e-01], dtype=float16), array([1.317e-05, 2.666e-03, 9.976e-01], dtype=float16), array([0.00317, 0.02965, 0.9673 ], dtype=float16), array([8.2552e-05, 1.4854e-02, 9.8486e-01], dtype=float16), array([3.844e-05, 2.136e-02, 9.785e-01], dtype=float16), array([8.125e-04, 2.650e-02, 9.727e-01], dtype=float16), array([2.311e-04, 1.269e-03, 9.985e-01], dtype=float16), array([7.20e-05, 4.95e-02, 9.50e-01], dtype=float16), array([3.793e-04, 1.744e-02, 9.824e-01], dtype=float16), array([2.742e-06, 2.266e-03, 9.976e-01], dtype=float16), array([6.735e-06, 3.527e-03, 9.966e-01], dtype=float16), array([0.001291, 0.1422  , 0.8564  ], dtype=float16), array([1.961e-04, 4.895e-02, 9.507e-01], dtype=float16), array([5.257e-05, 8.405e-02, 9.160e-01], dtype=float16), array([2.321e-04, 2.411e-02, 9.756e-01], dtype=float16), array([2.348e-04, 1.237e-02, 9.873e-01], dtype=float16), array([5.400e-05, 5.695e-03, 9.941e-01], dtype=float16), array([1.594e-04, 4.916e-02, 9.507e-01], dtype=float16), array([0.01776, 0.1113 , 0.871  ], dtype=float16), array([3.636e-06, 2.834e-03, 9.971e-01], dtype=float16), array([4.778e-04, 2.238e-02, 9.771e-01], dtype=float16), array([3.386e-05, 3.867e-04, 9.995e-01], dtype=float16), array([5.174e-04, 2.283e-02, 9.766e-01], dtype=float16), array([2.390e-05, 1.309e-03, 9.985e-01], dtype=float16), array([4.380e-04, 1.598e-02, 9.834e-01], dtype=float16), array([8.458e-05, 1.426e-02, 9.858e-01], dtype=float16), array([3.892e-05, 9.018e-03, 9.907e-01], dtype=float16), array([2.682e-04, 5.043e-03, 9.946e-01], dtype=float16), array([3.32e-05, 3.98e-03, 9.96e-01], dtype=float16), array([0.0879 , 0.8643 , 0.04785], dtype=float16), array([7.243e-04, 2.722e-03, 9.966e-01], dtype=float16), array([2.202e-04, 2.884e-03, 9.971e-01], dtype=float16), array([2.563e-05, 3.114e-04, 9.995e-01], dtype=float16), array([2.253e-05, 1.276e-03, 9.985e-01], dtype=float16), array([1.199e-04, 3.589e-02, 9.639e-01], dtype=float16), array([3.414e-04, 1.714e-02, 9.824e-01], dtype=float16), array([1.4663e-05, 1.0956e-02, 9.8926e-01], dtype=float16), array([1.065e-04, 6.676e-03, 9.932e-01], dtype=float16), array([2.637e-04, 9.506e-03, 9.902e-01], dtype=float16), array([6.789e-05, 1.112e-02, 9.888e-01], dtype=float16), array([5.82e-05, 4.37e-02, 9.56e-01], dtype=float16), array([2.003e-05, 4.642e-03, 9.951e-01], dtype=float16), array([4.113e-06, 1.276e-04, 1.000e+00], dtype=float16), array([1.9932e-04, 1.0443e-01, 8.9551e-01], dtype=float16), array([3.698e-03, 2.961e-04, 9.961e-01], dtype=float16), array([2.819e-05, 2.771e-03, 9.971e-01], dtype=float16), array([5.65e-05, 7.09e-02, 9.29e-01], dtype=float16), array([5.364e-04, 1.836e-02, 9.810e-01], dtype=float16), array([1.353e-05, 5.653e-03, 9.941e-01], dtype=float16), array([0.2266, 0.6763, 0.0972], dtype=float16), array([1.340e-04, 3.754e-02, 9.624e-01], dtype=float16), array([9.384e-04, 2.682e-02, 9.722e-01], dtype=float16), array([4.309e-05, 1.488e-03, 9.985e-01], dtype=float16), array([1.0175e-04, 1.3123e-02, 9.8682e-01], dtype=float16), array([1.501e-04, 2.264e-02, 9.771e-01], dtype=float16), array([6.026e-05, 1.450e-03, 9.985e-01], dtype=float16), array([6.026e-05, 1.003e-01, 8.999e-01], dtype=float16), array([1.866e-05, 1.889e-02, 9.810e-01], dtype=float16), array([2.545e-05, 2.629e-05, 1.000e+00], dtype=float16), array([0.001471, 0.1519  , 0.8467  ], dtype=float16), array([7.1406e-05, 1.0186e-04, 1.0000e+00], dtype=float16), array([0.004032, 0.2037  , 0.7925  ], dtype=float16), array([7.927e-06, 3.157e-03, 9.971e-01], dtype=float16), array([2.187e-05, 8.234e-02, 9.175e-01], dtype=float16), array([5.331e-04, 3.237e-01, 6.758e-01], dtype=float16), array([2.4557e-05, 1.3084e-02, 9.8682e-01], dtype=float16), array([9.835e-05, 3.302e-02, 9.668e-01], dtype=float16), array([3.928e-05, 7.309e-03, 9.927e-01], dtype=float16), array([1.682e-04, 4.663e-02, 9.531e-01], dtype=float16), array([2.229e-04, 1.775e-02, 9.819e-01], dtype=float16), array([5.92e-05, 8.57e-03, 9.91e-01], dtype=float16), array([9.042e-05, 2.188e-03, 9.976e-01], dtype=float16), array([1.127e-05, 3.057e-04, 9.995e-01], dtype=float16), array([1.931e-05, 6.916e-03, 9.932e-01], dtype=float16), array([5.68e-05, 1.11e-05, 1.00e+00], dtype=float16), array([8.881e-06, 1.007e-03, 9.990e-01], dtype=float16), array([5.412e-05, 7.713e-03, 9.922e-01], dtype=float16), array([0.006176, 0.01694 , 0.977   ], dtype=float16), array([6.396e-05, 1.417e-03, 9.985e-01], dtype=float16), array([3.481e-04, 1.546e-02, 9.844e-01], dtype=float16), array([0.01305, 0.919  , 0.06793], dtype=float16), array([0.04276, 0.05963, 0.8975 ], dtype=float16), array([3.598e-04, 4.983e-01, 5.015e-01], dtype=float16), array([1.239e-04, 3.125e-02, 9.688e-01], dtype=float16), array([0.00461, 0.01936, 0.976  ], dtype=float16), array([6.585e-04, 5.228e-02, 9.473e-01], dtype=float16), array([1.591e-05, 2.780e-04, 9.995e-01], dtype=float16), array([1.843e-04, 8.904e-03, 9.907e-01], dtype=float16), array([3.56e-04, 2.76e-02, 9.72e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.007e-05, 6.569e-03, 9.937e-01], dtype=float16), array([3.302e-05, 1.143e-03, 9.990e-01], dtype=float16), array([9.0480e-05, 1.3626e-02, 9.8633e-01], dtype=float16), array([0.001049, 0.1638  , 0.835   ], dtype=float16), array([1.363e-04, 4.596e-05, 1.000e+00], dtype=float16), array([0.04913, 0.779  , 0.1719 ], dtype=float16), array([4.201e-04, 7.428e-02, 9.253e-01], dtype=float16), array([4.315e-05, 4.683e-04, 9.995e-01], dtype=float16), array([3.994e-05, 1.353e-05, 1.000e+00], dtype=float16), array([4.663e-04, 1.258e-04, 9.995e-01], dtype=float16), array([1.311e-05, 7.420e-03, 9.927e-01], dtype=float16), array([3.81e-06, 1.29e-05, 1.00e+00], dtype=float16), array([6.070e-04, 1.552e-02, 9.839e-01], dtype=float16), array([2.962e-05, 1.740e-02, 9.824e-01], dtype=float16), array([9.489e-05, 1.169e-03, 9.985e-01], dtype=float16), array([0.00813 , 0.001703, 0.99    ], dtype=float16), array([5.293e-04, 6.976e-02, 9.297e-01], dtype=float16), array([2.596e-04, 4.193e-02, 9.580e-01], dtype=float16), array([2.044e-05, 4.959e-03, 9.951e-01], dtype=float16), array([4.023e-05, 3.237e-03, 9.966e-01], dtype=float16), array([8.9884e-05, 1.4084e-02, 9.8584e-01], dtype=float16), array([1.621e-04, 2.330e-02, 9.766e-01], dtype=float16), array([5.236e-04, 9.598e-03, 9.897e-01], dtype=float16), array([7.862e-05, 1.333e-02, 9.868e-01], dtype=float16), array([8.380e-05, 3.792e-03, 9.961e-01], dtype=float16), array([1.145e-04, 1.625e-02, 9.834e-01], dtype=float16), array([1.321e-04, 6.371e-03, 9.937e-01], dtype=float16), array([1.195e-04, 9.074e-04, 9.990e-01], dtype=float16), array([3.880e-05, 6.161e-04, 9.995e-01], dtype=float16), array([7.278e-05, 3.717e-02, 9.629e-01], dtype=float16), array([1.882e-04, 3.635e-03, 9.961e-01], dtype=float16), array([1.808e-04, 8.093e-02, 9.189e-01], dtype=float16), array([6.014e-05, 1.140e-02, 9.888e-01], dtype=float16), array([3.512e-04, 6.728e-04, 9.990e-01], dtype=float16), array([0.002367, 0.05234 , 0.9453  ], dtype=float16), array([4.106e-04, 3.394e-01, 6.602e-01], dtype=float16), array([4.79e-05, 7.99e-03, 9.92e-01], dtype=float16), array([5.066e-06, 3.441e-03, 9.966e-01], dtype=float16), array([2.060e-04, 3.008e-02, 9.697e-01], dtype=float16), array([8.58e-06, 6.54e-05, 1.00e+00], dtype=float16), array([3.296e-05, 3.479e-03, 9.966e-01], dtype=float16), array([2.494e-04, 1.305e-02, 9.868e-01], dtype=float16), array([1.16e-05, 9.79e-03, 9.90e-01], dtype=float16), array([6.509e-05, 5.759e-02, 9.424e-01], dtype=float16), array([1.358e-04, 1.567e-02, 9.844e-01], dtype=float16), array([1.260e-04, 1.764e-02, 9.824e-01], dtype=float16), array([1.0890e-04, 1.1566e-02, 9.8828e-01], dtype=float16), array([2.109e-04, 7.139e-01, 2.859e-01], dtype=float16), array([1.515e-04, 9.445e-03, 9.902e-01], dtype=float16), array([2.575e-04, 3.351e-02, 9.663e-01], dtype=float16), array([5.907e-05, 5.859e-03, 9.941e-01], dtype=float16), array([9.549e-05, 4.398e-03, 9.956e-01], dtype=float16), array([6.342e-05, 2.014e-03, 9.980e-01], dtype=float16), array([2.747e-04, 4.027e-04, 9.995e-01], dtype=float16), array([0.00561, 0.09827, 0.896  ], dtype=float16), array([1.013e-05, 2.027e-04, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.003986, 0.434   , 0.562   ], dtype=float16), array([4.178e-05, 4.349e-03, 9.956e-01], dtype=float16), array([3.002e-04, 5.132e-01, 4.866e-01], dtype=float16), array([6.229e-05, 3.920e-04, 9.995e-01], dtype=float16), array([5.162e-05, 5.012e-04, 9.995e-01], dtype=float16), array([4.859e-04, 3.065e-02, 9.688e-01], dtype=float16), array([1.246e-04, 1.384e-01, 8.613e-01], dtype=float16), array([1.085e-05, 8.230e-04, 9.990e-01], dtype=float16), array([6.640e-05, 2.735e-03, 9.971e-01], dtype=float16), array([2.897e-05, 3.918e-02, 9.609e-01], dtype=float16), array([2.325e-05, 1.444e-03, 9.985e-01], dtype=float16), array([1.345e-04, 3.677e-03, 9.961e-01], dtype=float16), array([1.167e-04, 3.195e-02, 9.678e-01], dtype=float16), array([9.661e-04, 1.348e-02, 9.854e-01], dtype=float16), array([0.001399, 0.2396  , 0.759   ], dtype=float16), array([1.047e-04, 3.628e-03, 9.961e-01], dtype=float16), array([1.247e-04, 1.291e-02, 9.868e-01], dtype=float16), array([1.701e-04, 8.716e-02, 9.126e-01], dtype=float16), array([5.24e-05, 7.71e-04, 9.99e-01], dtype=float16), array([1.565e-04, 1.624e-02, 9.834e-01], dtype=float16), array([0.1414, 0.1703, 0.6885], dtype=float16), array([0.001587, 0.5034  , 0.4949  ], dtype=float16), array([4.768e-06, 4.395e-03, 9.956e-01], dtype=float16), array([8.583e-06, 2.306e-04, 1.000e+00], dtype=float16), array([5.865e-04, 7.794e-02, 9.214e-01], dtype=float16), array([7.933e-05, 4.263e-04, 9.995e-01], dtype=float16), array([1.802e-04, 1.015e-01, 8.984e-01], dtype=float16), array([0.002348, 0.001758, 0.996   ], dtype=float16), array([0.002056, 0.256   , 0.7417  ], dtype=float16), array([1.937e-05, 6.542e-03, 9.937e-01], dtype=float16), array([6.7e-06, 6.2e-06, 1.0e+00], dtype=float16), array([0.00714, 0.01456, 0.9785 ], dtype=float16), array([0.02448, 0.08966, 0.8857 ], dtype=float16), array([5.227e-05, 3.157e-03, 9.966e-01], dtype=float16), array([1.836e-05, 6.378e-06, 1.000e+00], dtype=float16), array([6.330e-05, 5.577e-03, 9.941e-01], dtype=float16), array([1.1784e-04, 1.4473e-02, 9.8535e-01], dtype=float16), array([5.98e-05, 3.68e-02, 9.63e-01], dtype=float16), array([7.069e-05, 1.443e-02, 9.854e-01], dtype=float16), array([1.789e-04, 1.477e-02, 9.849e-01], dtype=float16), array([7.181e-04, 5.872e-02, 9.404e-01], dtype=float16), array([1.260e-04, 4.272e-03, 9.956e-01], dtype=float16), array([7.706e-04, 5.775e-03, 9.937e-01], dtype=float16), array([0.003983, 0.2382  , 0.758   ], dtype=float16), array([2.911e-04, 3.147e-01, 6.851e-01], dtype=float16), array([2.444e-05, 6.798e-03, 9.932e-01], dtype=float16), array([0.000994, 0.01608 , 0.983   ], dtype=float16), array([3.093e-05, 1.279e-03, 9.985e-01], dtype=float16), array([8.583e-06, 3.376e-04, 9.995e-01], dtype=float16), array([4.668e-04, 1.080e-02, 9.888e-01], dtype=float16), array([5.025e-05, 4.473e-04, 9.995e-01], dtype=float16), array([4.840e-04, 1.781e-01, 8.213e-01], dtype=float16), array([7.677e-05, 1.395e-02, 9.858e-01], dtype=float16), array([5.06e-04, 6.73e-04, 9.99e-01], dtype=float16), array([8.267e-05, 1.733e-02, 9.824e-01], dtype=float16), array([6.258e-05, 1.060e-02, 9.893e-01], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  9
---------------------------------------------------
Extracting Features for Prompt  10  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.39e-05, 2.25e-05, 1.00e+00], dtype=float16), array([4.113e-06, 1.240e-03, 9.985e-01], dtype=float16), array([3.368e-05, 1.783e-04, 1.000e+00], dtype=float16), array([3.904e-05, 2.157e-03, 9.980e-01], dtype=float16), array([4.05e-06, 6.20e-06, 1.00e+00], dtype=float16), array([9.39e-05, 8.33e-05, 1.00e+00], dtype=float16), array([3.674e-04, 6.409e-04, 9.990e-01], dtype=float16), array([2.183e-04, 2.669e-02, 9.731e-01], dtype=float16), array([3.61e-05, 1.05e-05, 1.00e+00], dtype=float16), array([4.655e-05, 5.154e-02, 9.482e-01], dtype=float16), array([4.148e-05, 2.642e-04, 9.995e-01], dtype=float16), array([0.001855, 0.445   , 0.5527  ], dtype=float16), array([1.687e-05, 9.163e-03, 9.907e-01], dtype=float16), array([2.867e-05, 2.631e-02, 9.736e-01], dtype=float16), array([2.6e-06, 8.5e-06, 1.0e+00], dtype=float16), array([3.04e-06, 9.54e-06, 1.00e+00], dtype=float16), array([8.833e-05, 5.345e-04, 9.995e-01], dtype=float16), array([2.03e-06, 5.25e-06, 1.00e+00], dtype=float16), array([1.086e-04, 6.451e-03, 9.937e-01], dtype=float16), array([0.01207, 0.01619, 0.9717 ], dtype=float16), array([2.4915e-05, 1.0544e-02, 9.8926e-01], dtype=float16), array([9.66e-06, 7.81e-06, 1.00e+00], dtype=float16), array([2.313e-05, 2.855e-02, 9.712e-01], dtype=float16), array([7.445e-05, 5.657e-03, 9.941e-01], dtype=float16), array([5.680e-05, 1.380e-02, 9.863e-01], dtype=float16), array([2.356e-04, 1.740e-03, 9.980e-01], dtype=float16), array([7.39e-06, 2.97e-05, 1.00e+00], dtype=float16), array([2.372e-05, 4.349e-03, 9.956e-01], dtype=float16), array([1.901e-04, 2.986e-02, 9.697e-01], dtype=float16), array([6.50e-06, 3.81e-05, 1.00e+00], dtype=float16), array([7.515e-04, 1.492e-03, 9.976e-01], dtype=float16), array([2.575e-04, 1.982e-02, 9.800e-01], dtype=float16), array([4.95e-06, 6.79e-06, 1.00e+00], dtype=float16), array([0.02234, 0.06256, 0.915  ], dtype=float16), array([0.001006, 0.1434  , 0.8555  ], dtype=float16), array([8.589e-05, 1.055e-02, 9.893e-01], dtype=float16), array([1.12e-05, 1.67e-05, 1.00e+00], dtype=float16), array([8.3876e-04, 1.4534e-02, 9.8486e-01], dtype=float16), array([2.283e-05, 7.767e-03, 9.922e-01], dtype=float16), array([5.3e-06, 3.1e-06, 1.0e+00], dtype=float16), array([2.56e-06, 5.01e-06, 1.00e+00], dtype=float16), array([2.694e-05, 9.418e-06, 1.000e+00], dtype=float16), array([1.175e-03, 1.304e-04, 9.985e-01], dtype=float16), array([1.013e-05, 2.891e-05, 1.000e+00], dtype=float16), array([2.186e-03, 2.286e-04, 9.976e-01], dtype=float16), array([1.995e-03, 4.189e-04, 9.976e-01], dtype=float16), array([3.737e-05, 2.248e-02, 9.775e-01], dtype=float16), array([9.54e-06, 8.95e-04, 9.99e-01], dtype=float16), array([6.1e-06, 8.0e-06, 1.0e+00], dtype=float16), array([4.351e-05, 6.940e-02, 9.307e-01], dtype=float16), array([0.091   , 0.005142, 0.904   ], dtype=float16), array([5.54e-06, 6.91e-06, 1.00e+00], dtype=float16), array([4.125e-05, 7.212e-06, 1.000e+00], dtype=float16), array([1.305e-05, 4.965e-05, 1.000e+00], dtype=float16), array([1.235e-04, 6.180e-02, 9.380e-01], dtype=float16), array([1.932e-04, 4.830e-04, 9.995e-01], dtype=float16), array([0.00982, 0.2114 , 0.779  ], dtype=float16), array([3.457e-05, 3.204e-03, 9.966e-01], dtype=float16), array([4.95e-05, 8.29e-06, 1.00e+00], dtype=float16), array([0.001883, 0.005848, 0.992   ], dtype=float16), array([1.675e-05, 9.446e-04, 9.990e-01], dtype=float16), array([9.36e-06, 2.07e-05, 1.00e+00], dtype=float16), array([5.782e-06, 5.233e-05, 1.000e+00], dtype=float16), array([2.15e-06, 5.78e-06, 1.00e+00], dtype=float16), array([0.002293, 0.0993  , 0.8984  ], dtype=float16), array([3.64e-06, 9.06e-06, 1.00e+00], dtype=float16), array([2.265e-05, 4.402e-03, 9.956e-01], dtype=float16), array([3.19e-05, 5.93e-03, 9.94e-01], dtype=float16), array([5.5e-06, 6.2e-06, 1.0e+00], dtype=float16), array([9.710e-05, 1.642e-03, 9.980e-01], dtype=float16), array([1.974e-04, 2.351e-01, 7.646e-01], dtype=float16), array([1.0794e-04, 1.2383e-02, 9.8730e-01], dtype=float16), array([4.816e-05, 3.019e-03, 9.971e-01], dtype=float16), array([8.166e-06, 2.909e-03, 9.971e-01], dtype=float16), array([1.0312e-04, 1.1375e-02, 9.8828e-01], dtype=float16), array([4.387e-05, 1.486e-02, 9.849e-01], dtype=float16), array([0.001438, 0.03427 , 0.9644  ], dtype=float16), array([2.2e-06, 5.2e-06, 1.0e+00], dtype=float16), array([2.503e-06, 3.076e-05, 1.000e+00], dtype=float16), array([1.997e-05, 1.812e-05, 1.000e+00], dtype=float16), array([1.081e-04, 7.336e-03, 9.927e-01], dtype=float16), array([4.423e-05, 1.934e-04, 1.000e+00], dtype=float16), array([4.17e-06, 6.56e-06, 1.00e+00], dtype=float16), array([3.541e-05, 9.680e-02, 9.033e-01], dtype=float16), array([5.2e-06, 8.2e-06, 1.0e+00], dtype=float16), array([9.382e-05, 2.002e-02, 9.800e-01], dtype=float16), array([0.00522, 0.1375 , 0.8574 ], dtype=float16), array([2.61e-05, 9.63e-03, 9.90e-01], dtype=float16), array([3.896e-04, 3.497e-02, 9.648e-01], dtype=float16), array([7.730e-04, 5.436e-04, 9.985e-01], dtype=float16), array([5.121e-04, 5.707e-02, 9.424e-01], dtype=float16), array([2.223e-05, 3.755e-06, 1.000e+00], dtype=float16), array([3.49e-05, 2.12e-05, 1.00e+00], dtype=float16), array([2.34e-05, 5.42e-06, 1.00e+00], dtype=float16), array([1.526e-05, 2.682e-06, 1.000e+00], dtype=float16), array([1.698e-04, 9.201e-03, 9.907e-01], dtype=float16), array([5.54e-06, 1.11e-05, 1.00e+00], dtype=float16), array([2.62e-06, 7.75e-06, 1.00e+00], dtype=float16), array([1.842e-05, 8.583e-04, 9.990e-01], dtype=float16), array([1.396e-04, 1.932e-03, 9.980e-01], dtype=float16), array([7.45e-06, 1.09e-05, 1.00e+00], dtype=float16), array([8.9e-06, 5.0e-06, 1.0e+00], dtype=float16), array([6.896e-05, 1.182e-02, 9.883e-01], dtype=float16), array([8.702e-06, 1.276e-05, 1.000e+00], dtype=float16), array([5.066e-06, 1.944e-02, 9.805e-01], dtype=float16), array([6.56e-06, 7.15e-07, 1.00e+00], dtype=float16), array([1.789e-04, 8.949e-03, 9.907e-01], dtype=float16), array([5.788e-05, 6.329e-02, 9.365e-01], dtype=float16), array([0.0012245, 0.01659  , 0.9824   ], dtype=float16), array([1.78e-04, 4.26e-02, 9.57e-01], dtype=float16), array([1.3387e-04, 1.1456e-01, 8.8525e-01], dtype=float16), array([1.496e-05, 1.576e-02, 9.844e-01], dtype=float16), array([0.002142, 0.00756 , 0.99    ], dtype=float16), array([4.905e-05, 3.971e-03, 9.961e-01], dtype=float16), array([1.124e-04, 3.590e-03, 9.961e-01], dtype=float16), array([2.21e-06, 1.35e-05, 1.00e+00], dtype=float16), array([6.914e-06, 4.387e-04, 9.995e-01], dtype=float16), array([1.384e-04, 3.314e-02, 9.668e-01], dtype=float16), array([2.364e-04, 2.775e-04, 9.995e-01], dtype=float16), array([1.02e-05, 2.22e-05, 1.00e+00], dtype=float16), array([1.687e-05, 4.601e-04, 9.995e-01], dtype=float16), array([1.061e-04, 1.023e-02, 9.897e-01], dtype=float16), array([1.389e-05, 7.385e-05, 1.000e+00], dtype=float16), array([2.09e-06, 1.56e-05, 1.00e+00], dtype=float16), array([1.681e-05, 1.007e-03, 9.990e-01], dtype=float16), array([3.9e-06, 2.4e-06, 1.0e+00], dtype=float16), array([3.475e-05, 1.377e-02, 9.863e-01], dtype=float16), array([2.378e-05, 2.634e-03, 9.976e-01], dtype=float16), array([2.4e-06, 3.7e-06, 1.0e+00], dtype=float16), array([8.821e-06, 4.539e-04, 9.995e-01], dtype=float16), array([1.286e-04, 1.200e-03, 9.985e-01], dtype=float16), array([1.943e-05, 4.023e-05, 1.000e+00], dtype=float16), array([1.0824e-04, 1.1081e-04, 1.0000e+00], dtype=float16), array([1.270e-05, 2.813e-05, 1.000e+00], dtype=float16), array([1.44e-05, 7.83e-03, 9.92e-01], dtype=float16), array([1.122e-04, 1.874e-02, 9.810e-01], dtype=float16), array([6.157e-05, 3.067e-02, 9.692e-01], dtype=float16), array([6.6e-07, 1.9e-06, 1.0e+00], dtype=float16), array([3.624e-05, 2.591e-02, 9.741e-01], dtype=float16), array([5.66e-06, 5.84e-06, 1.00e+00], dtype=float16), array([2.205e-05, 2.038e-05, 1.000e+00], dtype=float16), array([5.829e-05, 1.276e-05, 1.000e+00], dtype=float16), array([3.576e-06, 1.323e-05, 1.000e+00], dtype=float16), array([1.0675e-04, 1.2192e-02, 9.8779e-01], dtype=float16), array([2.08e-05, 2.05e-05, 1.00e+00], dtype=float16), array([5.478e-05, 5.347e-01, 4.653e-01], dtype=float16), array([8.887e-05, 1.768e-04, 9.995e-01], dtype=float16), array([5.4e-07, 1.6e-06, 1.0e+00], dtype=float16), array([5.92e-05, 6.45e-05, 1.00e+00], dtype=float16), array([1.106e-04, 6.491e-05, 1.000e+00], dtype=float16), array([3.36e-05, 2.72e-03, 9.97e-01], dtype=float16), array([7.09e-06, 1.97e-06, 1.00e+00], dtype=float16), array([4.289e-04, 3.250e-02, 9.673e-01], dtype=float16), array([8.446e-05, 4.150e-03, 9.956e-01], dtype=float16), array([5.96e-07, 1.97e-06, 1.00e+00], dtype=float16), array([1.31e-06, 4.05e-06, 1.00e+00], dtype=float16), array([1.067e-05, 1.217e-04, 1.000e+00], dtype=float16), array([1.131e-04, 3.522e-02, 9.648e-01], dtype=float16), array([7.564e-05, 1.121e-03, 9.990e-01], dtype=float16), array([2.613e-04, 2.281e-02, 9.771e-01], dtype=float16), array([8.225e-06, 6.361e-04, 9.995e-01], dtype=float16), array([0.00179, 0.05804, 0.94   ], dtype=float16), array([1.615e-05, 1.026e-03, 9.990e-01], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([1.264e-05, 1.311e-05, 1.000e+00], dtype=float16), array([1.79e-05, 8.83e-03, 9.91e-01], dtype=float16), array([6.646e-05, 1.261e-04, 1.000e+00], dtype=float16), array([2.134e-05, 1.496e-05, 1.000e+00], dtype=float16), array([2.78e-05, 3.16e-06, 1.00e+00], dtype=float16), array([0.03198, 0.1412 , 0.8267 ], dtype=float16), array([3.04e-05, 1.58e-05, 1.00e+00], dtype=float16), array([6.163e-05, 2.886e-03, 9.971e-01], dtype=float16), array([1.71e-05, 1.42e-05, 1.00e+00], dtype=float16), array([2.37e-05, 2.66e-05, 1.00e+00], dtype=float16), array([1.692e-04, 1.607e-02, 9.839e-01], dtype=float16), array([8.88e-06, 1.03e-05, 1.00e+00], dtype=float16), array([2.57e-05, 5.84e-06, 1.00e+00], dtype=float16), array([1.919e-05, 5.789e-04, 9.995e-01], dtype=float16), array([1.675e-05, 3.493e-05, 1.000e+00], dtype=float16), array([0.3252, 0.2128, 0.4622], dtype=float16), array([2.32e-06, 1.85e-06, 1.00e+00], dtype=float16), array([5.007e-05, 5.782e-05, 1.000e+00], dtype=float16), array([1.329e-05, 5.236e-04, 9.995e-01], dtype=float16), array([1.717e-05, 2.410e-04, 9.995e-01], dtype=float16), array([4.810e-05, 5.177e-03, 9.946e-01], dtype=float16), array([9.42e-06, 1.07e-05, 1.00e+00], dtype=float16), array([0.001738, 0.00954 , 0.989   ], dtype=float16), array([4.232e-05, 1.963e-04, 1.000e+00], dtype=float16), array([8.750e-05, 1.169e-02, 9.883e-01], dtype=float16), array([8.970e-05, 1.192e-02, 9.878e-01], dtype=float16), array([3.61e-05, 7.03e-06, 1.00e+00], dtype=float16), array([4.101e-05, 8.094e-05, 1.000e+00], dtype=float16), array([7.57e-06, 3.41e-05, 1.00e+00], dtype=float16), array([4.017e-05, 1.621e-03, 9.985e-01], dtype=float16), array([0.0635  , 0.003492, 0.933   ], dtype=float16), array([1.281e-05, 2.134e-05, 1.000e+00], dtype=float16), array([1.765e-04, 1.439e-02, 9.854e-01], dtype=float16), array([2.921e-04, 5.516e-03, 9.941e-01], dtype=float16), array([3.853e-03, 6.204e-04, 9.956e-01], dtype=float16), array([5.555e-05, 5.608e-03, 9.941e-01], dtype=float16), array([1.1206e-04, 2.8496e-03, 9.9707e-01], dtype=float16), array([1.371e-05, 1.672e-02, 9.834e-01], dtype=float16), array([7.09e-06, 3.78e-05, 1.00e+00], dtype=float16), array([6.485e-05, 6.281e-02, 9.370e-01], dtype=float16), array([4.59e-06, 1.93e-05, 1.00e+00], dtype=float16), array([2.849e-05, 3.321e-04, 9.995e-01], dtype=float16), array([2.080e-05, 8.976e-05, 1.000e+00], dtype=float16), array([1.299e-05, 4.274e-05, 1.000e+00], dtype=float16), array([0.001032, 0.03482 , 0.9644  ], dtype=float16), array([4.113e-05, 1.152e-03, 9.990e-01], dtype=float16), array([2.94e-04, 8.90e-04, 9.99e-01], dtype=float16), array([1.006e-04, 1.281e-02, 9.873e-01], dtype=float16), array([3.34e-06, 4.95e-06, 1.00e+00], dtype=float16), array([5.883e-05, 6.264e-03, 9.937e-01], dtype=float16), array([7.51e-06, 9.07e-05, 1.00e+00], dtype=float16), array([5.479e-04, 1.116e-02, 9.883e-01], dtype=float16), array([4.578e-04, 1.656e-02, 9.829e-01], dtype=float16), array([2.456e-04, 1.168e-05, 9.995e-01], dtype=float16), array([6.45e-05, 3.52e-06, 1.00e+00], dtype=float16), array([3.046e-05, 1.276e-04, 1.000e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.004433, 0.00305 , 0.9927  ], dtype=float16), array([3.040e-04, 8.034e-03, 9.917e-01], dtype=float16), array([4.83e-06, 2.53e-05, 1.00e+00], dtype=float16), array([3.713e-05, 1.643e-03, 9.985e-01], dtype=float16), array([2.652e-05, 1.665e-02, 9.834e-01], dtype=float16), array([2.855e-05, 2.815e-03, 9.971e-01], dtype=float16), array([2.205e-05, 1.591e-05, 1.000e+00], dtype=float16), array([3.93e-06, 1.96e-05, 1.00e+00], dtype=float16), array([1.13e-04, 8.19e-05, 1.00e+00], dtype=float16), array([4.77e-06, 5.84e-06, 1.00e+00], dtype=float16), array([1.025e-05, 4.011e-05, 1.000e+00], dtype=float16), array([1.502e-05, 2.232e-03, 9.976e-01], dtype=float16), array([1.2124e-04, 2.1362e-02, 9.7852e-01], dtype=float16), array([0.01622, 0.12006, 0.864  ], dtype=float16), array([5.978e-05, 6.287e-03, 9.937e-01], dtype=float16), array([6.56e-07, 2.72e-05, 1.00e+00], dtype=float16), array([9.418e-06, 1.717e-02, 9.829e-01], dtype=float16), array([3.976e-05, 1.540e-02, 9.844e-01], dtype=float16), array([1.21e-05, 9.66e-06, 1.00e+00], dtype=float16), array([1.198e-04, 4.105e-03, 9.956e-01], dtype=float16), array([0.709  , 0.1772 , 0.11365], dtype=float16), array([5.704e-05, 2.911e-02, 9.707e-01], dtype=float16), array([2.956e-05, 2.155e-02, 9.785e-01], dtype=float16), array([1.91e-06, 5.54e-06, 1.00e+00], dtype=float16), array([2.456e-05, 5.722e-06, 1.000e+00], dtype=float16), array([1.979e-05, 2.015e-05, 1.000e+00], dtype=float16), array([4.572e-05, 2.719e-02, 9.727e-01], dtype=float16), array([1.092e-04, 1.282e-03, 9.985e-01], dtype=float16), array([7.63e-05, 1.10e-05, 1.00e+00], dtype=float16), array([8.291e-05, 1.697e-02, 9.829e-01], dtype=float16), array([8.0e-06, 4.9e-06, 1.0e+00], dtype=float16), array([1.370e-04, 1.365e-05, 1.000e+00], dtype=float16), array([2.635e-04, 2.428e-02, 9.756e-01], dtype=float16), array([3.463e-05, 2.795e-02, 9.722e-01], dtype=float16), array([3.7e-06, 4.0e-06, 1.0e+00], dtype=float16), array([2.241e-05, 1.404e-02, 9.858e-01], dtype=float16), array([0.003016, 0.0403  , 0.9565  ], dtype=float16), array([5.093e-04, 1.918e-03, 9.976e-01], dtype=float16), array([1.13e-06, 1.85e-06, 1.00e+00], dtype=float16), array([1.335e-05, 8.643e-06, 1.000e+00], dtype=float16), array([2.664e-05, 1.812e-05, 1.000e+00], dtype=float16), array([5.704e-05, 8.523e-06, 1.000e+00], dtype=float16), array([5.393e-04, 7.347e-03, 9.922e-01], dtype=float16), array([5.865e-05, 1.663e-05, 1.000e+00], dtype=float16), array([1.39e-05, 5.99e-05, 1.00e+00], dtype=float16), array([3.016e-05, 1.784e-02, 9.819e-01], dtype=float16), array([5.42e-06, 4.77e-06, 1.00e+00], dtype=float16), array([0.001301, 0.0812  , 0.9175  ], dtype=float16), array([7.331e-06, 1.884e-05, 1.000e+00], dtype=float16), array([3.576e-05, 4.728e-04, 9.995e-01], dtype=float16), array([2.434e-04, 9.441e-05, 9.995e-01], dtype=float16), array([5.06e-05, 8.60e-04, 9.99e-01], dtype=float16), array([1.447e-04, 1.659e-02, 9.834e-01], dtype=float16), array([3.07e-05, 1.15e-05, 1.00e+00], dtype=float16), array([2.072e-04, 3.583e-02, 9.639e-01], dtype=float16), array([1.94e-05, 8.00e-03, 9.92e-01], dtype=float16)]
len X_train_features:  163
X_dev_features.shape:  56
X_test_features.shape:  56
Succefullly added prompt_features for prompt_number  10
---------------------------------------------------
Training based on Prompt 1
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
Best parameters found:
{'C': 7, 'coef0': 3.0, 'degree': 4, 'gamma': 1, 'kernel': 'poly'}
RMSE for train:  6.110643885867872
MAE for train:  4.644694719039683
RMSE for dev:  5.617791057754054
MAE for dev:  4.5438559124752675
RMSE for test:  6.947333050908202
MAE for test:  5.480862466263993
weights:  {'1': 0.22007740105809157}
Training based on Prompt 2
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
Best parameters found:
{'C': 5, 'coef0': 0.0, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}
RMSE for train:  5.784083362689997
MAE for train:  4.284178538979273
RMSE for dev:  5.585818651577064
MAE for dev:  4.368355374695672
RMSE for test:  6.3872094813975835
MAE for test:  4.973488989693793
weights:  {'1': 0.22007740105809157, '2': 0.2289191043825427}
Training based on Prompt 3
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
Best parameters found:
{'C': 2, 'coef0': 0.0, 'degree': 4, 'gamma': 1, 'kernel': 'poly'}
RMSE for train:  6.23458309942496
MAE for train:  4.831664420971718
RMSE for dev:  5.995316990174523
MAE for dev:  4.732019018038364
RMSE for test:  7.004952921169726
MAE for test:  5.623251018942349
weights:  {'1': 0.22007740105809157, '2': 0.2289191043825427, '3': 0.21132628507789583}
Training based on Prompt 4
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
Best parameters found:
{'C': 2, 'coef0': 1.0, 'degree': 4, 'gamma': 1, 'kernel': 'poly'}
RMSE for train:  6.243964338306926
MAE for train:  4.74919058712676
RMSE for dev:  6.1567844112998396
MAE for dev:  5.02607184107577
RMSE for test:  7.17299408782842
MAE for test:  5.602040513121159
weights:  {'1': 0.22007740105809157, '2': 0.2289191043825427, '3': 0.21132628507789583, '4': 0.19896253607587952}
Training based on Prompt 5
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
Best parameters found:
{'C': 10, 'coef0': 0.0, 'degree': 4, 'gamma': 1, 'kernel': 'poly'}
RMSE for train:  6.175138170812416
MAE for train:  4.691592582324188
RMSE for dev:  5.486039686183693
MAE for dev:  4.275114449698109
RMSE for test:  6.709955289372922
MAE for test:  5.440602873913773
weights:  {'1': 0.22007740105809157, '2': 0.2289191043825427, '3': 0.21132628507789583, '4': 0.19896253607587952, '5': 0.2339118663994167}
Training based on Prompt 6
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
Best parameters found:
{'C': 10, 'coef0': 2.0, 'degree': 4, 'gamma': 'scale', 'kernel': 'poly'}
RMSE for train:  6.083349869830273
MAE for train:  4.586603207024545
RMSE for dev:  5.936655943599403
MAE for dev:  4.643634284254399
RMSE for test:  7.271629445494482
MAE for test:  5.844754649105755
weights:  {'1': 0.22007740105809157, '2': 0.2289191043825427, '3': 0.21132628507789583, '4': 0.19896253607587952, '5': 0.2339118663994167, '6': 0.21534856941486383}
Training based on Prompt 7
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
Best parameters found:
{'C': 7, 'coef0': 1.0, 'degree': 4, 'gamma': 'scale', 'kernel': 'poly'}
RMSE for train:  6.344535118347179
MAE for train:  4.916876720518338
RMSE for dev:  5.937491919775415
MAE for dev:  4.748258318837548
RMSE for test:  7.144156258940945
MAE for test:  5.655965882676504
weights:  {'1': 0.22007740105809157, '2': 0.2289191043825427, '3': 0.21132628507789583, '4': 0.19896253607587952, '5': 0.2339118663994167, '6': 0.21534856941486383, '7': 0.21060353772934925}
Training based on Prompt 8
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
Best parameters found:
{'C': 0.01, 'coef0': 0.0, 'degree': 4, 'gamma': 0.001, 'kernel': 'poly'}
RMSE for train:  6.341812750078257
MAE for train:  4.938036809815949
RMSE for dev:  5.909072927045563
MAE for dev:  4.739285714285712
RMSE for test:  7.14462835182588
MAE for test:  5.657142857142852
weights:  {'1': 0.22007740105809157, '2': 0.2289191043825427, '3': 0.21132628507789583, '4': 0.19896253607587952, '5': 0.2339118663994167, '6': 0.21534856941486383, '7': 0.21060353772934925, '8': 0.211002260738508}
Training based on Prompt 9
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
Best parameters found:
{'C': 100, 'coef0': 3.0, 'degree': 4, 'gamma': 'scale', 'kernel': 'poly'}
RMSE for train:  6.032408249412983
MAE for train:  4.513785110092282
RMSE for dev:  6.029699664132987
MAE for dev:  4.759378236243662
RMSE for test:  7.27647469392027
MAE for test:  5.658937180548692
weights:  {'1': 0.22007740105809157, '2': 0.2289191043825427, '3': 0.21132628507789583, '4': 0.19896253607587952, '5': 0.2339118663994167, '6': 0.21534856941486383, '7': 0.21060353772934925, '8': 0.211002260738508, '9': 0.21011147892907325}
Training based on Prompt 10
train shape:  (163, 3)
dev shape:  (56, 3)
test shape:  (56, 3)
Best parameters found:
{'C': 7, 'coef0': 0.0, 'degree': 4, 'gamma': 1, 'kernel': 'poly'}
RMSE for train:  6.339285633395187
MAE for train:  4.927503245622215
RMSE for dev:  5.941967491389913
MAE for dev:  4.7734217967235555
RMSE for test:  7.188198285221015
MAE for test:  5.683588143211613
weights:  {'1': 0.22007740105809157, '2': 0.2289191043825427, '3': 0.21132628507789583, '4': 0.19896253607587952, '5': 0.2339118663994167, '6': 0.21534856941486383, '7': 0.21060353772934925, '8': 0.211002260738508, '9': 0.21011147892907325, '10': 0.2094933241990878}
prompt_number:  1
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  2
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  3
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  4
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  5
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  6
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  7
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  8
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  9
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
prompt_number:  10
weighted_prompt_features_train.shape:  (163, 3)
weighted_prompt_features_dev.shape:  (56, 3)
weighted_prompt_features_test.shape:  (56, 3)
average_array_train.shape:  (163, 3)
average_array_dev.shape:  (56, 3)
average_array_test.shape:  (56, 3)
final y_train shape:  (163,)
RMSE for train:  5.562387390048681
MAE for train:  4.263619181240362
RMSE for dev:  5.388682514157233
MAE for dev:  4.171710034501927
RMSE for test:  6.049120835470331
MAE for test:  4.918854608925654
y_dev:  [ 2  3  0  8 11 20  8  5  7  9 11 16 11 12  9  9  2 16  0 17  2  9 11  0
 12  3  0  6  2 10  8  0 18  6 18  4  1  1  2  4  0  6  7  1  9  4 12  3
  1  4  0  3  0  7 19  0]
y_pred_dev:  [ 2.18702285  2.43914088  2.68701665  6.17569991  5.66171772 10.53816871
  4.92256277  3.15793352  3.63561856  7.00670733  4.57920087  2.26517023
  4.4069475   2.09146583  6.45716124  8.31415686  8.39397172  8.49859033
  2.43832701  5.86827666  2.20221655  6.55322088  8.98533492  2.30706897
  2.87837674  4.454563    9.99062844  9.1086367   2.1590261   2.37306288
  8.81602827  2.26701005  6.8318993  11.74453915 12.34759085  3.27200909
  2.5363803   2.25782627  2.5882029   2.36820908  2.80655969  2.70445445
  6.20979682  2.45358418  2.80479406  2.62613553  2.48594268  8.75007559
  3.73789403  2.34917524  3.31093689  7.20567673  3.88782406  4.24670128
  8.44685749  5.82682606]
y_test:  [ 5 13 12  2  5  7  0  3  4  6  0  0  9 22  2  0  0  0  2  9 15 16 19 17
 19  3  5  7 12  8 11 16 19  9  2  5  7  0  8 12  6 19 13 11  3 13 16 20
  0 10  7  0  7 15  1  3]
y_pred_test:  [11.83578617 11.12755997  6.9554916   9.29118276  4.16388341  9.18627404
  6.50328596  4.60675168  2.12507096  5.56665972  3.3610703   2.11243884
  2.26301595  7.21530318  2.8209637   5.40639693  3.16199713  4.2605615
  4.90428679  2.12021616 11.2155186   8.65379365  6.29658878  5.64129436
  7.64795075  2.46657313  9.16003095  8.69620331  8.8208565   2.10647553
  5.4502554   9.16330479  9.51466552  4.90819825  2.06722005  2.15137816
  4.42307273  2.21479208  2.16028801  7.27355361  8.42009378 11.55487139
  2.95476121  8.21973823  5.05969254  4.8439465   3.78585155  7.80020556
  1.95606985  3.97376284  5.44103262  4.18523149  6.92782567  9.56022542
  5.62171594  6.11700202]
=== JOB_STATISTICS ===
=== current date     : Wed 14 Feb 2024 12:17:44 PM CET
= Job-ID             : 768599 on tinygpu
= Job-Name           : job_llama_fine_tuned_in_pipeline.sh
= Job-Command        : /home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline/job_llama_fine_tuned_in_pipeline.sh
= Initial workdir    : /home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline
= Queue/Partition    : a100
= Slurm account      : empk with QOS=normal
= Requested resources: cpu=32,mem=120000M,node=1,billing=32,gres/gpu=1,gres/gpu:a100=1 for 20:00:00
= Elapsed runtime    : 00:36:55
= Total RAM usage    : 1.3 GiB of requested 117 GiB (1.1%)   
= Node list          : tg093
= Subm/Elig/Start/End: 2024-02-14T11:40:48 / 2024-02-14T11:40:48 / 2024-02-14T11:40:49 / 2024-02-14T12:17:44
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           76.4G   104.9G   209.7G        N/A  45,090      500K   1,000K        N/A    
    /home/vault       1479.7G     0.0K     0.0K        N/A      74K     300K     450K        N/A    
    /home/woody        311.0G   500.0G   750.0G        N/A     314K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:01:00.0, 656341, 58 %, 26 %, 7226 MiB, 543325 ms
