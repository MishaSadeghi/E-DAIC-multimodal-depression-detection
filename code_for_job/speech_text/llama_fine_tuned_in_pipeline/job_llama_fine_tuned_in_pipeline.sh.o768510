### Starting TaskPrologue of job 768510 on tg092 at Wed 14 Feb 2024 09:49:36 AM CET
Running on cores 0-31 with governor ondemand
Wed Feb 14 09:49:36 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:01:00.0 Off |                    0 |
| N/A   40C    P0              59W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
### Finished TaskPrologue

/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline
def_dev:      id  ...                                               text
0  300  ...   which will record your body. So I'll show you...
1  301  ...   Yeah, there's all sorts of different studies ...
2  306  ...   Okay, looks like we're good. But let's move a...
3  317  ...   Okay. How long is this? This is probably goin...
4  320  ...   Okay, everything looks good. Okay. Perfect. O...

[5 rows x 7 columns]
max PHQ score in df_train:  23
prompt_number:  1
prompt:   Your task is to read the following text which is an interview with a person and to summarize the key points that might be related to the depression of the person. Be concise and to the point.
prompt_number:  2
prompt:   Your task is to read the following text which is an interview with a person and to summarize the key points that might be related to the depression of the person. Be concise and to the point. It is very essential that you write your answer in the first-person perspective, as if the interviewee is narrating about himself or herself. 
prompt_number:  3
prompt:   After reading the interview, briefly summarize the main aspects that pertain to the person's depression. 
prompt_number:  4
prompt:   Based on the interview, highlight the key factors that might be indicative of the interviewee's depression. 
prompt_number:  5
prompt:   Your task is to summarize the interviewee's main points that could be linked to their depression. Keep it concise. 
prompt_number:  6
prompt:   After reading the interview, identify and summarize the main challenges or difficulties the interviewee faces that are indicative of depression. 
prompt_number:  7
prompt:   Based on the interview, provide a concise analysis of the interviewee's emotional state and behaviors that may indicate the presence of depression. 
prompt_number:  8
prompt:   Read the interview carefully and extract the most significant indicators of depression exhibited by the interviewee. Summarize them concisely. 
prompt_number:  9
prompt:   Your task is to analyze the interviewee's responses and highlight the key signs or symptoms of depression that are evident in the interview. 
prompt_number:  10
prompt:   Provide a brief summary of the interview, focusing on aspects that strongly suggest the presence of depression in the interviewee. 
Extracting Features for Prompt  1  ..... 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 14.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.42s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-02-14 09:50:47.635397: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
output:  [array([2.07e-05, 8.78e-03, 9.91e-01], dtype=float16), array([7.510e-06, 6.037e-04, 9.995e-01], dtype=float16), array([2.530e-04, 1.011e-02, 9.897e-01], dtype=float16), array([4.363e-04, 5.903e-04, 9.990e-01], dtype=float16), array([1.440e-04, 5.745e-03, 9.941e-01], dtype=float16), array([3.877e-04, 6.366e-02, 9.360e-01], dtype=float16), array([0.00927 , 0.001953, 0.989   ], dtype=float16), array([2.384e-06, 1.271e-04, 1.000e+00], dtype=float16), array([0.001191, 0.02646 , 0.972   ], dtype=float16), array([2.01e-05, 2.05e-05, 1.00e+00], dtype=float16), array([1.686e-04, 8.644e-03, 9.912e-01], dtype=float16), array([6.554e-03, 4.107e-05, 9.932e-01], dtype=float16), array([5.260e-04, 6.494e-01, 3.499e-01], dtype=float16), array([2.549e-04, 4.951e-03, 9.946e-01], dtype=float16), array([7.117e-05, 6.577e-01, 3.425e-01], dtype=float16), array([6.889e-03, 1.519e-04, 9.932e-01], dtype=float16), array([3.200e-04, 1.932e-04, 9.995e-01], dtype=float16), array([1.550e-06, 2.682e-03, 9.976e-01], dtype=float16), array([2.739e-04, 1.636e-02, 9.834e-01], dtype=float16), array([3.860e-01, 5.202e-04, 6.133e-01], dtype=float16), array([1.156e-05, 3.332e-05, 1.000e+00], dtype=float16), array([0.00754, 0.2837 , 0.7085 ], dtype=float16), array([2.575e-04, 6.353e-01, 3.647e-01], dtype=float16), array([3.595e-04, 3.617e-04, 9.995e-01], dtype=float16), array([0.001156, 0.745   , 0.2537  ], dtype=float16), array([3.054e-04, 7.858e-03, 9.917e-01], dtype=float16), array([7.766e-05, 6.073e-03, 9.937e-01], dtype=float16), array([8.464e-06, 2.897e-05, 1.000e+00], dtype=float16), array([3.648e-04, 1.535e-02, 9.844e-01], dtype=float16), array([7.075e-05, 7.147e-02, 9.287e-01], dtype=float16), array([2.742e-04, 2.732e-04, 9.995e-01], dtype=float16), array([2.342e-05, 3.209e-04, 9.995e-01], dtype=float16), array([1.109e-04, 2.438e-02, 9.756e-01], dtype=float16), array([0.003641, 0.001235, 0.995   ], dtype=float16), array([0.00651, 0.4512 , 0.542  ], dtype=float16), array([0.10425 , 0.005215, 0.8906  ], dtype=float16), array([4.327e-05, 6.886e-04, 9.995e-01], dtype=float16), array([3.952e-05, 9.634e-01, 3.677e-02], dtype=float16), array([1.150e-05, 1.979e-04, 1.000e+00], dtype=float16), array([5.178e-04, 1.385e-01, 8.608e-01], dtype=float16), array([3.320e-05, 8.675e-03, 9.912e-01], dtype=float16), array([0.006775, 0.005413, 0.988   ], dtype=float16), array([0.0471, 0.361 , 0.592 ], dtype=float16), array([6.920e-05, 1.943e-05, 1.000e+00], dtype=float16), array([9.37e-04, 3.56e-05, 9.99e-01], dtype=float16), array([0.003 , 0.721 , 0.2756], dtype=float16), array([2.50e-06, 1.97e-06, 1.00e+00], dtype=float16), array([1.448e-05, 8.736e-04, 9.990e-01], dtype=float16), array([3.04e-06, 5.96e-08, 1.00e+00], dtype=float16), array([8.88e-06, 1.29e-05, 1.00e+00], dtype=float16), array([1.055e-05, 3.421e-05, 1.000e+00], dtype=float16), array([3.655e-04, 1.968e-03, 9.976e-01], dtype=float16), array([0.02951, 0.01866, 0.9517 ], dtype=float16), array([3.695e-05, 1.500e-02, 9.849e-01], dtype=float16), array([1.317e-04, 6.977e-03, 9.927e-01], dtype=float16), array([0.0021, 0.285 , 0.713 ], dtype=float16), array([0.002254, 0.0429  , 0.955   ], dtype=float16), array([5.112e-04, 4.122e-04, 9.990e-01], dtype=float16), array([0.002   , 0.001652, 0.9966  ], dtype=float16), array([0.00603, 0.02052, 0.9736 ], dtype=float16), array([6.020e-06, 2.635e-05, 1.000e+00], dtype=float16), array([0.00355, 0.0351 , 0.9614 ], dtype=float16), array([8.e-07, 2.e-07, 1.e+00], dtype=float16), array([1.94e-05, 1.93e-04, 1.00e+00], dtype=float16), array([0.002884, 0.0214  , 0.9756  ], dtype=float16), array([7.749e-06, 7.763e-04, 9.990e-01], dtype=float16), array([1.657e-05, 1.429e-01, 8.569e-01], dtype=float16), array([6.20e-05, 7.32e-04, 9.99e-01], dtype=float16), array([4.679e-05, 7.195e-04, 9.990e-01], dtype=float16), array([1.6e-05, 6.3e-06, 1.0e+00], dtype=float16), array([0.002316, 0.01102 , 0.987   ], dtype=float16), array([8.3e-07, 6.0e-08, 1.0e+00], dtype=float16), array([7.004e-05, 3.998e-03, 9.961e-01], dtype=float16), array([5.662e-05, 1.444e-03, 9.985e-01], dtype=float16), array([1.180e-04, 1.636e-03, 9.980e-01], dtype=float16), array([1.281e-05, 3.468e-03, 9.966e-01], dtype=float16), array([0.005325, 0.001263, 0.9937  ], dtype=float16), array([1.186e-05, 2.390e-04, 9.995e-01], dtype=float16), array([1.45e-05, 1.87e-05, 1.00e+00], dtype=float16), array([2.238e-04, 2.454e-01, 7.544e-01], dtype=float16), array([8.434e-05, 2.103e-02, 9.790e-01], dtype=float16), array([1.504e-04, 2.609e-02, 9.736e-01], dtype=float16), array([1.13e-04, 8.36e-05, 1.00e+00], dtype=float16), array([3.821e-05, 2.770e-04, 9.995e-01], dtype=float16), array([3.612e-05, 5.150e-03, 9.946e-01], dtype=float16), array([0.0013275, 0.8643   , 0.1346   ], dtype=float16), array([0.2332 , 0.04706, 0.7197 ], dtype=float16), array([1.456e-04, 8.173e-04, 9.990e-01], dtype=float16), array([0.3423 , 0.02925, 0.6284 ], dtype=float16), array([5.943e-05, 4.451e-04, 9.995e-01], dtype=float16), array([5.305e-06, 1.022e-03, 9.990e-01], dtype=float16), array([1.49e-06, 1.03e-05, 1.00e+00], dtype=float16), array([2.973e-04, 1.137e-02, 9.883e-01], dtype=float16), array([1.956e-04, 6.157e-03, 9.937e-01], dtype=float16), array([1.770e-05, 3.054e-04, 9.995e-01], dtype=float16), array([0.1417, 0.598 , 0.2603], dtype=float16), array([4.977e-05, 1.883e-03, 9.980e-01], dtype=float16), array([5.859e-05, 1.266e-01, 8.735e-01], dtype=float16), array([3.046e-05, 4.268e-04, 9.995e-01], dtype=float16), array([3.76e-06, 3.34e-06, 1.00e+00], dtype=float16), array([5.546e-04, 4.098e-04, 9.990e-01], dtype=float16), array([4.250e-05, 7.137e-03, 9.927e-01], dtype=float16), array([2.41e-05, 1.65e-05, 1.00e+00], dtype=float16), array([3.403e-05, 3.305e-03, 9.966e-01], dtype=float16), array([4.631e-05, 2.987e-03, 9.971e-01], dtype=float16), array([4.804e-05, 1.355e-04, 1.000e+00], dtype=float16), array([2.921e-06, 1.003e-04, 1.000e+00], dtype=float16), array([1.922e-04, 3.910e-05, 1.000e+00], dtype=float16), array([3.868e-05, 2.842e-01, 7.158e-01], dtype=float16), array([3.64e-06, 2.35e-05, 1.00e+00], dtype=float16), array([1.336e-04, 2.891e-05, 1.000e+00], dtype=float16), array([1.633e-05, 1.611e-03, 9.985e-01], dtype=float16), array([4.077e-05, 1.597e-03, 9.985e-01], dtype=float16), array([6.807e-05, 1.081e-03, 9.990e-01], dtype=float16), array([3.76e-06, 9.86e-05, 1.00e+00], dtype=float16), array([1.490e-05, 2.444e-04, 9.995e-01], dtype=float16), array([3.934e-06, 1.316e-04, 1.000e+00], dtype=float16), array([3.207e-05, 4.784e-03, 9.951e-01], dtype=float16), array([4.294e-04, 4.818e-03, 9.946e-01], dtype=float16), array([2.736e-05, 2.672e-03, 9.971e-01], dtype=float16), array([1.264e-05, 3.317e-03, 9.966e-01], dtype=float16), array([8.71e-05, 8.54e-04, 9.99e-01], dtype=float16), array([1.508e-05, 7.644e-04, 9.990e-01], dtype=float16), array([5.417e-04, 1.074e-02, 9.888e-01], dtype=float16), array([2.57e-05, 2.87e-05, 1.00e+00], dtype=float16), array([0.007084, 0.4033  , 0.59    ], dtype=float16), array([6.735e-06, 3.332e-03, 9.966e-01], dtype=float16), array([3.79e-05, 1.04e-03, 9.99e-01], dtype=float16), array([1.089e-04, 7.835e-03, 9.922e-01], dtype=float16), array([1.013e-06, 5.186e-05, 1.000e+00], dtype=float16), array([1.115e-05, 1.372e-02, 9.863e-01], dtype=float16), array([1.429e-04, 3.619e-02, 9.639e-01], dtype=float16), array([1.132e-04, 3.274e-01, 6.724e-01], dtype=float16), array([0.0086 , 0.08105, 0.91   ], dtype=float16), array([8.821e-06, 5.779e-04, 9.995e-01], dtype=float16), array([1.31e-06, 4.95e-06, 1.00e+00], dtype=float16), array([7.004e-05, 5.344e-02, 9.463e-01], dtype=float16), array([3.81e-06, 1.33e-05, 1.00e+00], dtype=float16), array([3.743e-05, 1.761e-02, 9.824e-01], dtype=float16), array([5.42e-06, 4.77e-06, 1.00e+00], dtype=float16), array([1.854e-05, 5.341e-05, 1.000e+00], dtype=float16), array([0.001829, 0.02048 , 0.9775  ], dtype=float16), array([5.805e-05, 8.267e-05, 1.000e+00], dtype=float16), array([8.516e-04, 1.265e-04, 9.990e-01], dtype=float16), array([2.217e-04, 2.484e-02, 9.751e-01], dtype=float16), array([1.143e-04, 9.409e-01, 5.893e-02], dtype=float16), array([1.439e-04, 5.364e-07, 1.000e+00], dtype=float16), array([7.153e-06, 3.393e-04, 9.995e-01], dtype=float16), array([6.258e-06, 1.172e-03, 9.990e-01], dtype=float16), array([8.70e-06, 9.24e-06, 1.00e+00], dtype=float16), array([6.551e-05, 6.886e-04, 9.990e-01], dtype=float16), array([1.525e-04, 1.531e-03, 9.985e-01], dtype=float16), array([1.751e-03, 8.875e-05, 9.980e-01], dtype=float16), array([1.487e-04, 9.262e-03, 9.907e-01], dtype=float16), array([0.002386, 0.03116 , 0.9663  ], dtype=float16), array([9.644e-05, 2.562e-03, 9.976e-01], dtype=float16), array([2.17e-05, 8.14e-05, 1.00e+00], dtype=float16), array([8.e-07, 6.e-07, 1.e+00], dtype=float16), array([0.7407 , 0.00286, 0.2566 ], dtype=float16), array([5.507e-05, 1.853e-03, 9.980e-01], dtype=float16), array([1.26e-05, 1.54e-05, 1.00e+00], dtype=float16), array([0.1444 , 0.01012, 0.8457 ], dtype=float16), array([0.01013, 0.06042, 0.9297 ], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([4.65e-06, 1.30e-05, 1.00e+00], dtype=float16), array([9.167e-05, 2.186e-04, 9.995e-01], dtype=float16), array([5.345e-04, 7.868e-05, 9.995e-01], dtype=float16), array([0.00821, 0.1431 , 0.8486 ], dtype=float16), array([1.215e-04, 2.438e-05, 1.000e+00], dtype=float16), array([0.00523 , 0.001127, 0.9937  ], dtype=float16), array([0.0291  , 0.004726, 0.9663  ], dtype=float16), array([1.615e-05, 5.989e-03, 9.941e-01], dtype=float16), array([1.318e-04, 1.804e-01, 8.193e-01], dtype=float16), array([7.286e-04, 4.947e-06, 9.990e-01], dtype=float16), array([8.574e-04, 7.507e-02, 9.243e-01], dtype=float16), array([3.934e-06, 1.261e-04, 1.000e+00], dtype=float16), array([1.4210e-04, 1.1334e-01, 8.8672e-01], dtype=float16), array([2.4e-07, 6.0e-08, 1.0e+00], dtype=float16), array([1.854e-05, 9.298e-06, 1.000e+00], dtype=float16), array([2.450e-05, 2.876e-03, 9.971e-01], dtype=float16), array([2.464e-03, 8.373e-04, 9.966e-01], dtype=float16), array([0.04053, 0.388  , 0.5713 ], dtype=float16), array([1.997e-05, 2.124e-02, 9.785e-01], dtype=float16), array([7.124e-04, 1.379e-02, 9.854e-01], dtype=float16), array([1.4186e-05, 1.0345e-02, 9.8975e-01], dtype=float16), array([2.353e-04, 4.692e-01, 5.308e-01], dtype=float16), array([0.469  , 0.02698, 0.504  ], dtype=float16), array([5.126e-06, 4.473e-04, 9.995e-01], dtype=float16), array([4.666e-04, 1.920e-02, 9.805e-01], dtype=float16), array([1.1742e-05, 1.0556e-04, 1.0000e+00], dtype=float16), array([0.03513, 0.05957, 0.9053 ], dtype=float16), array([3.989e-04, 1.248e-04, 9.995e-01], dtype=float16), array([3.439e-05, 3.374e-04, 9.995e-01], dtype=float16), array([5.941e-04, 8.279e-05, 9.995e-01], dtype=float16), array([0.001834, 0.00829 , 0.9897  ], dtype=float16), array([1.392e-04, 1.283e-04, 9.995e-01], dtype=float16), array([1.647e-04, 3.047e-02, 9.692e-01], dtype=float16), array([0.1714  , 0.002909, 0.8257  ], dtype=float16), array([0.3276, 0.2207, 0.4517], dtype=float16), array([9.656e-06, 1.086e-03, 9.990e-01], dtype=float16), array([1.180e-05, 5.245e-03, 9.946e-01], dtype=float16), array([4.292e-05, 1.285e-04, 1.000e+00], dtype=float16), array([4.643e-05, 4.532e-02, 9.546e-01], dtype=float16), array([5.72e-06, 6.14e-06, 1.00e+00], dtype=float16), array([3.50e-05, 3.78e-05, 1.00e+00], dtype=float16), array([6.437e-06, 2.388e-04, 9.995e-01], dtype=float16), array([2.325e-05, 1.388e-02, 9.863e-01], dtype=float16), array([6.735e-05, 3.843e-04, 9.995e-01], dtype=float16), array([2.496e-04, 1.451e-01, 8.545e-01], dtype=float16), array([2.265e-05, 1.317e-05, 1.000e+00], dtype=float16), array([1.570e-03, 2.421e-04, 9.980e-01], dtype=float16), array([7.749e-06, 9.878e-01, 1.201e-02], dtype=float16), array([4.714e-04, 1.153e-03, 9.985e-01], dtype=float16), array([2.384e-05, 6.920e-05, 1.000e+00], dtype=float16), array([2.999e-04, 1.888e-04, 9.995e-01], dtype=float16), array([0.001369, 0.02336 , 0.975   ], dtype=float16), array([1.186e-05, 2.682e-06, 1.000e+00], dtype=float16), array([0.02054, 0.00918, 0.97   ], dtype=float16), array([0.1395, 0.1927, 0.6675], dtype=float16), array([1.25e-05, 3.89e-05, 1.00e+00], dtype=float16)]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/hpc/empk/empk004h/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
output:  [array([0.2307 , 0.00846, 0.7607 ], dtype=float16), array([0.003029, 0.8955  , 0.1015  ], dtype=float16), array([1.097e-05, 2.987e-04, 9.995e-01], dtype=float16), array([4.71e-06, 3.61e-05, 1.00e+00], dtype=float16), array([9.799e-05, 1.958e-02, 9.805e-01], dtype=float16), array([7.296e-05, 2.789e-05, 1.000e+00], dtype=float16), array([9.590e-05, 1.360e-02, 9.863e-01], dtype=float16), array([8.464e-06, 9.475e-04, 9.990e-01], dtype=float16), array([2.939e-05, 2.357e-03, 9.976e-01], dtype=float16), array([1.323e-05, 1.021e-03, 9.990e-01], dtype=float16), array([1.907e-05, 5.131e-04, 9.995e-01], dtype=float16), array([2.24e-05, 2.59e-05, 1.00e+00], dtype=float16), array([1.305e-05, 1.073e-06, 1.000e+00], dtype=float16), array([1.118e-04, 4.993e-03, 9.951e-01], dtype=float16), array([1.895e-05, 1.326e-02, 9.868e-01], dtype=float16), array([3.099e-06, 5.597e-05, 1.000e+00], dtype=float16), array([2.14e-05, 9.68e-03, 9.90e-01], dtype=float16), array([1.615e-05, 1.945e-03, 9.980e-01], dtype=float16), array([4.940e-04, 1.081e-03, 9.985e-01], dtype=float16), array([1.5e-06, 1.2e-07, 1.0e+00], dtype=float16), array([4.625e-05, 2.937e-04, 9.995e-01], dtype=float16), array([0.001079, 0.05038 , 0.9487  ], dtype=float16), array([4.318e-04, 1.276e-02, 9.868e-01], dtype=float16), array([7.451e-06, 2.927e-05, 1.000e+00], dtype=float16), array([8.631e-05, 2.062e-03, 9.980e-01], dtype=float16), array([4.17e-05, 4.47e-06, 1.00e+00], dtype=float16), array([1.907e-05, 2.375e-03, 9.976e-01], dtype=float16), array([0.001106, 0.478   , 0.521   ], dtype=float16), array([0.0227 , 0.00784, 0.969  ], dtype=float16), array([4.063e-04, 1.502e-03, 9.980e-01], dtype=float16), array([7.313e-05, 8.442e-01, 1.558e-01], dtype=float16), array([2.518e-04, 2.861e-06, 9.995e-01], dtype=float16), array([4.964e-04, 5.918e-04, 9.990e-01], dtype=float16), array([1.8847e-04, 1.1414e-01, 8.8574e-01], dtype=float16), array([2.393e-04, 1.965e-03, 9.976e-01], dtype=float16), array([1.520e-05, 1.926e-04, 1.000e+00], dtype=float16), array([7.14e-04, 2.39e-04, 9.99e-01], dtype=float16), array([8.166e-06, 1.350e-03, 9.985e-01], dtype=float16), array([2.000e-04, 2.072e-02, 9.790e-01], dtype=float16), array([1.897e-04, 3.244e-02, 9.673e-01], dtype=float16), array([3.380e-05, 5.573e-03, 9.946e-01], dtype=float16), array([0.011185, 0.07556 , 0.913   ], dtype=float16), array([0.0366  , 0.003778, 0.9595  ], dtype=float16), array([4.189e-04, 1.541e-04, 9.995e-01], dtype=float16), array([5.960e-05, 9.076e-02, 9.092e-01], dtype=float16), array([7.215e-04, 5.453e-02, 9.448e-01], dtype=float16), array([1.470e-03, 6.342e-05, 9.985e-01], dtype=float16), array([4.227e-03, 4.320e-04, 9.951e-01], dtype=float16), array([4.e-07, 2.e-07, 1.e+00], dtype=float16), array([1.948e-04, 4.687e-04, 9.995e-01], dtype=float16), array([1.373e-04, 1.431e-06, 1.000e+00], dtype=float16), array([4.4e-05, 3.8e-06, 1.0e+00], dtype=float16), array([0.004025, 0.002216, 0.9937  ], dtype=float16), array([0.1353  , 0.001313, 0.8633  ], dtype=float16), array([1.1396e-04, 6.0059e-02, 9.3994e-01], dtype=float16), array([2.998e-05, 1.777e-03, 9.980e-01], dtype=float16)]
Traceback (most recent call last):
  File "/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline/llama_fine_tuned_in_pipeline.py", line 383, in <module>
    X_train_features, X_dev_features, X_test_features = extract_features(df_train, df_dev, df_test)
                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline/llama_fine_tuned_in_pipeline.py", line 347, in extract_features
    print('X_train_features.shape: ', X_train_features.shape)
                                      ^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'shape'
srun: error: tg092: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=768510.0
=== JOB_STATISTICS ===
=== current date     : Wed 14 Feb 2024 09:51:31 AM CET
= Job-ID             : 768510 on tinygpu
= Job-Name           : job_llama_fine_tuned_in_pipeline.sh
= Job-Command        : /home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline/job_llama_fine_tuned_in_pipeline.sh
= Initial workdir    : /home/hpc/empk/empk004h/depression-detection/code_for_job/speech_text/llama_fine_tuned_in_pipeline
= Queue/Partition    : a100
= Slurm account      : empk with QOS=normal
= Requested resources: cpu=32,mem=120000M,node=1,billing=32,gres/gpu=1,gres/gpu:a100=1 for 20:00:00
= Elapsed runtime    : 00:01:56
= Total RAM usage    : 1.2 GiB of requested 117 GiB (1.0%)   
= Node list          : tg092
= Subm/Elig/Start/End: 2024-02-14T09:49:35 / 2024-02-14T09:49:35 / 2024-02-14T09:49:35 / 2024-02-14T09:51:31
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           76.4G   104.9G   209.7G        N/A  45,080      500K   1,000K        N/A    
    /home/vault       1479.7G     0.0K     0.0K        N/A      74K     300K     450K        N/A    
    /home/woody        311.0G   500.0G   750.0G        N/A     314K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:01:00.0, 1371836, 34 %, 15 %, 7174 MiB, 87989 ms
