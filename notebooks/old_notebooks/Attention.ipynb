{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Preprocess the facial action unit data for each video frame\n",
    "# by aligning and cropping the faces, and possibly extracting\n",
    "# relevant features such as Action Units (AUs) or Facial Action\n",
    "# Coding System (FACS) codes.\n",
    "\n",
    "# Assume that the preprocessed data is stored in a NumPy array\n",
    "# called \"X\" with shape (n_samples, sequence_length, n_features).\n",
    "X = ...\n",
    "\n",
    "# Split the dataset into training and test sets, and pad or truncate\n",
    "# the sequences of facial action units so that they all have the same length.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define a neural network model with an attention layer\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(sequence_length, n_features)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Attention(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with an appropriate loss function\n",
    "# (such as binary cross-entropy) and optimization algorithm\n",
    "# (such as Adam).\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Use the model to make predictions on new data\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71616c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attention layer of the model\n",
    "attention_layer = model.layers[2]\n",
    "\n",
    "# Get the attention weights for a given input\n",
    "attention_weights = attention_layer.get_weights()[0]\n",
    "attention_weights = np.sum(attention_weights, axis=0)\n",
    "\n",
    "# Normalize the attention weights\n",
    "attention_weights = attention_weights / np.max(attention_weights)\n",
    "\n",
    "# Visualize the attention weights\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(attention_weights)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff298e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess the facial action unit data for each video frame\n",
    "# by aligning and cropping the faces, and possibly extracting\n",
    "# relevant features such as Action Units (AUs) or Facial Action\n",
    "# Coding System (FACS) codes.\n",
    "\n",
    "# Assume that the preprocessed data is stored in a NumPy array\n",
    "# called \"X\" with shape (n_samples, sequence_length, n_features).\n",
    "X = ...\n",
    "\n",
    "# Get the attention weights for a given sample\n",
    "sample_index = 0  # choose a sample index\n",
    "attention_weights = attention_layer.get_weights()[0]\n",
    "attention_weights = np.sum(attention_weights, axis=0)[sample_index]\n",
    "\n",
    "# Normalize the attention weights\n",
    "attention_weights = attention_weights / np.max(attention_weights)\n",
    "\n",
    "# Get the facial action unit data for the sample\n",
    "facial_action_units = X[sample_index]\n",
    "\n",
    "# Sort the attention weights in descending order\n",
    "sorted_indices = np.argsort(attention_weights)[::-1]\n",
    "\n",
    "# Extract the most important facial action units from the most important frames\n",
    "most_important_facial_action_units = facial_action_units[sorted_indices]\n",
    "most_important_facial_action_units = most_important_facial_action_units[:n_most_important]  # choose a value for n_most_important\n",
    "\n",
    "# You can then analyze the most important facial action units to see if there are any patterns or trends that might be indicative of depression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of rules that map action units to emotions\n",
    "rules = {\n",
    "    'happiness': {6: 1, 12: 0},  # cheek raiser and lip corner puller absent\n",
    "    'sadness': {1: 1, 4: 1, 15: 1},  # inner brow raiser, brow lowerer, and lid tightener present\n",
    "    'surprise': {1: 1, 2: 1, 5: 1},  # inner brow raiser, outer brow raiser, and brow lowerer present\n",
    "    'fear': {1: 1, 2: 1, 4: 1, 5: 1, 7: 1, 20: 1},  # inner brow raiser, outer brow raiser, brow lowerer, lid tightener, and upper lip raiser present\n",
    "    'anger': {4: 1, 5: 1, 7: 1, 23: 1},  # brow lowerer, lid tightener, and upper lip raiser present\n",
    "    'disgust': {9: 1, 15: 1, 16: 1},  # nose wrinkler, lid tightener, and upper lip raiser present\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "To find the proportion of an interview that each action unit (AU) occurred, \n",
    "you can use the action unit occurrences data from the dataset and calculate the \n",
    "proportion of frames in which each AU occurred. \n",
    "Here is a sample Python code that demonstrates how you could do this:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the action unit occurrences from the CSV file\n",
    "occurrences = pd.read_csv('occurrences.csv')\n",
    "\n",
    "# Calculate the total number of frames in the interview\n",
    "n_frames = occurrences.shape[0]\n",
    "\n",
    "# Initialize a dictionary to store the proportions of frames in which each AU occurred\n",
    "au_proportions = {}\n",
    "\n",
    "# Iterate over the columns in the dataframe\n",
    "for column in occurrences.columns:\n",
    "    # If the column is an action unit, calculate the proportion of frames in which it occurred\n",
    "    if column.startswith('AU'):\n",
    "        au_proportions[column] = occurrences[column].sum() / n_frames\n",
    "\n",
    "# Print the proportions of frames in which each AU occurred\n",
    "print(au_proportions)\n",
    "\n",
    "\n",
    "This code loads the action unit occurrences from the CSV file, \n",
    "calculates the total number of frames in the interview, initializes a \n",
    "dictionary to store the proportions of frames in which each AU occurred, \n",
    "and then iterates over the columns in the dataframe to calculate the proportion \n",
    "of frames in which each AU occurred. Finally, it prints the proportions of frames \n",
    "in which each AU occurred.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "I have a dataset with 3 modalities, video, text, and audio. \n",
    "regarding videos, I have OpenFace output which is Action Units, \n",
    "head pose, and eye gaze. regarding audio, I have MFCC and eGeMaps \n",
    "features and regarding text I have transcripts. I have also deep \n",
    "representations form videos from Resnet and VGG networks. \n",
    "I have these data from interviews with 200 participants. \n",
    "the label of the dataset the depression severity of each \n",
    "person which can be a number from 1 to 50. can you please tell\n",
    "me how I can train a machine-learning model with all these modalities to detect depression severity?\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the action unit intensities and occurrences from the CSV file\n",
    "au_intensities = pd.read_csv('au_intensities.csv')\n",
    "au_occurrences = pd.read_csv('au_occurrences.csv')\n",
    "\n",
    "# Load the head pose and eye gaze features from the CSV file\n",
    "head_pose = pd.read_csv('head_pose.csv')\n",
    "eye_gaze = pd.read_csv('eye_gaze.csv')\n",
    "\n",
    "# Load the MFCC and eGeMaps features from the CSV file\n",
    "mfcc = pd.read_csv('mfcc.csv')\n",
    "egemaps = pd.read_csv('egemaps.csv')\n",
    "\n",
    "# Load the transcripts from the CSV file\n",
    "transcripts = pd.read_csv('transcripts.csv')\n",
    "\n",
    "# Load the deep representations from the CSV files\n",
    "resnet = pd.read_csv('resnet.csv')\n",
    "vgg = pd.read_csv('vgg.csv')\n",
    "\n",
    "# Load the depression severity labels from the CSV file\n",
    "labels = pd.read_csv('labels.csv')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "au_intensities_train, au_intensities_test, au_occurrences_train, au_occurrences_test, head_pose_train, head_pose_test, eye_gaze_train, eye_gaze_test, mfcc_train, mfcc_test, egemaps_train, egemaps_test, transcripts_train, transcripts_test, resnet\n",
    "\n",
    "# Train separate models for each modality using the training data\n",
    "au_intensities_model = RandomForestClassifier().fit(au_intensities_train, labels_train)\n",
    "au_occurrences_model = RandomForestClassifier().fit(au_occurrences_train, labels_train)\n",
    "head_pose_model = RandomForestClassifier().fit(head_pose_train, labels_train)\n",
    "eye_gaze_model = RandomForestClassifier().fit(eye_gaze_train, labels_train)\n",
    "mfcc_model = RandomForestClassifier().fit(mfcc_train, labels_train)\n",
    "egemaps_model = RandomForestClassifier().fit(egemaps_train, labels_train)\n",
    "transcripts_model = RandomForestClassifier().fit(transcripts_train, labels_train)\n",
    "resnet_model = RandomForestClassifier().fit(resnet_train, labels_train)\n",
    "vgg_model = RandomForestClassifier().fit(vgg_train, labels_train)\n",
    "\n",
    "# Make predictions on the test data using each of the models\n",
    "au_intensities_predictions = au_intensities_model.predict(au_intensities_test)\n",
    "au_occurrences_predictions = au_occurrences_model.predict(au_occurrences_test)\n",
    "head_pose_predictions = head_pose_model.predict(head_pose_test)\n",
    "eye_gaze_predictions = eye_gaze_model.predict(eye_gaze_test)\n",
    "mfcc_predictions = mfcc_model.predict(mfcc_test)\n",
    "egemaps_predictions = egemaps_model.predict(egemaps_test)\n",
    "transcripts_predictions = transcripts_model.predict(transcripts_test)\n",
    "resnet_predictions = resnet_model.predict(resnet_test)\n",
    "vgg_predictions = vgg_model.predict(vgg_test)\n",
    "\n",
    "# Combine the predictions using a weighted average\n",
    "predictions = (au_intensities_predictions * 0.1 + au_occurrences_predictions * 0.1 + head_pose_predictions * 0.1 + eye_gaze_predictions * 0.1 + mfcc_predictions * 0.1 + egemaps_predictions * 0.1 + transcripts_predictions * 0.1 + resnet_predictions * 0.2 + vgg_predictions * 0.2) / 2\n",
    "\n",
    "# Calculate the mean squared error between the predictions and the true labels\n",
    "mse = mean_squared_error(labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db996670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data using each of the models\n",
    "au_intensities_predictions = au_intensities_model.predict(au_intensities_test)\n",
    "au_occurrences_predictions = au_occurrences_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fda31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the eye gaze data from the CSV file\n",
    "eye_gaze = pd.read_csv('eye_gaze.csv')\n",
    "\n",
    "# Load the head location and rotation data from the CSV file\n",
    "head_location = pd.read_csv('head_location.csv')\n",
    "head_rotation = pd.read_csv('head_rotation.csv')\n",
    "\n",
    "# Set the threshold for significant eye gaze changes\n",
    "eye_gaze_threshold = 0.05\n",
    "\n",
    "# Initialize variables to store the eye gaze frame numbers and the eye gaze direction vectors\n",
    "eye_gaze_frames = []\n",
    "eye_gaze_vectors = []\n",
    "\n",
    "# Iterate through the eye gaze data\n",
    "for i in range(1, eye_gaze.shape[0]):\n",
    "    # Calculate the difference between the current frame and the previous frame\n",
    "    eye_gaze_difference = abs(eye_gaze.iloc[i][['X', 'Y']] - eye_gaze.iloc[i-1][['X', 'Y']])\n",
    "    \n",
    "    # If the difference is above the threshold, store the frame number and the eye gaze direction vector\n",
    "    if eye_gaze_difference.sum() > eye_gaze_threshold:\n",
    "        eye_gaze_frames.append(i)\n",
    "        eye_gaze_vectors.append(eye_gaze.iloc[i][['X', 'Y', 'Z']].values)\n",
    "\n",
    "# Convert the head rotation data to a rotation matrix\n",
    "Rx, Ry, Rz = head_rotation[['Rx', 'Ry', 'Rz']].values.T\n",
    "Rx = np.array([[1, 0, 0], [0, np.cos(Rx), -np.sin(Rx)], [0, np.sin(Rx), np.cos(Rx)]])\n",
    "Ry = np.array([[np.cos(Ry), 0, np.sin(Ry)], [0, 1, 0], [-np.sin(Ry), 0, np.cos(Ry)]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
