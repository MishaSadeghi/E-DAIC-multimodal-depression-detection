### Starting TaskPrologue of job 782828 on tg095 at Fri 08 Mar 2024 11:12:45 AM CET
Running on cores 32-63 with governor ondemand
Fri Mar  8 11:12:45 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:41:00.0 Off |                    0 |
| N/A   38C    P0              57W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
### Finished TaskPrologue

/home/hpc/empk/empk004h/depression-detection/experiments_4_new_prompts
folder_path:  /home/hpc/empk/empk004h/depression-detection/data/original_transcripts_completions/prompt_1
csv_files:  ['df_train_prompt1.csv', 'df_dev_prompt1.csv', 'df_test_prompt1.csv']
fine tuning for prompt:  prompt_1
len train_loader:  11
Epoch 0, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.681
Epoch 0, Batch 1, Learning Rate: 0.00005000, Train Loss: 1.237
Epoch 0, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.619
Epoch 0, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.784
Epoch 0, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.939
Epoch 0, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.877
Epoch 0, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.586
Epoch 0, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.870
Epoch 0, Batch 8, Learning Rate: 0.00005000, Train Loss: 1.037
Epoch 0, Batch 9, Learning Rate: 0.00005000, Train Loss: 1.150
Epoch 0, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.891
Epoch 0, Batch 10, Dev Loss: 0.800
len train_loader:  11
Epoch 1, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.770
Epoch 1, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.774
Epoch 1, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.751
Epoch 1, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.559
Epoch 1, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.904
Epoch 1, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.598
Epoch 1, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.856
Epoch 1, Batch 7, Learning Rate: 0.00005000, Train Loss: 1.348
Epoch 1, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.464
Epoch 1, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.796
Epoch 1, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.751
Epoch 1, Batch 10, Dev Loss: 0.742
len train_loader:  11
Epoch 2, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.927
Epoch 2, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.781
Epoch 2, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.569
Epoch 2, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.776
Epoch 2, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.611
Epoch 2, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.772
Epoch 2, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.527
Epoch 2, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.622
Epoch 2, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.774
Epoch 2, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.597
Epoch 2, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.640
Epoch 2, Batch 10, Dev Loss: 0.740
len train_loader:  11
Epoch 3, Batch 0, Learning Rate: 0.00005000, Train Loss: 1.046
Epoch 3, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.863
Epoch 3, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.501
Epoch 3, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.384
Epoch 3, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.726
Epoch 3, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.635
Epoch 3, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.606
Epoch 3, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.630
Epoch 3, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.578
Epoch 3, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.385
Epoch 3, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.101
Epoch 3, Batch 10, Dev Loss: 0.750
len train_loader:  11
Epoch 4, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.250
Epoch 4, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.802
Epoch 4, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.954
Epoch 4, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.635
Epoch 4, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.493
Epoch 4, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.475
Epoch 4, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.415
Epoch 4, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.471
Epoch 4, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.845
Epoch 4, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.532
Epoch 4, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.368
Epoch 4, Batch 10, Dev Loss: 0.814
len train_loader:  11
Epoch 5, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.260
Epoch 5, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.379
Epoch 5, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.671
Epoch 5, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.605
Epoch 5, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.175
Epoch 5, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.173
Epoch 5, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.725
Epoch 5, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.892
Epoch 5, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.524
Epoch 5, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.748
Epoch 5, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.141
Epoch 5, Batch 10, Dev Loss: 0.851
Early stopping triggered. No improvement in dev loss.
folder_path:  /home/hpc/empk/empk004h/depression-detection/data/original_transcripts_completions/prompt_2
csv_files:  ['df_dev_prompt2.csv', 'df_test_prompt2.csv', 'df_train_prompt2.csv']
fine tuning for prompt:  prompt_2
len train_loader:  11
Epoch 0, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.985
Epoch 0, Batch 1, Learning Rate: 0.00005000, Train Loss: 1.061
Epoch 0, Batch 2, Learning Rate: 0.00005000, Train Loss: 1.164
Epoch 0, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.924
Epoch 0, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.692
Epoch 0, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.936
Epoch 0, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.986
Epoch 0, Batch 7, Learning Rate: 0.00005000, Train Loss: 1.000
Epoch 0, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.854
Epoch 0, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.594
Epoch 0, Batch 10, Learning Rate: 0.00005000, Train Loss: 1.204
Epoch 0, Batch 10, Dev Loss: 0.764
len train_loader:  11
Epoch 1, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.912
Epoch 1, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.807
Epoch 1, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.631
Epoch 1, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.606
Epoch 1, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.784
Epoch 1, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.881
Epoch 1, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.751
Epoch 1, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.893
Epoch 1, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.547
Epoch 1, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.631
Epoch 1, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.391
Epoch 1, Batch 10, Dev Loss: 0.759
len train_loader:  11
Epoch 2, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.637
Epoch 2, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.652
Epoch 2, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.789
Epoch 2, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.996
Epoch 2, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.416
Epoch 2, Batch 5, Learning Rate: 0.00005000, Train Loss: 1.100
Epoch 2, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.747
Epoch 2, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.840
Epoch 2, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.689
Epoch 2, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.733
Epoch 2, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.272
Epoch 2, Batch 10, Dev Loss: 0.770
len train_loader:  11
Epoch 3, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.625
Epoch 3, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.484
Epoch 3, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.829
Epoch 3, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.582
Epoch 3, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.629
Epoch 3, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.711
Epoch 3, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.803
Epoch 3, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.893
Epoch 3, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.671
Epoch 3, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.634
Epoch 3, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.925
Epoch 3, Batch 10, Dev Loss: 0.789
len train_loader:  11
Epoch 4, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.586
Epoch 4, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.655
Epoch 4, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.611
Epoch 4, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.615
Epoch 4, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.367
Epoch 4, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.624
Epoch 4, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.745
Epoch 4, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.577
Epoch 4, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.395
Epoch 4, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.385
Epoch 4, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.277
Epoch 4, Batch 10, Dev Loss: 1.019
Early stopping triggered. No improvement in dev loss.
folder_path:  /home/hpc/empk/empk004h/depression-detection/data/original_transcripts_completions/prompt_3
csv_files:  ['df_test_prompt3.csv', 'df_dev_prompt3.csv', 'df_train_prompt3.csv']
fine tuning for prompt:  prompt_3
len train_loader:  11
Epoch 0, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.603
Epoch 0, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.993
Epoch 0, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.969
Epoch 0, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.841
Epoch 0, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.837
Epoch 0, Batch 5, Learning Rate: 0.00005000, Train Loss: 1.237
Epoch 0, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.831
Epoch 0, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.832
Epoch 0, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.559
Epoch 0, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.688
Epoch 0, Batch 10, Learning Rate: 0.00005000, Train Loss: 1.389
Epoch 0, Batch 10, Dev Loss: 0.749
len train_loader:  11
Epoch 1, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.874
Epoch 1, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.849
Epoch 1, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.575
Epoch 1, Batch 3, Learning Rate: 0.00005000, Train Loss: 1.012
Epoch 1, Batch 4, Learning Rate: 0.00005000, Train Loss: 1.178
Epoch 1, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.930
Epoch 1, Batch 6, Learning Rate: 0.00005000, Train Loss: 1.246
Epoch 1, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.623
Epoch 1, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.470
Epoch 1, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.684
Epoch 1, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.827
Epoch 1, Batch 10, Dev Loss: 0.717
len train_loader:  11
Epoch 2, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.945
Epoch 2, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.668
Epoch 2, Batch 2, Learning Rate: 0.00005000, Train Loss: 1.088
Epoch 2, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.725
Epoch 2, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.611
Epoch 2, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.735
Epoch 2, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.781
Epoch 2, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.826
Epoch 2, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.459
Epoch 2, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.531
Epoch 2, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.017
Epoch 2, Batch 10, Dev Loss: 0.913
len train_loader:  11
Epoch 3, Batch 0, Learning Rate: 0.00005000, Train Loss: 1.885
Epoch 3, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.500
Epoch 3, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.476
Epoch 3, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.454
Epoch 3, Batch 4, Learning Rate: 0.00005000, Train Loss: 1.017
Epoch 3, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.576
Epoch 3, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.695
Epoch 3, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.784
Epoch 3, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.514
Epoch 3, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.804
Epoch 3, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.502
Epoch 3, Batch 10, Dev Loss: 0.707
len train_loader:  11
Epoch 4, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.677
Epoch 4, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.646
Epoch 4, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.723
Epoch 4, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.444
Epoch 4, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.724
Epoch 4, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.899
Epoch 4, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.561
Epoch 4, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.665
Epoch 4, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.738
Epoch 4, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.801
Epoch 4, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.049
Epoch 4, Batch 10, Dev Loss: 0.752
len train_loader:  11
Epoch 5, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.570
Epoch 5, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.474
Epoch 5, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.574
Epoch 5, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.680
Epoch 5, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.908
Epoch 5, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.400
Epoch 5, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.569
Epoch 5, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.980
Epoch 5, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.502
Epoch 5, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.645
Epoch 5, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.017
Epoch 5, Batch 10, Dev Loss: 0.812
len train_loader:  11
Epoch 6, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.862
Epoch 6, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.516
Epoch 6, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.529
Epoch 6, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.671
Epoch 6, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.720
Epoch 6, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.512
Epoch 6, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.809
Epoch 6, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.432
Epoch 6, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.575
Epoch 6, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.550
Epoch 6, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.452
Epoch 6, Batch 10, Dev Loss: 0.821
Early stopping triggered. No improvement in dev loss.
folder_path:  /home/hpc/empk/empk004h/depression-detection/data/revised_transcripts_completions/prompt_1
csv_files:  ['df_train_prompt1.csv', 'df_dev_prompt1.csv', 'df_test_prompt1.csv']
fine tuning for prompt:  prompt_1
len train_loader:  11
Epoch 0, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.915
Epoch 0, Batch 1, Learning Rate: 0.00005000, Train Loss: 1.077
Epoch 0, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.717
Epoch 0, Batch 3, Learning Rate: 0.00005000, Train Loss: 1.218
Epoch 0, Batch 4, Learning Rate: 0.00005000, Train Loss: 1.114
Epoch 0, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.973
Epoch 0, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.860
Epoch 0, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.696
Epoch 0, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.834
Epoch 0, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.643
Epoch 0, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.699
Epoch 0, Batch 10, Dev Loss: 0.817
len train_loader:  11
Epoch 1, Batch 0, Learning Rate: 0.00005000, Train Loss: 1.095
Epoch 1, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.885
Epoch 1, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.788
Epoch 1, Batch 3, Learning Rate: 0.00005000, Train Loss: 1.147
Epoch 1, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.637
Epoch 1, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.855
Epoch 1, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.711
Epoch 1, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.940
Epoch 1, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.779
Epoch 1, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.735
Epoch 1, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.980
Epoch 1, Batch 10, Dev Loss: 0.711
len train_loader:  11
Epoch 2, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.826
Epoch 2, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.757
Epoch 2, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.565
Epoch 2, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.796
Epoch 2, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.656
Epoch 2, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.844
Epoch 2, Batch 6, Learning Rate: 0.00005000, Train Loss: 1.070
Epoch 2, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.687
Epoch 2, Batch 8, Learning Rate: 0.00005000, Train Loss: 1.079
Epoch 2, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.569
Epoch 2, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.296
Epoch 2, Batch 10, Dev Loss: 0.774
len train_loader:  11
Epoch 3, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.828
Epoch 3, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.847
Epoch 3, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.693
Epoch 3, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.550
Epoch 3, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.510
Epoch 3, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.973
Epoch 3, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.672
Epoch 3, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.717
Epoch 3, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.684
Epoch 3, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.543
Epoch 3, Batch 10, Learning Rate: 0.00005000, Train Loss: 1.030
Epoch 3, Batch 10, Dev Loss: 0.758
len train_loader:  11
Epoch 4, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.722
Epoch 4, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.577
Epoch 4, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.679
Epoch 4, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.731
Epoch 4, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.591
Epoch 4, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.856
Epoch 4, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.800
Epoch 4, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.571
Epoch 4, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.546
Epoch 4, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.897
Epoch 4, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.573
Epoch 4, Batch 10, Dev Loss: 0.784
Early stopping triggered. No improvement in dev loss.
folder_path:  /home/hpc/empk/empk004h/depression-detection/data/revised_transcripts_completions/prompt_2
csv_files:  ['df_dev_prompt2.csv', 'df_test_prompt2.csv', 'df_train_prompt2.csv']
fine tuning for prompt:  prompt_2
len train_loader:  11
Epoch 0, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.786
Epoch 0, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.743
Epoch 0, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.988
Epoch 0, Batch 3, Learning Rate: 0.00005000, Train Loss: 1.220
Epoch 0, Batch 4, Learning Rate: 0.00005000, Train Loss: 1.128
Epoch 0, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.860
Epoch 0, Batch 6, Learning Rate: 0.00005000, Train Loss: 1.128
Epoch 0, Batch 7, Learning Rate: 0.00005000, Train Loss: 1.017
Epoch 0, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.801
Epoch 0, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.705
Epoch 0, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.862
Epoch 0, Batch 10, Dev Loss: 0.752
len train_loader:  11
Epoch 1, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.641
Epoch 1, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.577
Epoch 1, Batch 2, Learning Rate: 0.00005000, Train Loss: 1.104
Epoch 1, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.918
Epoch 1, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.980
Epoch 1, Batch 5, Learning Rate: 0.00005000, Train Loss: 1.224
Epoch 1, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.765
Epoch 1, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.629
Epoch 1, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.776
Epoch 1, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.879
Epoch 1, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.272
Epoch 1, Batch 10, Dev Loss: 0.762
len train_loader:  11
Epoch 2, Batch 0, Learning Rate: 0.00005000, Train Loss: 1.028
Epoch 2, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.836
Epoch 2, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.771
Epoch 2, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.793
Epoch 2, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.603
Epoch 2, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.639
Epoch 2, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.657
Epoch 2, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.530
Epoch 2, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.608
Epoch 2, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.776
Epoch 2, Batch 10, Learning Rate: 0.00005000, Train Loss: 1.492
Epoch 2, Batch 10, Dev Loss: 0.799
len train_loader:  11
Epoch 3, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.582
Epoch 3, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.604
Epoch 3, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.500
Epoch 3, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.588
Epoch 3, Batch 4, Learning Rate: 0.00005000, Train Loss: 1.018
Epoch 3, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.939
Epoch 3, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.805
Epoch 3, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.851
Epoch 3, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.913
Epoch 3, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.614
Epoch 3, Batch 10, Learning Rate: 0.00005000, Train Loss: 1.094
Epoch 3, Batch 10, Dev Loss: 0.752
len train_loader:  11
Epoch 4, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.726
Epoch 4, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.627
Epoch 4, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.588
Epoch 4, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.959
Epoch 4, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.442
Epoch 4, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.797
Epoch 4, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.554
Epoch 4, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.693
Epoch 4, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.396
Epoch 4, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.651
Epoch 4, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.158
Epoch 4, Batch 10, Dev Loss: 0.828
len train_loader:  11
Epoch 5, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.671
Epoch 5, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.370
Epoch 5, Batch 2, Learning Rate: 0.00005000, Train Loss: 1.054
Epoch 5, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.457
Epoch 5, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.425
Epoch 5, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.691
Epoch 5, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.684
Epoch 5, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.761
Epoch 5, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.607
Epoch 5, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.396
Epoch 5, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.084
Epoch 5, Batch 10, Dev Loss: 0.782
len train_loader:  11
Epoch 6, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.347
Epoch 6, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.496
Epoch 6, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.234
Epoch 6, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.442
Epoch 6, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.518
Epoch 6, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.444
Epoch 6, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.338
Epoch 6, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.676
Epoch 6, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.390
Epoch 6, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.395
Epoch 6, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.535
Epoch 6, Batch 10, Dev Loss: 1.456
Early stopping triggered. No improvement in dev loss.
folder_path:  /home/hpc/empk/empk004h/depression-detection/data/revised_transcripts_completions/prompt_3
csv_files:  ['df_test_prompt3.csv', 'df_dev_prompt3.csv', 'df_train_prompt3.csv']
fine tuning for prompt:  prompt_3
len train_loader:  11
Epoch 0, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.650
Epoch 0, Batch 1, Learning Rate: 0.00005000, Train Loss: 1.367
Epoch 0, Batch 2, Learning Rate: 0.00005000, Train Loss: 1.644
Epoch 0, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.847
Epoch 0, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.769
Epoch 0, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.970
Epoch 0, Batch 6, Learning Rate: 0.00005000, Train Loss: 1.104
Epoch 0, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.865
Epoch 0, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.970
Epoch 0, Batch 9, Learning Rate: 0.00005000, Train Loss: 1.194
Epoch 0, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.343
Epoch 0, Batch 10, Dev Loss: 0.749
len train_loader:  11
Epoch 1, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.787
Epoch 1, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.716
Epoch 1, Batch 2, Learning Rate: 0.00005000, Train Loss: 1.206
Epoch 1, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.780
Epoch 1, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.929
Epoch 1, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.649
Epoch 1, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.780
Epoch 1, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.763
Epoch 1, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.648
Epoch 1, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.787
Epoch 1, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.917
Epoch 1, Batch 10, Dev Loss: 0.686
len train_loader:  11
Epoch 2, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.615
Epoch 2, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.728
Epoch 2, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.660
Epoch 2, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.875
Epoch 2, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.854
Epoch 2, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.574
Epoch 2, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.857
Epoch 2, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.813
Epoch 2, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.508
Epoch 2, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.644
Epoch 2, Batch 10, Learning Rate: 0.00005000, Train Loss: 1.239
Epoch 2, Batch 10, Dev Loss: 0.755
len train_loader:  11
Epoch 3, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.580
Epoch 3, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.614
Epoch 3, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.585
Epoch 3, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.978
Epoch 3, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.720
Epoch 3, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.481
Epoch 3, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.688
Epoch 3, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.404
Epoch 3, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.581
Epoch 3, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.700
Epoch 3, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.565
Epoch 3, Batch 10, Dev Loss: 0.714
len train_loader:  11
Epoch 4, Batch 0, Learning Rate: 0.00005000, Train Loss: 0.360
Epoch 4, Batch 1, Learning Rate: 0.00005000, Train Loss: 0.137
Epoch 4, Batch 2, Learning Rate: 0.00005000, Train Loss: 0.719
Epoch 4, Batch 3, Learning Rate: 0.00005000, Train Loss: 0.470
Epoch 4, Batch 4, Learning Rate: 0.00005000, Train Loss: 0.727
Epoch 4, Batch 5, Learning Rate: 0.00005000, Train Loss: 0.525
Epoch 4, Batch 6, Learning Rate: 0.00005000, Train Loss: 0.676
Epoch 4, Batch 7, Learning Rate: 0.00005000, Train Loss: 0.580
Epoch 4, Batch 8, Learning Rate: 0.00005000, Train Loss: 0.708
Epoch 4, Batch 9, Learning Rate: 0.00005000, Train Loss: 0.559
Epoch 4, Batch 10, Learning Rate: 0.00005000, Train Loss: 0.297
Epoch 4, Batch 10, Dev Loss: 0.765
Early stopping triggered. No improvement in dev loss.
=== JOB_STATISTICS ===
=== current date     : Fri 08 Mar 2024 11:42:14 AM CET
= Job-ID             : 782828 on tinygpu
= Job-Name           : job_fine_tuning_deproberta.sh
= Job-Command        : /home/hpc/empk/empk004h/depression-detection/experiments_4_new_prompts/job_fine_tuning_deproberta.sh
= Initial workdir    : /home/hpc/empk/empk004h/depression-detection/experiments_4_new_prompts
= Queue/Partition    : a100
= Slurm account      : empk with QOS=normal
= Requested resources:  for 20:00:00
= Elapsed runtime    : 00:29:30
= Total RAM usage    : 3.2 GiB of requested  GiB (%)   
= Node list          : tg095
= Subm/Elig/Start/End: 2024-03-08T11:01:40 / 2024-03-08T11:01:40 / 2024-03-08T11:12:44 / 2024-03-08T11:42:14
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           77.4G   104.9G   209.7G        N/A  46,102      500K   1,000K        N/A    
    /home/vault         96.2T   150.3T   171.8T        N/A      75K     300K     450K        N/A    
    /home/woody        338.7G   500.0G   750.0G        N/A     315K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:41:00.0, 445731, 12 %, 2 %, 26958 MiB, 1755844 ms
